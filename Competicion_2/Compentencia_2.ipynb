{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VKzA7zRbvQbX"
      },
      "source": [
        "# **Competencia 2 - CC6205 Natural Language Processing ðŸ“š**\n",
        "\n",
        "Integrantes:\n",
        "\n",
        "Usuario del equipo en CodaLab (Obligatorio):\n",
        "\n",
        "Fecha lÃ­mite de entrega ðŸ“†: 30 de Junio.\n",
        "\n",
        "Tiempo estimado de dedicaciÃ³n:\n",
        "\n",
        "Link competencia: Poner el link [aquÃ­](https://codalab.lisn.upsaclay.fr/competitions/13646?secret_key=c2dbdef5-9869-4b0a-845a-2dc529b026fb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "46D0dmPvvYgh"
      },
      "source": [
        "### **Objetivo**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6jKwaiF2vZ03"
      },
      "source": [
        "El objetivo de esta competencia es resolver una de las tareas mÃ¡s importantes en el Ã¡rea del procesamiento de lenguage natural, relacionada con la extracciÃ³n de informaciÃ³n: [Named Entity Recognition (NER)](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf).\n",
        "\n",
        "En particular, y al igual que en la competencia anterior, deberÃ¡n crear distintos modelos que apunten a resolver la tarea de NER en EspaÃ±ol. Para esto, les entregaremos un dataset real perteneciente a la lista de espera NO GES en Chile. Es importante destacar que existe una falta de trabajos realizados en el Ã¡rea de NER en EspaÃ±ol y aÃºn mÃ¡s en el contexto clÃ­nico, por ende puede ser considerado como una tarea bien desafiante y quizÃ¡s les interesa trabajar en el Ã¡rea mÃ¡s adelante en sus carreras.\n",
        "\n",
        "En este notebook les entregaremos un baseline como referencia de los resultados que esperamos puedan obtener. Recuerden que el no superar a los baselines en alguna de las tres mÃ©tricas conlleva un descuento de 0.5 puntos hasta 1.5 puntos.\n",
        "\n",
        "Como hemos estado viendo redes neuronales tanto en catedras, tareas y auxiliares (o prÃ³ximamente lo harÃ¡n), esperamos que (por lo menos) utilicen Redes Neuronales Recurrentes (RNN) para resolverla.\n",
        "\n",
        "Nuevamente, hay total libertad para utilizar el software y los modelos que deseen, siempre y cuando estos no traigan los modelos ya implementados. (De todas maneras como es un corpus nuevo, es difÃ­cil que haya algÃºn modelo ya implementado con estas entidades)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw75E0nFveQm"
      },
      "source": [
        "### **ExplicaciÃ³n de la competencia**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h_WN-LyGvmuX"
      },
      "source": [
        "La tarea **NER** que van a resolver en esta competencia es comÃºnmente abordada como un problema de Sequence Labeling.\n",
        "\n",
        "**Â¿QuÃ© es Sequence Labeling?**\n",
        "\n",
        "En breves palabras, dada una secuencia de tokens (oraciÃ³n) sequence labeling tiene por objetivo asignar una etiqueta a cada token de dicha secuencia. En pocas palabras, dada una lista de tokens esperamos encontrar la mejor secuencia de etiquetas asociadas a esa lista. Ahora veamos de quÃ© se trata este problema.\n",
        "\n",
        "**Named Entity Recognition (NER)**\n",
        "\n",
        "NER es un ejemplo de un problema de Sequence Labeling. Pero antes de definir formalmente esta tarea, es necesario definir algunos conceptos claves para poder entenderla de la mejor manera:\n",
        "\n",
        "- *Token*: Un token es una secuencia de caracteres, puede ser una palabra, un nÃºmero o un sÃ­mbolo.\n",
        "\n",
        "- *Entidad*: No es mÃ¡s que un trozo de texto (uno o mÃ¡s tokens) asociado a una categorÃ­a predefinida. Originalmente se solÃ­an utilizar categorÃ­as como nombres de personas, organizaciones, ubicaciones, pero actualmente se ha extendido a diferentes dominios.\n",
        "\n",
        "- *LÃ­mites de una entidad*: Son los Ã­ndices de los tokens de inicio y fÃ­n dentro de una entidad.\n",
        "\n",
        "- *Tipo de entidad*: Es la categorÃ­a predefinida asociada a la entidad.\n",
        "\n",
        "Dicho esto, definimos formalmente una entidad como una tupla: $(s, e, t)$, donde $s, t$ son los lÃ­mites de la entidad (Ã­ndices de los tokens de inicio y fin, respectivamente) y t corresponde al tipo de entidad o categorÃ­a. Ya veremos mÃ¡s ejemplos luego de describir el Dataset.\n",
        "\n",
        "**Corpus de la Lista de espera**\n",
        "\n",
        "Trabajaran con un conjunto de datos reales correspondiente a interconsultas de la lista de espera NO GES en Chile. Si quieren saber mÃ¡s sobre cÃ³mo fueron generados los datos pueden revisar el paper publicado hace unos meses atrÃ¡s en el workshop de EMNLP, una de las conferencias mÃ¡s importantes de NLP: [https://www.aclweb.org/anthology/2020.clinicalnlp-1.32/](https://www.aclweb.org/anthology/2020.clinicalnlp-1.32/).\n",
        "\n",
        "Este corpus Chileno estÃ¡ constituido originalmente por 7 tipos de entidades pero por simplicidad en esta competencia trabajarÃ¡n con las siguientes:\n",
        "\n",
        "- **Disease**\n",
        "- **Body_Part**\n",
        "- **Medication**\n",
        "- **Procedures**\n",
        "- **Family_Member**\n",
        "\n",
        "Si quieren obtener mÃ¡s informaciÃ³n sobre estas entidades pueden consultar la [guÃ­a de anotaciÃ³n](https://plncmm.github.io/annodoc/). AdemÃ¡s, mencionar que este corpus estÃ¡ restringido bajo una licencia que permite solamente su uso acadÃ©mico, asÃ­ que no puede ser compartido mÃ¡s allÃ¡ de este curso o sin permisos por parte de los autores en caso que quieran utilizarlo fuera. Si este Ãºltimo es el caso entonces pueden escribir directamente al correo: pln@cmm.uchile.cl. Al aceptar los tÃ©rminos y condiciones de la competencia estÃ¡n de acuerdo con los puntos descritos anteriormente.\n",
        "\n",
        "\n",
        "**Formato ConLL**\n",
        "\n",
        "Los archivos que serÃ¡n entregados a ustedes vienen en un formato estÃ¡ndar utilizado en NER, llamado ConLL. No es mÃ¡s que un archivo de texto, que cumple las siguientes propiedades.\n",
        "\n",
        "- Un salto de linea corresponde a la separaciÃ³n entre oraciones. Esto es importante ya que al entrenar una red neuronal ustedes pasaran una lista de oraciones como input, mÃ¡s conocidos como batches.\n",
        "\n",
        "- La primera columna del archivo contiene todos los tokens de la particiÃ³n.\n",
        "\n",
        "- La segunda columna del archivo contiene el tipo de entidad asociado al token de la primera columna.\n",
        "\n",
        "- Los tipos de entidades siguen un formato clÃ¡sico en NER denominado *IOB2*. Si un tipo de entidad comienza con el prefijo \"B-\" (Beginning) significa que es el token de inicio de una entidad, si comienza con \"I-\" (Inside) es un token distinto al de inicio y si un token estÃ¡ asociado a la categorÃ­a O (Outside) significa que no pertenece a ninguna entidad.\n",
        "\n",
        "AquÃ­ va un ejemplo:\n",
        "\n",
        "```\n",
        "PACIENTE O\n",
        "PRESENTA O\n",
        "FRACTURA B-Disease\n",
        "CORONARIA I-Disease\n",
        "COMPLICADA I-Disease\n",
        "EN O\n",
        "PIE B-Body_Part\n",
        "IZQUIERDO I-Body_Part\n",
        ". O\n",
        "SE O\n",
        "REALIZA O\n",
        "INSTRUMENTACION B-Procedure\n",
        "INTRACONDUCTO I-Procedure\n",
        ". O\n",
        "```\n",
        "\n",
        "SegÃºn nuestra definiciÃ³n tenemos las siguientes tres entidades (enumerando desde 0):\n",
        "\n",
        "- $(2, 4, Disease)$\n",
        "- $(6, 7, Body Part)$\n",
        "- $(11, 12, Procedure)$\n",
        "\n",
        "Repasen un par de veces todos estos conceptos antes de pasar a la siguiente secciÃ³n del notebook.\n",
        "Es importante entender bien este formato ya que al medir el rendimiento de sus modelos, consideraremos una **mÃ©trica estricta**. Esta mÃ©trica se llama asÃ­ ya que considera correcta una predicciÃ³n de su modelo, sÃ³lo si al compararlo con las entidades reales **coinciden tanto los lÃ­mites de la entidad como el tipo.**\n",
        "\n",
        "Para ejemplificar, tomando el caso anterior, si el modelo es capaz de encontrar la siguiente entidad: $(2, 3, Disease)$, entonces se considera incorrecto ya que pudo predecir dos de los tres tokens de dicha enfermedad. Es decir, buscamos una mÃ©trica que sea alta a nivel de entidad y no a nivel de token.\n",
        "\n",
        "Antes de pasar a explicar las reglas, se recomienda visitar los siguientes links para entender bien el baseline de la competencia:\n",
        "\n",
        "-  [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)\n",
        "-  [Recurrent Neural Networks](slides/NLP-RNN.pdf) | [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)\n",
        "\n",
        "\n",
        "Recuerden que todo el material se encuentra disponible en el [github del curso](https://github.com/dccuchile/CC6205)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iFYoEx8SwETe"
      },
      "source": [
        "### **Reglas de la competencia**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wouh5u_uwGxW"
      },
      "source": [
        "**texto en negrita**- Para que su competencia sea evaluada, deben participar en la competencia y enviar este notebook con su informe.\n",
        "- Para participar, deben registrarse en la competencia en Codalab en grupos de mÃ¡ximo 4 alumnos. Cada grupo debe tener un nombre de equipo. (Â¡Y deben reportarlo en su informe, por favor!)\n",
        "- Las mÃ©tricas usadas serÃ¡n mÃ©tricas estrictas (ya explicado anteriormente) utilizando mÃ©tricas clÃ¡sicas como lo son precisiÃ³n, recall y micro f1-score.\n",
        "- En esta tarea se recomienda usar GPU. Pueden ejecutar su tarea en colab (lo cual trae todo instalado) o pueden intentar ejecutÃ¡ndolo en su computador. En este caso, deberÃ¡ ser compatible con cuda y deberÃ¡n instalar todo por su cuenta.\n",
        "- En total pueden hacer un **mÃ¡ximo de 5 envÃ­os**.\n",
        "- Por favor, todas sus dudas haganlas por el canal de Discord. Los emails que lleguen al equipo docente serÃ¡n remitidos a ese medio. Recuerden el Ã¡nimo colaborativo del curso.\n",
        "- Estar top 5 en alguna de las tres mÃ©tricas equivale a una bonificaciÃ³n en su nota final.\n",
        "\n",
        "Ã‰xito!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z9CPYaLRwMJu"
      },
      "source": [
        "### **Baseline**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qTOYgJrUwQZG"
      },
      "source": [
        "# En este punto esperamos que tengan conocimiento sobre redes neuronales y en particular redes neuronales recurrentes (RNN), si no siempre pueden escribirnos por el canal de Discord para aclarar dudas. La RNN del baseline adjunto a este notebook estÃ¡ programado en la librerÃ­a [`pytorch`](https://pytorch.org/) pero ustedes pueden utilizar keras, tensorflow si asÃ­ lo desean. El cÃ³digo contiene lo siguiente:\n",
        "\n",
        "- La carga de los datasets, creaciÃ³n de batches de texto y padding (esto es importante ya que si utilizan redes neuronales tienen que tener el mismo largo los inputs).\n",
        "\n",
        "- La implementaciÃ³n bÃ¡sica de una red `LSTM` simple de solo un nivel y sin bi-direccionalidad.\n",
        "\n",
        "- La construcciÃ³n del formato del output requerido para que lo puedan probar en la tarea en codalab.\n",
        "\n",
        "Se espera que como mÃ­nimo ustedes puedan experimentar con el baseline utilizando (pero no limitÃ¡ndose) estas sugerencias:\n",
        "\n",
        "*   Probar la tÃ©cnica de early stopping.\n",
        "*   Variar la cantidad de parÃ¡metros de la capa de embeddings.\n",
        "*   Variar la cantidad de capas RNN.\n",
        "*   Variar la cantidad de parÃ¡metros de las capas de RNN.\n",
        "*   Inicializar la capa de embeddings con modelos pre-entrenados. (word2vec, glove, conceptnet, etc...). [Embeddings en espaÃ±ol aquÃ­](https://github.com/dccuchile/spanish-word-embeddings). TambiÃ©n aquÃ­ pueden encontrar unos embeddings clÃ­nicos en EspaÃ±ol: [https://zenodo.org/record/3924799](https://zenodo.org/record/3924799)\n",
        "*   Variar la cantidad de Ã©pocas de entrenamiento.\n",
        "*   Variar el optimizador, learning rate, batch size, usar CRF loss, etc.\n",
        "*   Probar una capa de CRF para garantizar el     formato IOB2.\n",
        "*   Probar bi-direccionalidad.\n",
        "*   Incluir dropout.\n",
        "*   Probar modelos de tipo GRU.\n",
        "*   Probar usando capas de atenciÃ³n.\n",
        "*   Probar Embedding Contextuales (les puede ser de utilidad [flair](https://github.com/flairNLP/flair))\n",
        "*   Probar modelos de transformers en espaÃ±ol usando [Huggingface](https://github.com/huggingface/transformers) o el framework Flair."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BDcRm24CwRpQ"
      },
      "source": [
        "### **Reporte**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pk6DdcviwWGl"
      },
      "source": [
        "\n",
        "\n",
        "Este debe cumplir la siguiente estructura:\n",
        "\n",
        "1.\t**IntroducciÃ³n**: Presentar brevemente el contexto, problema a resolver, incluyendo la formalizaciÃ³n de la task (cÃ³mo son los inputs y outputs del problema) y los desafÃ­os que ven al analizar el corpus entregado. (**0.5 puntos**)\n",
        "\n",
        "2.\t**Modelos**: Describir brevemente los modelos, mÃ©todos e hiperparÃ¡metros utilizados. (**1.0 puntos**)\n",
        "\n",
        "4.\t**MÃ©tricas de evaluaciÃ³n**: Describir las mÃ©tricas utilizadas en la evaluaciÃ³n indicando quÃ© miden y cuÃ¡l es su interpretaciÃ³n en este problema en particular. (**0.5 puntos**)\n",
        "\n",
        "5.  **DiseÃ±o experimental**: Esta es una de las secciones mÃ¡s importantes del reporte. Deben describir minuciosamente los experimentos que realizarÃ¡n en la siguiente secciÃ³n. Describir las variables de control que manejarÃ¡n, algunos ejemplos pueden ser: Los hiperparÃ¡metros de los modelos, tipo de embeddings utilizados, tipos de arquitecturas. Ser claros con el conjunto de hiperparÃ¡metros que probarÃ¡n, la decisiÃ³n en las funciones de optimizaciÃ³n, funciÃ³n de pÃ©rdida,  regulaciÃ³n, etc. BÃ¡sicamente explicar quÃ© es lo que veremos en la siguiente secciÃ³n.\n",
        "(**1 punto**)\n",
        "\n",
        "6.\t**Experimentos**: Reportar todos sus experimentos y cÃ³digo en esta secciÃ³n. Comparar los resultados obtenidos utilizando diferentes modelos. Â¡Es vital haber realizado varios experimentos para sacar una buena nota! (**2.0 puntos**)\n",
        "\n",
        "7.\t**Conclusiones**: Discutir resultados, proponer trabajo futuro. (**1 punto**)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Pha0UE7xwdD-"
      },
      "source": [
        "# **Entregable.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lrtAdFcHwfvH"
      },
      "source": [
        "## **IntroducciÃ³n**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K0v5GBSBwj_f"
      },
      "source": [
        "    Escriba su introducciÃ³n aquÃ­"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gbFqV96DwnPm"
      },
      "source": [
        "## **Modelos**\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-IXjMIIgwqIF"
      },
      "source": [
        "## **MÃ©tricas de evaluaciÃ³n**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V-wEYlkXwv8O"
      },
      "source": [
        "- **MÃ©trica estricta:**\n",
        "- **Precision:**\n",
        "- **Recall:**\n",
        "- **Micro F1 score:**\n",
        "Recuerde hacer la distinciÃ³n entre lo que serÃ­a una mÃ©trica de micro f1-score vs macro f1-score."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xN5n3R0kwwxn"
      },
      "source": [
        "## **DiseÃ±o experimental**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HLi3WSMxwzIu"
      },
      "source": [
        "    DescripciÃ³n de la metodologÃ­a"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o1_3E5Z3w2Ro"
      },
      "source": [
        "## **Experimentos**\n",
        "\n",
        "\n",
        "El cÃ³digo que les entregaremos servirÃ¡ de baseline para luego implementar mejores modelos.\n",
        "En general, el cÃ³digo asociado a la carga de los datos, las funciones de entrenamiento, de evaluaciÃ³n y la predicciÃ³n de los datos de la competencia no deberÃ­an cambiar.\n",
        "Solo deben preocuparse de cambiar la arquitectura del modelo, sus hiperparÃ¡metros y reportar, lo cual lo pueden hacer en las subsecciones *modelos*."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6P7r9w-ew7on"
      },
      "source": [
        "###  **Carga de datos y Preprocesamiento**\n",
        "\n",
        "Para cargar los datos y preprocesarlos usaremos la librerÃ­a [`torchtext`](https://github.com/pytorch/text). Tener cuidado ya que hace algunos meses esta librerÃ­a tuvo cambios radicales, quedando las funcionalidades pasadas en un nuevo paquete llamado legacy. Esto ya que si quieren usar mÃ¡s funciones de la librerÃ­a entonces vean los cambios en la documentaciÃ³n.\n",
        "\n",
        "En particular usaremos su mÃ³dulo `data`, el cual segÃºn su documentaciÃ³n original provee:\n",
        "\n",
        "    - Ability to describe declaratively how to load a custom NLP dataset that's in a \"normal\" format\n",
        "    - Ability to define a preprocessing pipeline\n",
        "    - Batching, padding, and numericalizing (including building a vocabulary object)\n",
        "    - Wrapper for dataset splits (train, validation, test)\n",
        "\n",
        "\n",
        "El proceso serÃ¡ el siguiente:\n",
        "\n",
        "1. Descargar los datos desde github y examinarlos.\n",
        "2. Definir los campos (`fields`) que cargaremos desde los archivos.\n",
        "3. Cargar los datasets.\n",
        "4. Crear el vocabulario."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2DrHr4q76LGS"
      },
      "source": [
        "## **InstalaciÃ³n Python 3.8**\n",
        "\n",
        "Para el correcto desarrollo de la competencia usaremos la librerÃ­a ```torchtext```, lamentablemente esta librerÃ­a sufriÃ³ una serie de cambios que hace la API antigua no funcione. Decidimos mantener su debido a que no existen muchos ejemplos de migraciÃ³n de la API antigua a la nueva.\n",
        "\n",
        "Google colab hace poco actualizo la versiÃ³n por default de python a la versiÃ³n 3.10. La versiÃ³n que utilizaremos de ```torchtext``` no compatible con la versiÃ³n 3.10, por lo que es necesario instalar la versiÃ³n 3.8 de Python, para hacerlo deben ejecutar el siguiente comando:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2uShPrC69JC",
        "outputId": "8d2eb4a0-a53c-4d3f-e6f2-af635633e3cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-06-04 19:47:59--  https://repo.anaconda.com/miniconda/Miniconda3-py38_4.8.2-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89817099 (86M) [application/x-sh]\n",
            "Saving to: â€˜mini.shâ€™\n",
            "\n",
            "mini.sh             100%[===================>]  85.66M   373MB/s    in 0.2s    \n",
            "\n",
            "2023-06-04 19:47:59 (373 MB/s) - â€˜mini.shâ€™ saved [89817099/89817099]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\bdone\n",
            "Solving environment: | \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==1.3.0=py38_0\n",
            "    - ca-certificates==2020.1.1=0\n",
            "    - certifi==2019.11.28=py38_0\n",
            "    - cffi==1.14.0=py38h2e261b9_0\n",
            "    - chardet==3.0.4=py38_1003\n",
            "    - conda-package-handling==1.6.0=py38h7b6447c_0\n",
            "    - conda==4.8.2=py38_0\n",
            "    - cryptography==2.8=py38h1ba5d50_0\n",
            "    - idna==2.8=py38_1000\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_0\n",
            "    - openssl==1.1.1d=h7b6447c_4\n",
            "    - pip==20.0.2=py38_1\n",
            "    - pycosat==0.6.3=py38h7b6447c_0\n",
            "    - pycparser==2.19=py_0\n",
            "    - pyopenssl==19.1.0=py38_0\n",
            "    - pysocks==1.7.1=py38_0\n",
            "    - python==3.8.1=h0371630_1\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py38_1\n",
            "    - ruamel_yaml==0.15.87=py38h7b6447c_0\n",
            "    - setuptools==45.2.0=py38_0\n",
            "    - six==1.14.0=py38_0\n",
            "    - sqlite==3.31.1=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.42.1=py_0\n",
            "    - urllib3==1.25.8=py38_0\n",
            "    - wheel==0.34.2=py38_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-1.3.0-py38_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.1.1-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.11.28-py38_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.0-py38h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py38_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.8.2-py38_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py38h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.8-py38h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py38_1000\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_0\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1d-h7b6447c_4\n",
            "  pip                pkgs/main/linux-64::pip-20.0.2-py38_1\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py38h7b6447c_0\n",
            "  pycparser          pkgs/main/noarch::pycparser-2.19-py_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-py38_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py38_0\n",
            "  python             pkgs/main/linux-64::python-3.8.1-h0371630_1\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py38_1\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py38h7b6447c_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-45.2.0-py38_0\n",
            "  six                pkgs/main/linux-64::six-1.14.0-py38_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.31.1-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.42.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py38_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.34.2-py38_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: \\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - jupyter\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-5.1          |            1_gnu          21 KB\n",
            "    anyio-3.5.0                |   py38h06a4308_0         165 KB\n",
            "    argon2-cffi-21.3.0         |     pyhd3eb1b0_0          15 KB\n",
            "    argon2-cffi-bindings-21.2.0|   py38h7f8727e_0          33 KB\n",
            "    asttokens-2.0.5            |     pyhd3eb1b0_0          20 KB\n",
            "    attrs-22.1.0               |   py38h06a4308_0          85 KB\n",
            "    babel-2.11.0               |   py38h06a4308_0         6.8 MB\n",
            "    backcall-0.2.0             |     pyhd3eb1b0_0          13 KB\n",
            "    beautifulsoup4-4.12.2      |   py38h06a4308_0         209 KB\n",
            "    bleach-4.1.0               |     pyhd3eb1b0_0         123 KB\n",
            "    ca-certificates-2023.01.10 |       h06a4308_0         120 KB\n",
            "    certifi-2023.5.7           |   py38h06a4308_0         152 KB\n",
            "    comm-0.1.2                 |   py38h06a4308_0          13 KB\n",
            "    dbus-1.13.18               |       hb2f20db_0         504 KB\n",
            "    debugpy-1.5.1              |   py38h295c915_0         1.7 MB\n",
            "    decorator-5.1.1            |     pyhd3eb1b0_0          12 KB\n",
            "    defusedxml-0.7.1           |     pyhd3eb1b0_0          23 KB\n",
            "    entrypoints-0.4            |   py38h06a4308_0          16 KB\n",
            "    executing-0.8.3            |     pyhd3eb1b0_0          18 KB\n",
            "    expat-2.4.9                |       h6a678d5_0         156 KB\n",
            "    fontconfig-2.14.1          |       h4c34cd2_2         281 KB\n",
            "    freetype-2.12.1            |       h4a9f257_0         626 KB\n",
            "    giflib-5.2.1               |       h5eee18b_3          80 KB\n",
            "    glib-2.69.1                |       he621ea3_2         1.9 MB\n",
            "    gst-plugins-base-1.14.1    |       h6a678d5_1         2.2 MB\n",
            "    gstreamer-1.14.1           |       h5eee18b_1         1.7 MB\n",
            "    icu-58.2                   |       he6710b0_3        10.5 MB\n",
            "    importlib-metadata-6.0.0   |   py38h06a4308_0          38 KB\n",
            "    importlib_metadata-6.0.0   |       hd3eb1b0_0           8 KB\n",
            "    importlib_resources-5.2.0  |     pyhd3eb1b0_1          21 KB\n",
            "    ipykernel-6.19.2           |   py38hb070fc8_0         218 KB\n",
            "    ipython-8.12.0             |   py38h06a4308_0         1.1 MB\n",
            "    ipython_genutils-0.2.0     |     pyhd3eb1b0_1          27 KB\n",
            "    ipywidgets-8.0.4           |   py38h06a4308_0         194 KB\n",
            "    jedi-0.18.1                |   py38h06a4308_1         982 KB\n",
            "    jinja2-3.1.2               |   py38h06a4308_0         211 KB\n",
            "    jpeg-9e                    |       h5eee18b_1         262 KB\n",
            "    json5-0.9.6                |     pyhd3eb1b0_0          21 KB\n",
            "    jsonschema-4.17.3          |   py38h06a4308_0         140 KB\n",
            "    jupyter-1.0.0              |   py38h06a4308_8           7 KB\n",
            "    jupyter_client-8.1.0       |   py38h06a4308_0         178 KB\n",
            "    jupyter_console-6.6.3      |   py38h06a4308_0          45 KB\n",
            "    jupyter_core-5.3.0         |   py38h06a4308_0          89 KB\n",
            "    jupyter_server-1.23.4      |   py38h06a4308_0         382 KB\n",
            "    jupyterlab-3.5.3           |   py38h06a4308_0         4.4 MB\n",
            "    jupyterlab_pygments-0.1.2  |             py_0           8 KB\n",
            "    jupyterlab_server-2.10.3   |     pyhd3eb1b0_1          48 KB\n",
            "    jupyterlab_widgets-3.0.5   |   py38h06a4308_0         178 KB\n",
            "    krb5-1.19.4                |       h568e23c_0         1.3 MB\n",
            "    lerc-3.0                   |       h295c915_0         196 KB\n",
            "    libclang-14.0.6            |default_hc6dbbc7_1         137 KB\n",
            "    libclang13-14.0.6          |default_he11475f_1         9.8 MB\n",
            "    libdeflate-1.17            |       h5eee18b_0          69 KB\n",
            "    libedit-3.1.20221030       |       h5eee18b_0         181 KB\n",
            "    libevent-2.1.12            |       h8f2d780_0         425 KB\n",
            "    libffi-3.4.4               |       h6a678d5_0         142 KB\n",
            "    libgcc-ng-11.2.0           |       h1234567_1         5.3 MB\n",
            "    libgomp-11.2.0             |       h1234567_1         474 KB\n",
            "    libllvm14-14.0.6           |       hdb19cb5_3        33.4 MB\n",
            "    libpng-1.6.39              |       h5eee18b_0         304 KB\n",
            "    libpq-12.9                 |       h16c4e8d_3         2.1 MB\n",
            "    libsodium-1.0.18           |       h7b6447c_0         244 KB\n",
            "    libstdcxx-ng-11.2.0        |       h1234567_1         4.7 MB\n",
            "    libtiff-4.5.0              |       h6a678d5_2         479 KB\n",
            "    libuuid-1.41.5             |       h5eee18b_0          27 KB\n",
            "    libwebp-1.2.4              |       h11a3e52_1          86 KB\n",
            "    libwebp-base-1.2.4         |       h5eee18b_1         376 KB\n",
            "    libxcb-1.15                |       h7f8727e_0         505 KB\n",
            "    libxkbcommon-1.0.1         |       h5eee18b_1         590 KB\n",
            "    libxml2-2.10.3             |       hcbfbd50_0         755 KB\n",
            "    libxslt-1.1.37             |       h2085143_0         266 KB\n",
            "    lxml-4.9.2                 |   py38h5eee18b_0         1.5 MB\n",
            "    lz4-c-1.9.4                |       h6a678d5_0         154 KB\n",
            "    markupsafe-2.1.1           |   py38h7f8727e_0          21 KB\n",
            "    matplotlib-inline-0.1.6    |   py38h06a4308_0          16 KB\n",
            "    mistune-0.8.4              |py38h7b6447c_1000          55 KB\n",
            "    nbclassic-0.5.5            |   py38h06a4308_0         6.1 MB\n",
            "    nbclient-0.5.13            |   py38h06a4308_0          91 KB\n",
            "    nbconvert-6.5.4            |   py38h06a4308_0         513 KB\n",
            "    nbformat-5.7.0             |   py38h06a4308_0         133 KB\n",
            "    ncurses-6.4                |       h6a678d5_0         914 KB\n",
            "    nest-asyncio-1.5.6         |   py38h06a4308_0          14 KB\n",
            "    notebook-6.5.4             |   py38h06a4308_0         540 KB\n",
            "    notebook-shim-0.2.2        |   py38h06a4308_0          22 KB\n",
            "    nspr-4.35                  |       h6a678d5_0         244 KB\n",
            "    nss-3.89.1                 |       h6a678d5_0         2.1 MB\n",
            "    openssl-1.1.1t             |       h7f8727e_0         3.7 MB\n",
            "    packaging-23.0             |   py38h06a4308_0          68 KB\n",
            "    pandocfilters-1.5.0        |     pyhd3eb1b0_0          11 KB\n",
            "    parso-0.8.3                |     pyhd3eb1b0_0          70 KB\n",
            "    pcre-8.45                  |       h295c915_0         207 KB\n",
            "    pexpect-4.8.0              |     pyhd3eb1b0_3          53 KB\n",
            "    pickleshare-0.7.5          |  pyhd3eb1b0_1003          13 KB\n",
            "    pkgutil-resolve-name-1.3.10|   py38h06a4308_0           9 KB\n",
            "    platformdirs-2.5.2         |   py38h06a4308_0          23 KB\n",
            "    ply-3.11                   |           py38_0          81 KB\n",
            "    prometheus_client-0.14.1   |   py38h06a4308_0          90 KB\n",
            "    prompt-toolkit-3.0.36      |   py38h06a4308_0         574 KB\n",
            "    prompt_toolkit-3.0.36      |       hd3eb1b0_0           5 KB\n",
            "    psutil-5.9.0               |   py38h5eee18b_0         330 KB\n",
            "    ptyprocess-0.7.0           |     pyhd3eb1b0_2          17 KB\n",
            "    pure_eval-0.2.2            |     pyhd3eb1b0_0          14 KB\n",
            "    pygments-2.15.1            |   py38h06a4308_1         1.8 MB\n",
            "    pyqt-5.15.7                |   py38h6a678d5_1         5.1 MB\n",
            "    pyqt5-sip-12.11.0          |   py38h6a678d5_1          87 KB\n",
            "    pyrsistent-0.18.0          |   py38heee7806_0          94 KB\n",
            "    python-3.8.16              |       h7a1cb2a_3        23.7 MB\n",
            "    python-dateutil-2.8.2      |     pyhd3eb1b0_0         233 KB\n",
            "    python-fastjsonschema-2.16.2|   py38h06a4308_0         230 KB\n",
            "    pytz-2022.7                |   py38h06a4308_0         209 KB\n",
            "    pyzmq-25.0.2               |   py38h6a678d5_0         462 KB\n",
            "    qt-main-5.15.2             |       h8373d8f_8        53.7 MB\n",
            "    qt-webengine-5.15.9        |       hbbf29b9_6        49.2 MB\n",
            "    qtconsole-5.4.2            |   py38h06a4308_0         191 KB\n",
            "    qtpy-2.2.0                 |   py38h06a4308_0          84 KB\n",
            "    qtwebkit-5.212             |       h3fafdc1_5        16.2 MB\n",
            "    readline-8.2               |       h5eee18b_0         357 KB\n",
            "    send2trash-1.8.0           |     pyhd3eb1b0_1          19 KB\n",
            "    sip-6.6.2                  |   py38h6a678d5_0         425 KB\n",
            "    sniffio-1.2.0              |   py38h06a4308_1          15 KB\n",
            "    soupsieve-2.4              |   py38h06a4308_0          69 KB\n",
            "    sqlite-3.41.2              |       h5eee18b_0         1.2 MB\n",
            "    stack_data-0.2.0           |     pyhd3eb1b0_0          22 KB\n",
            "    terminado-0.17.1           |   py38h06a4308_0          31 KB\n",
            "    tinycss2-1.2.1             |   py38h06a4308_0          40 KB\n",
            "    tk-8.6.12                  |       h1ccaba5_0         3.0 MB\n",
            "    toml-0.10.2                |     pyhd3eb1b0_0          20 KB\n",
            "    tomli-2.0.1                |   py38h06a4308_0          24 KB\n",
            "    tornado-6.2                |   py38h5eee18b_0         590 KB\n",
            "    traitlets-5.7.1            |   py38h06a4308_0         200 KB\n",
            "    typing-extensions-4.5.0    |   py38h06a4308_0           9 KB\n",
            "    typing_extensions-4.5.0    |   py38h06a4308_0          46 KB\n",
            "    wcwidth-0.2.5              |     pyhd3eb1b0_0          26 KB\n",
            "    webencodings-0.5.1         |           py38_1          20 KB\n",
            "    websocket-client-0.58.0    |   py38h06a4308_4          66 KB\n",
            "    widgetsnbextension-4.0.5   |   py38h06a4308_0         875 KB\n",
            "    xz-5.4.2                   |       h5eee18b_0         642 KB\n",
            "    zeromq-4.3.4               |       h2531618_0         331 KB\n",
            "    zipp-3.11.0                |   py38h06a4308_0          19 KB\n",
            "    zlib-1.2.13                |       h5eee18b_0         103 KB\n",
            "    zstd-1.5.5                 |       hc292b87_0         647 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       275.0 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu\n",
            "  anyio              pkgs/main/linux-64::anyio-3.5.0-py38h06a4308_0\n",
            "  argon2-cffi        pkgs/main/noarch::argon2-cffi-21.3.0-pyhd3eb1b0_0\n",
            "  argon2-cffi-bindi~ pkgs/main/linux-64::argon2-cffi-bindings-21.2.0-py38h7f8727e_0\n",
            "  asttokens          pkgs/main/noarch::asttokens-2.0.5-pyhd3eb1b0_0\n",
            "  attrs              pkgs/main/linux-64::attrs-22.1.0-py38h06a4308_0\n",
            "  babel              pkgs/main/linux-64::babel-2.11.0-py38h06a4308_0\n",
            "  backcall           pkgs/main/noarch::backcall-0.2.0-pyhd3eb1b0_0\n",
            "  beautifulsoup4     pkgs/main/linux-64::beautifulsoup4-4.12.2-py38h06a4308_0\n",
            "  bleach             pkgs/main/noarch::bleach-4.1.0-pyhd3eb1b0_0\n",
            "  comm               pkgs/main/linux-64::comm-0.1.2-py38h06a4308_0\n",
            "  dbus               pkgs/main/linux-64::dbus-1.13.18-hb2f20db_0\n",
            "  debugpy            pkgs/main/linux-64::debugpy-1.5.1-py38h295c915_0\n",
            "  decorator          pkgs/main/noarch::decorator-5.1.1-pyhd3eb1b0_0\n",
            "  defusedxml         pkgs/main/noarch::defusedxml-0.7.1-pyhd3eb1b0_0\n",
            "  entrypoints        pkgs/main/linux-64::entrypoints-0.4-py38h06a4308_0\n",
            "  executing          pkgs/main/noarch::executing-0.8.3-pyhd3eb1b0_0\n",
            "  expat              pkgs/main/linux-64::expat-2.4.9-h6a678d5_0\n",
            "  fontconfig         pkgs/main/linux-64::fontconfig-2.14.1-h4c34cd2_2\n",
            "  freetype           pkgs/main/linux-64::freetype-2.12.1-h4a9f257_0\n",
            "  giflib             pkgs/main/linux-64::giflib-5.2.1-h5eee18b_3\n",
            "  glib               pkgs/main/linux-64::glib-2.69.1-he621ea3_2\n",
            "  gst-plugins-base   pkgs/main/linux-64::gst-plugins-base-1.14.1-h6a678d5_1\n",
            "  gstreamer          pkgs/main/linux-64::gstreamer-1.14.1-h5eee18b_1\n",
            "  icu                pkgs/main/linux-64::icu-58.2-he6710b0_3\n",
            "  importlib-metadata pkgs/main/linux-64::importlib-metadata-6.0.0-py38h06a4308_0\n",
            "  importlib_metadata pkgs/main/noarch::importlib_metadata-6.0.0-hd3eb1b0_0\n",
            "  importlib_resourc~ pkgs/main/noarch::importlib_resources-5.2.0-pyhd3eb1b0_1\n",
            "  ipykernel          pkgs/main/linux-64::ipykernel-6.19.2-py38hb070fc8_0\n",
            "  ipython            pkgs/main/linux-64::ipython-8.12.0-py38h06a4308_0\n",
            "  ipython_genutils   pkgs/main/noarch::ipython_genutils-0.2.0-pyhd3eb1b0_1\n",
            "  ipywidgets         pkgs/main/linux-64::ipywidgets-8.0.4-py38h06a4308_0\n",
            "  jedi               pkgs/main/linux-64::jedi-0.18.1-py38h06a4308_1\n",
            "  jinja2             pkgs/main/linux-64::jinja2-3.1.2-py38h06a4308_0\n",
            "  jpeg               pkgs/main/linux-64::jpeg-9e-h5eee18b_1\n",
            "  json5              pkgs/main/noarch::json5-0.9.6-pyhd3eb1b0_0\n",
            "  jsonschema         pkgs/main/linux-64::jsonschema-4.17.3-py38h06a4308_0\n",
            "  jupyter            pkgs/main/linux-64::jupyter-1.0.0-py38h06a4308_8\n",
            "  jupyter_client     pkgs/main/linux-64::jupyter_client-8.1.0-py38h06a4308_0\n",
            "  jupyter_console    pkgs/main/linux-64::jupyter_console-6.6.3-py38h06a4308_0\n",
            "  jupyter_core       pkgs/main/linux-64::jupyter_core-5.3.0-py38h06a4308_0\n",
            "  jupyter_server     pkgs/main/linux-64::jupyter_server-1.23.4-py38h06a4308_0\n",
            "  jupyterlab         pkgs/main/linux-64::jupyterlab-3.5.3-py38h06a4308_0\n",
            "  jupyterlab_pygmen~ pkgs/main/noarch::jupyterlab_pygments-0.1.2-py_0\n",
            "  jupyterlab_server  pkgs/main/noarch::jupyterlab_server-2.10.3-pyhd3eb1b0_1\n",
            "  jupyterlab_widgets pkgs/main/linux-64::jupyterlab_widgets-3.0.5-py38h06a4308_0\n",
            "  krb5               pkgs/main/linux-64::krb5-1.19.4-h568e23c_0\n",
            "  lerc               pkgs/main/linux-64::lerc-3.0-h295c915_0\n",
            "  libclang           pkgs/main/linux-64::libclang-14.0.6-default_hc6dbbc7_1\n",
            "  libclang13         pkgs/main/linux-64::libclang13-14.0.6-default_he11475f_1\n",
            "  libdeflate         pkgs/main/linux-64::libdeflate-1.17-h5eee18b_0\n",
            "  libevent           pkgs/main/linux-64::libevent-2.1.12-h8f2d780_0\n",
            "  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1\n",
            "  libllvm14          pkgs/main/linux-64::libllvm14-14.0.6-hdb19cb5_3\n",
            "  libpng             pkgs/main/linux-64::libpng-1.6.39-h5eee18b_0\n",
            "  libpq              pkgs/main/linux-64::libpq-12.9-h16c4e8d_3\n",
            "  libsodium          pkgs/main/linux-64::libsodium-1.0.18-h7b6447c_0\n",
            "  libtiff            pkgs/main/linux-64::libtiff-4.5.0-h6a678d5_2\n",
            "  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0\n",
            "  libwebp            pkgs/main/linux-64::libwebp-1.2.4-h11a3e52_1\n",
            "  libwebp-base       pkgs/main/linux-64::libwebp-base-1.2.4-h5eee18b_1\n",
            "  libxcb             pkgs/main/linux-64::libxcb-1.15-h7f8727e_0\n",
            "  libxkbcommon       pkgs/main/linux-64::libxkbcommon-1.0.1-h5eee18b_1\n",
            "  libxml2            pkgs/main/linux-64::libxml2-2.10.3-hcbfbd50_0\n",
            "  libxslt            pkgs/main/linux-64::libxslt-1.1.37-h2085143_0\n",
            "  lxml               pkgs/main/linux-64::lxml-4.9.2-py38h5eee18b_0\n",
            "  lz4-c              pkgs/main/linux-64::lz4-c-1.9.4-h6a678d5_0\n",
            "  markupsafe         pkgs/main/linux-64::markupsafe-2.1.1-py38h7f8727e_0\n",
            "  matplotlib-inline  pkgs/main/linux-64::matplotlib-inline-0.1.6-py38h06a4308_0\n",
            "  mistune            pkgs/main/linux-64::mistune-0.8.4-py38h7b6447c_1000\n",
            "  nbclassic          pkgs/main/linux-64::nbclassic-0.5.5-py38h06a4308_0\n",
            "  nbclient           pkgs/main/linux-64::nbclient-0.5.13-py38h06a4308_0\n",
            "  nbconvert          pkgs/main/linux-64::nbconvert-6.5.4-py38h06a4308_0\n",
            "  nbformat           pkgs/main/linux-64::nbformat-5.7.0-py38h06a4308_0\n",
            "  nest-asyncio       pkgs/main/linux-64::nest-asyncio-1.5.6-py38h06a4308_0\n",
            "  notebook           pkgs/main/linux-64::notebook-6.5.4-py38h06a4308_0\n",
            "  notebook-shim      pkgs/main/linux-64::notebook-shim-0.2.2-py38h06a4308_0\n",
            "  nspr               pkgs/main/linux-64::nspr-4.35-h6a678d5_0\n",
            "  nss                pkgs/main/linux-64::nss-3.89.1-h6a678d5_0\n",
            "  packaging          pkgs/main/linux-64::packaging-23.0-py38h06a4308_0\n",
            "  pandocfilters      pkgs/main/noarch::pandocfilters-1.5.0-pyhd3eb1b0_0\n",
            "  parso              pkgs/main/noarch::parso-0.8.3-pyhd3eb1b0_0\n",
            "  pcre               pkgs/main/linux-64::pcre-8.45-h295c915_0\n",
            "  pexpect            pkgs/main/noarch::pexpect-4.8.0-pyhd3eb1b0_3\n",
            "  pickleshare        pkgs/main/noarch::pickleshare-0.7.5-pyhd3eb1b0_1003\n",
            "  pkgutil-resolve-n~ pkgs/main/linux-64::pkgutil-resolve-name-1.3.10-py38h06a4308_0\n",
            "  platformdirs       pkgs/main/linux-64::platformdirs-2.5.2-py38h06a4308_0\n",
            "  ply                pkgs/main/linux-64::ply-3.11-py38_0\n",
            "  prometheus_client  pkgs/main/linux-64::prometheus_client-0.14.1-py38h06a4308_0\n",
            "  prompt-toolkit     pkgs/main/linux-64::prompt-toolkit-3.0.36-py38h06a4308_0\n",
            "  prompt_toolkit     pkgs/main/noarch::prompt_toolkit-3.0.36-hd3eb1b0_0\n",
            "  psutil             pkgs/main/linux-64::psutil-5.9.0-py38h5eee18b_0\n",
            "  ptyprocess         pkgs/main/noarch::ptyprocess-0.7.0-pyhd3eb1b0_2\n",
            "  pure_eval          pkgs/main/noarch::pure_eval-0.2.2-pyhd3eb1b0_0\n",
            "  pygments           pkgs/main/linux-64::pygments-2.15.1-py38h06a4308_1\n",
            "  pyqt               pkgs/main/linux-64::pyqt-5.15.7-py38h6a678d5_1\n",
            "  pyqt5-sip          pkgs/main/linux-64::pyqt5-sip-12.11.0-py38h6a678d5_1\n",
            "  pyrsistent         pkgs/main/linux-64::pyrsistent-0.18.0-py38heee7806_0\n",
            "  python-dateutil    pkgs/main/noarch::python-dateutil-2.8.2-pyhd3eb1b0_0\n",
            "  python-fastjsonsc~ pkgs/main/linux-64::python-fastjsonschema-2.16.2-py38h06a4308_0\n",
            "  pytz               pkgs/main/linux-64::pytz-2022.7-py38h06a4308_0\n",
            "  pyzmq              pkgs/main/linux-64::pyzmq-25.0.2-py38h6a678d5_0\n",
            "  qt-main            pkgs/main/linux-64::qt-main-5.15.2-h8373d8f_8\n",
            "  qt-webengine       pkgs/main/linux-64::qt-webengine-5.15.9-hbbf29b9_6\n",
            "  qtconsole          pkgs/main/linux-64::qtconsole-5.4.2-py38h06a4308_0\n",
            "  qtpy               pkgs/main/linux-64::qtpy-2.2.0-py38h06a4308_0\n",
            "  qtwebkit           pkgs/main/linux-64::qtwebkit-5.212-h3fafdc1_5\n",
            "  send2trash         pkgs/main/noarch::send2trash-1.8.0-pyhd3eb1b0_1\n",
            "  sip                pkgs/main/linux-64::sip-6.6.2-py38h6a678d5_0\n",
            "  sniffio            pkgs/main/linux-64::sniffio-1.2.0-py38h06a4308_1\n",
            "  soupsieve          pkgs/main/linux-64::soupsieve-2.4-py38h06a4308_0\n",
            "  stack_data         pkgs/main/noarch::stack_data-0.2.0-pyhd3eb1b0_0\n",
            "  terminado          pkgs/main/linux-64::terminado-0.17.1-py38h06a4308_0\n",
            "  tinycss2           pkgs/main/linux-64::tinycss2-1.2.1-py38h06a4308_0\n",
            "  toml               pkgs/main/noarch::toml-0.10.2-pyhd3eb1b0_0\n",
            "  tomli              pkgs/main/linux-64::tomli-2.0.1-py38h06a4308_0\n",
            "  tornado            pkgs/main/linux-64::tornado-6.2-py38h5eee18b_0\n",
            "  traitlets          pkgs/main/linux-64::traitlets-5.7.1-py38h06a4308_0\n",
            "  typing-extensions  pkgs/main/linux-64::typing-extensions-4.5.0-py38h06a4308_0\n",
            "  typing_extensions  pkgs/main/linux-64::typing_extensions-4.5.0-py38h06a4308_0\n",
            "  wcwidth            pkgs/main/noarch::wcwidth-0.2.5-pyhd3eb1b0_0\n",
            "  webencodings       pkgs/main/linux-64::webencodings-0.5.1-py38_1\n",
            "  websocket-client   pkgs/main/linux-64::websocket-client-0.58.0-py38h06a4308_4\n",
            "  widgetsnbextension pkgs/main/linux-64::widgetsnbextension-4.0.5-py38h06a4308_0\n",
            "  zeromq             pkgs/main/linux-64::zeromq-4.3.4-h2531618_0\n",
            "  zipp               pkgs/main/linux-64::zipp-3.11.0-py38h06a4308_0\n",
            "  zstd               pkgs/main/linux-64::zstd-1.5.5-hc292b87_0\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                                2020.1.1-0 --> 2023.01.10-h06a4308_0\n",
            "  certifi                                 2019.11.28-py38_0 --> 2023.5.7-py38h06a4308_0\n",
            "  libedit                           3.1.20181209-hc058e9b_0 --> 3.1.20221030-h5eee18b_0\n",
            "  libffi                                   3.2.1-hd88cf55_4 --> 3.4.4-h6a678d5_0\n",
            "  libgcc-ng                                9.1.0-hdf63c60_0 --> 11.2.0-h1234567_1\n",
            "  libstdcxx-ng                             9.1.0-hdf63c60_0 --> 11.2.0-h1234567_1\n",
            "  ncurses                                    6.2-he6710b0_0 --> 6.4-h6a678d5_0\n",
            "  openssl                                 1.1.1d-h7b6447c_4 --> 1.1.1t-h7f8727e_0\n",
            "  python                                   3.8.1-h0371630_1 --> 3.8.16-h7a1cb2a_3\n",
            "  readline                                   7.0-h7b6447c_5 --> 8.2-h5eee18b_0\n",
            "  sqlite                                  3.31.1-h7b6447c_0 --> 3.41.2-h5eee18b_0\n",
            "  tk                                       8.6.8-hbc83047_0 --> 8.6.12-h1ccaba5_0\n",
            "  xz                                       5.2.4-h14c3975_4 --> 5.4.2-h5eee18b_0\n",
            "  zlib                                    1.2.11-h7b6447c_3 --> 1.2.13-h5eee18b_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... \n",
            "The environment is inconsistent, please check the package plan carefully\n",
            "The following packages are causing the inconsistency:\n",
            "\n",
            "  - defaults/noarch::parso==0.8.3=pyhd3eb1b0_0\n",
            "  - defaults/noarch::defusedxml==0.7.1=pyhd3eb1b0_0\n",
            "  - defaults/linux-64::ruamel_yaml==0.15.87=py38h7b6447c_0\n",
            "  - defaults/linux-64::jupyter_server==1.23.4=py38h06a4308_0\n",
            "  - defaults/noarch::prompt_toolkit==3.0.36=hd3eb1b0_0\n",
            "  - defaults/noarch::argon2-cffi==21.3.0=pyhd3eb1b0_0\n",
            "  - defaults/noarch::stack_data==0.2.0=pyhd3eb1b0_0\n",
            "  - defaults/linux-64::debugpy==1.5.1=py38h295c915_0\n",
            "  - defaults/linux-64::tinycss2==1.2.1=py38h06a4308_0\n",
            "  - defaults/linux-64::tomli==2.0.1=py38h06a4308_0\n",
            "  - defaults/linux-64::pyqt5-sip==12.11.0=py38h6a678d5_1\n",
            "  - defaults/noarch::pandocfilters==1.5.0=pyhd3eb1b0_0\n",
            "  - defaults/linux-64::dbus==1.13.18=hb2f20db_0\n",
            "  - defaults/linux-64::gstreamer==1.14.1=h5eee18b_1\n",
            "  - defaults/linux-64::prompt-toolkit==3.0.36=py38h06a4308_0\n",
            "  - defaults/linux-64::prometheus_client==0.14.1=py38h06a4308_0\n",
            "  - defaults/linux-64::urllib3==1.25.8=py38_0\n",
            "  - defaults/linux-64::notebook==6.5.4=py38h06a4308_0\n",
            "  - defaults/linux-64::conda==4.8.2=py38_0\n",
            "  - defaults/linux-64::gst-plugins-base==1.14.1=h6a678d5_1\n",
            "  - defaults/linux-64::traitlets==5.7.1=py38h06a4308_0\n",
            "  - defaults/linux-64::webencodings==0.5.1=py38_1\n",
            "  - defaults/linux-64::soupsieve==2.4=py38h06a4308_0\n",
            "  - defaults/linux-64::entrypoints==0.4=py38h06a4308_0\n",
            "  - defaults/noarch::importlib_metadata==6.0.0=hd3eb1b0_0\n",
            "  - defaults/linux-64::qtpy==2.2.0=py38h06a4308_0\n",
            "  - defaults/noarch::json5==0.9.6=pyhd3eb1b0_0\n",
            "  - defaults/linux-64::platformdirs==2.5.2=py38h06a4308_0\n",
            "  - defaults/linux-64::libtiff==4.5.0=h6a678d5_2\n",
            "  - defaults/linux-64::pip==20.0.2=py38_1\n",
            "  - defaults/linux-64::zstd==1.5.5=hc292b87_0\n",
            "  - defaults/linux-64::qt-webengine==5.15.9=hbbf29b9_6\n",
            "  - defaults/linux-64::typing-extensions==4.5.0=py38h06a4308_0\n",
            "  - defaults/linux-64::libwebp==1.2.4=h11a3e52_1\n",
            "  - defaults/linux-64::giflib==5.2.1=h5eee18b_3\n",
            "  - defaults/noarch::pure_eval==0.2.2=pyhd3eb1b0_0\n",
            "  - defaults/noarch::toml==0.10.2=pyhd3eb1b0_0\n",
            "  - defaults/linux-64::idna==2.8=py38_1000\n",
            "  - defaults/linux-64::libxcb==1.15=h7f8727e_0\n",
            "  - defaults/linux-64::zipp==3.11.0=py38h06a4308_0\n",
            "  - defaults/noarch::asttokens==2.0.5=pyhd3eb1b0_0\n",
            "  - defaults/linux-64::pyzmq==25.0.2=py38h6a678d5_0\n",
            "  - defaults/linux-64::ipython==8.12.0=py38h06a4308_0\n",
            "  - defaults/linux-64::libgcc-ng==11.2.0=h1234567_1\n",
            "  - defaults/linux-64::requests==2.22.0=py38_1\n",
            "  - defaults/linux-64::freetype==2.12.1=h4a9f257_0\n",
            "  - defaults/linux-64::libstdcxx-ng==11.2.0=h1234567_1\n",
            "  - defaults/linux-64::notebook-shim==0.2.2=py38h06a4308_0\n",
            "  - defaults/linux-64::yaml==0.1.7=had09818_2\n",
            "  - defaults/linux-64::libuuid==1.41.5=h5eee18b_0\n",
            "  - defaults/linux-64::libsodium==1.0.18=h7b6447c_0\n",
            "  - defaults/linux-64::typing_extensions==4.5.0=py38h06a4308_0\n",
            "  - defaults/linux-64::lxml==4.9.2=py38h5eee18b_0\n",
            "  - defaults/linux-64::jinja2==3.1.2=py38h06a4308_0\n",
            "  - defaults/noarch::pycparser==2.19=py_0\n",
            "  - defaults/linux-64::ply==3.11=py38_0\n",
            "  - defaults/linux-64::tornado==6.2=py38h5eee18b_0\n",
            "  - defaults/linux-64::qt-main==5.15.2=h8373d8f_8\n",
            "  - defaults/linux-64::sqlite==3.41.2=h5eee18b_0\n",
            "  - defaults/linux-64::krb5==1.19.4=h568e23c_0\n",
            "  - defaults/linux-64::nbconvert==6.5.4=py38h06a4308_0\n",
            "  - defaults/linux-64::glib==2.69.1=he621ea3_2\n",
            "  - defaults/noarch::ipython_genutils==0.2.0=pyhd3eb1b0_1\n",
            "  - defaults/linux-64::terminado==0.17.1=py38h06a4308_0\n",
            "  - defaults/linux-64::jupyter_client==8.1.0=py38h06a4308_0\n",
            "  - defaults/linux-64::matplotlib-inline==0.1.6=py38h06a4308_0\n",
            "  - defaults/linux-64::libclang==14.0.6=default_hc6dbbc7_1\n",
            "  - defaults/linux-64::argon2-cffi-bindings==21.2.0=py38h7f8727e_0\n",
            "  - defaults/linux-64::nbformat==5.7.0=py38h06a4308_0\n",
            "  - defaults/linux-64::nest-asyncio==1.5.6=py38h06a4308_0\n",
            "  - defaults/linux-64::pyopenssl==19.1.0=py38_0\n",
            "  - defaults/linux-64::libffi==3.4.4=h6a678d5_0\n",
            "  - defaults/linux-64::libllvm14==14.0.6=hdb19cb5_3\n",
            "  - defaults/linux-64::cffi==1.14.0=py38h2e261b9_0\n",
            "  - defaults/linux-64::pyqt==5.15.7=py38h6a678d5_1\n",
            "  - defaults/linux-64::libdeflate==1.17=h5eee18b_0\n",
            "  - defaults/linux-64::python==3.8.16=h7a1cb2a_3\n",
            "  - defaults/linux-64::pygments==2.15.1=py38h06a4308_1\n",
            "  - defaults/linux-64::zlib==1.2.13=h5eee18b_0\n",
            "  - defaults/linux-64::readline==8.2=h5eee18b_0\n",
            "  - defaults/linux-64::nbclassic==0.5.5=py38h06a4308_0\n",
            "  - defaults/linux-64::ipywidgets==8.0.4=py38h06a4308_0\n",
            "  - defaults/linux-64::libxslt==1.1.37=h2085143_0\n",
            "  - defaults/linux-64::ncurses==6.4=h6a678d5_0\n",
            "  - defaults/noarch::decorator==5.1.1=pyhd3eb1b0_0\n",
            "  - defaults/linux-64::pycosat==0.6.3=py38h7b6447c_0\n",
            "  - defaults/linux-64::jedi==0.18.1=py38h06a4308_1\n",
            "  - defaults/noarch::pexpect==4.8.0=pyhd3eb1b0_3\n",
            "  - defaults/linux-64::jupyter_console==6.6.3=py38h06a4308_0\n",
            "  - defaults/linux-64::expat==2.4.9=h6a678d5_0\n",
            "  - defaults/linux-64::pyrsistent==0.18.0=py38heee7806_0\n",
            "  - defaults/linux-64::sniffio==1.2.0=py38h06a4308_1\n",
            "  - defaults/linux-64::tk==8.6.12=h1ccaba5_0\n",
            "  - defaults/linux-64::certifi==2023.5.7=py38h06a4308_0\n",
            "  - defaults/linux-64::openssl==1.1.1t=h7f8727e_0\n",
            "  - defaults/linux-64::importlib-metadata==6.0.0=py38h06a4308_0\n",
            "  - defaults/noarch::ptyprocess==0.7.0=pyhd3eb1b0_2\n",
            "  - defaults/linux-64::xz==5.4.2=h5eee18b_0\n",
            "  - defaults/noarch::send2trash==1.8.0=pyhd3eb1b0_1\n",
            "  - defaults/linux-64::mistune==0.8.4=py38h7b6447c_1000\n",
            "  - defaults/linux-64::zeromq==4.3.4=h2531618_0\n",
            "  - defaults/linux-64::chardet==3.0.4=py38_1003\n",
            "  - defaults/linux-64::fontconfig==2.14.1=h4c34cd2_2\n",
            "  - defaults/noarch::python-dateutil==2.8.2=pyhd3eb1b0_0\n",
            "  - defaults/linux-64::comm==0.1.2=py38h06a4308_0\n",
            "  - defaults/noarch::jupyterlab_pygments==0.1.2=py_0\n",
            "  - defaults/linux-64::jupyterlab==3.5.3=py38h06a4308_0\n",
            "  - defaults/linux-64::jupyter_core==5.3.0=py38h06a4308_0\n",
            "  - defaults/noarch::wcwidth==0.2.5=pyhd3eb1b0_0\n",
            "  - defaults/linux-64::jpeg==9e=h5eee18b_1\n",
            "  - defaults/linux-64::libevent==2.1.12=h8f2d780_0\n",
            "  - defaults/linux-64::libpq==12.9=h16c4e8d_3\n",
            "  - defaults/linux-64::pytz==2022.7=py38h06a4308_0\n",
            "  - defaults/linux-64::libwebp-base==1.2.4=h5eee18b_1\n",
            "  - defaults/linux-64::lz4-c==1.9.4=h6a678d5_0\n",
            "  - defaults/noarch::jupyterlab_server==2.10.3=pyhd3eb1b0_1\n",
            "  - defaults/linux-64::setuptools==45.2.0=py38_0\n",
            "  - defaults/linux-64::libxml2==2.10.3=hcbfbd50_0\n",
            "  - defaults/linux-64::qtconsole==5.4.2=py38h06a4308_0\n",
            "  - defaults/noarch::tqdm==4.42.1=py_0\n",
            "  - defaults/linux-64::lerc==3.0=h295c915_0\n",
            "  - defaults/noarch::bleach==4.1.0=pyhd3eb1b0_0\n",
            "  - defaults/linux-64::conda-package-handling==1.6.0=py38h7b6447c_0\n",
            "  - defaults/linux-64::cryptography==2.8=py38h1ba5d50_0\n",
            "  - defaults/linux-64::wheel==0.34.2=py38_0\n",
            "  - defaults/linux-64::jupyter==1.0.0=py38h06a4308_8\n",
            "  - defaults/linux-64::anyio==3.5.0=py38h06a4308_0\n",
            "  - defaults/linux-64::markupsafe==2.1.1=py38h7f8727e_0\n",
            "  - defaults/noarch::backcall==0.2.0=pyhd3eb1b0_0\n",
            "  - defaults/linux-64::pysocks==1.7.1=py38_0\n",
            "  - defaults/linux-64::asn1crypto==1.3.0=py38_0\n",
            "  - defaults/linux-64::libpng==1.6.39=h5eee18b_0\n",
            "  - defaults/linux-64::ipykernel==6.19.2=py38hb070fc8_0\n",
            "  - defaults/noarch::executing==0.8.3=pyhd3eb1b0_0\n",
            "  - defaults/linux-64::nbclient==0.5.13=py38h06a4308_0\n",
            "  - defaults/linux-64::websocket-client==0.58.0=py38h06a4308_4\n",
            "  - defaults/noarch::importlib_resources==5.2.0=pyhd3eb1b0_1\n",
            "  - defaults/linux-64::libedit==3.1.20221030=h5eee18b_0\n",
            "  - defaults/noarch::pickleshare==0.7.5=pyhd3eb1b0_1003\n",
            "  - defaults/linux-64::psutil==5.9.0=py38h5eee18b_0\n",
            "  - defaults/linux-64::libclang13==14.0.6=default_he11475f_1\n",
            "  - defaults/linux-64::python-fastjsonschema==2.16.2=py38h06a4308_0\n",
            "  - defaults/linux-64::pcre==8.45=h295c915_0\n",
            "  - defaults/linux-64::attrs==22.1.0=py38h06a4308_0\n",
            "  - defaults/linux-64::widgetsnbextension==4.0.5=py38h06a4308_0\n",
            "  - defaults/linux-64::beautifulsoup4==4.12.2=py38h06a4308_0\n",
            "  - defaults/linux-64::nss==3.89.1=h6a678d5_0\n",
            "  - defaults/linux-64::babel==2.11.0=py38h06a4308_0\n",
            "  - defaults/linux-64::icu==58.2=he6710b0_3\n",
            "  - defaults/linux-64::pkgutil-resolve-name==1.3.10=py38h06a4308_0\n",
            "  - defaults/linux-64::nspr==4.35=h6a678d5_0\n",
            "  - defaults/linux-64::six==1.14.0=py38_0\n",
            "  - defaults/linux-64::packaging==23.0=py38h06a4308_0\n",
            "  - defaults/linux-64::jsonschema==4.17.3=py38h06a4308_0\n",
            "  - defaults/linux-64::libxkbcommon==1.0.1=h5eee18b_1\n",
            "  - defaults/linux-64::sip==6.6.2=py38h6a678d5_0\n",
            "  - defaults/linux-64::qtwebkit==5.212=h3fafdc1_5\n",
            "  - defaults/linux-64::jupyterlab_widgets==3.0.5=py38h06a4308_0\n",
            "done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - google-colab\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    aiohttp-3.8.1              |   py38h0a891b7_1         574 KB  conda-forge\n",
            "    aiosignal-1.3.1            |     pyhd8ed1ab_0          12 KB  conda-forge\n",
            "    async-timeout-4.0.2        |     pyhd8ed1ab_0           9 KB  conda-forge\n",
            "    ca-certificates-2023.5.7   |       hbcca054_0         145 KB  conda-forge\n",
            "    cachetools-5.3.0           |     pyhd8ed1ab_0          14 KB  conda-forge\n",
            "    charset-normalizer-2.0.4   |     pyhd3eb1b0_0          35 KB\n",
            "    cryptography-3.4.8         |   py38h3e25421_1         1.1 MB  conda-forge\n",
            "    frozenlist-1.3.3           |   py38h5eee18b_0          45 KB\n",
            "    google-auth-2.19.1         |     pyh1a96a4e_0          99 KB  conda-forge\n",
            "    google-colab-1.0.0         |     pyh44b312d_0          77 KB  conda-forge\n",
            "    libblas-3.9.0              |15_linux64_openblas          12 KB  conda-forge\n",
            "    libcblas-3.9.0             |15_linux64_openblas          12 KB  conda-forge\n",
            "    libgfortran-ng-13.1.0      |       h69a702a_0          23 KB  conda-forge\n",
            "    libgfortran5-13.1.0        |       h15d22d2_0         1.4 MB  conda-forge\n",
            "    liblapack-3.9.0            |15_linux64_openblas          12 KB  conda-forge\n",
            "    libopenblas-0.3.20         |pthreads_h78a6416_0        10.1 MB  conda-forge\n",
            "    multidict-6.0.2            |   py38h5eee18b_0          49 KB\n",
            "    numpy-1.22.3               |   py38h99721a1_2         6.8 MB  conda-forge\n",
            "    pandas-1.2.3               |   py38h51da96c_0        12.1 MB  conda-forge\n",
            "    portpicker-1.5.2           |     pyhd8ed1ab_0          17 KB  conda-forge\n",
            "    pyasn1-0.4.8               |             py_0          53 KB  conda-forge\n",
            "    pyasn1-modules-0.2.7       |             py_0          60 KB  conda-forge\n",
            "    pyopenssl-20.0.1           |     pyhd8ed1ab_0          48 KB  conda-forge\n",
            "    python_abi-3.8             |           2_cp38           4 KB  conda-forge\n",
            "    pyu2f-0.1.5                |     pyhd8ed1ab_0          31 KB  conda-forge\n",
            "    rsa-4.9                    |     pyhd8ed1ab_0          29 KB  conda-forge\n",
            "    yarl-1.7.2                 |   py38h0a891b7_2         133 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        32.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  aiohttp            conda-forge/linux-64::aiohttp-3.8.1-py38h0a891b7_1\n",
            "  aiosignal          conda-forge/noarch::aiosignal-1.3.1-pyhd8ed1ab_0\n",
            "  async-timeout      conda-forge/noarch::async-timeout-4.0.2-pyhd8ed1ab_0\n",
            "  cachetools         conda-forge/noarch::cachetools-5.3.0-pyhd8ed1ab_0\n",
            "  charset-normalizer pkgs/main/noarch::charset-normalizer-2.0.4-pyhd3eb1b0_0\n",
            "  frozenlist         pkgs/main/linux-64::frozenlist-1.3.3-py38h5eee18b_0\n",
            "  google-auth        conda-forge/noarch::google-auth-2.19.1-pyh1a96a4e_0\n",
            "  google-colab       conda-forge/noarch::google-colab-1.0.0-pyh44b312d_0\n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-15_linux64_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-15_linux64_openblas\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-13.1.0-h69a702a_0\n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-13.1.0-h15d22d2_0\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-15_linux64_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.20-pthreads_h78a6416_0\n",
            "  multidict          pkgs/main/linux-64::multidict-6.0.2-py38h5eee18b_0\n",
            "  numpy              conda-forge/linux-64::numpy-1.22.3-py38h99721a1_2\n",
            "  pandas             conda-forge/linux-64::pandas-1.2.3-py38h51da96c_0\n",
            "  portpicker         conda-forge/noarch::portpicker-1.5.2-pyhd8ed1ab_0\n",
            "  pyasn1             conda-forge/noarch::pyasn1-0.4.8-py_0\n",
            "  pyasn1-modules     conda-forge/noarch::pyasn1-modules-0.2.7-py_0\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.8-2_cp38\n",
            "  pyu2f              conda-forge/noarch::pyu2f-0.1.5-pyhd8ed1ab_0\n",
            "  rsa                conda-forge/noarch::rsa-4.9-pyhd8ed1ab_0\n",
            "  yarl               conda-forge/linux-64::yarl-1.7.2-py38h0a891b7_2\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2023.5.7-hbcca054_0\n",
            "  cryptography       pkgs/main::cryptography-2.8-py38h1ba5~ --> conda-forge::cryptography-3.4.8-py38h3e25421_1\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-~ --> conda-forge/noarch::pyopenssl-20.0.1-pyhd8ed1ab_0\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py38_4.8.2-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py38\" --user"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2qe63L_27IJS"
      },
      "source": [
        "Esto tardarÃ¡ un poco, asi que deben tener paciencia..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yQDJJPeU7NAH"
      },
      "source": [
        "#TambiÃ©n si cambia de entorno a CPU a GPU, es recomendable que vuelvan a ejecutar el comando para asegurarse que la versiÃ³n 3.8 se haya instalado."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mra7hbPf7arZ"
      },
      "source": [
        "#Luego, de ejecutar el comando es importante **actualizar** la pÃ¡gina del navegador, para reiniciar el kernel."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UM-foLCv7nHf"
      },
      "source": [
        "Luego de haber ejecutado el comando de instalaciÃ³n y haber actualizado la pÃ¡gina, puede asegurarse que las versiÃ³n 3.8 de Python fue instalada con el siguiente comando:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCvuLiog769f",
        "outputId": "510831f6-0c15-410c-f506-6d2c24ec0e43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Current Version:- 3.8.16 (default, Mar  2 2023, 03:21:46) \n",
            "[GCC 11.2.0]\n"
          ]
        }
      ],
      "source": [
        "# Reload the web page and execute this cell\n",
        "import sys\n",
        "print(\"User Current Version:-\", sys.version)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT9uh_p-7-bQ"
      },
      "source": [
        "Luego de esto ya podemos usar la versiÃ³n de torchtext:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4X1c7hM8Jpv",
        "outputId": "f00f6db7-ba60-4450-9be2-965936d3b8e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torchtext==0.10.0 (from versions: 0.1.1, 0.2.0, 0.2.1, 0.2.3, 0.3.1, 0.4.0, 0.5.0, 0.6.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.1, 0.15.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchtext==0.10.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U torchtext==0.10.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AtF7l84w8Mxf"
      },
      "source": [
        "Para asegurar que la versiÃ³n de torchtext quedo instalada correctamente, ejecute la siguiente celda. Si no hay error, significa que la instalaciÃ³n se ejecuto correctamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47_OlELWxA9d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext import data, datasets, legacy\n",
        "\n",
        "# Garantizar reproducibilidad de los experimentos\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvc4X1N08ZGW"
      },
      "source": [
        "#Es importante mencionar que deben ejecutar la instalaciÃ³n cada vez que vuelvan a trabajar en google colab luego de haberlo cerrado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMsK1EKlxF4v"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vxWr0EALxHLt"
      },
      "source": [
        "#### **Obtener datos**\n",
        "\n",
        "Descargamos los datos de entrenamiento, validaciÃ³n y prueba en nuestro directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Vlu2QFOxLUZ",
        "outputId": "3fc6e268-3d10-44f6-d1c2-90576c5684f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-07-06 16:12:04--  https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt\n",
            "Resolving github.com (github.com)... 20.201.28.151\n",
            "Connecting to github.com (github.com)|20.201.28.151|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/77198f00-c145-11eb-83d1-11e647241ab6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230706%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230706T201204Z&X-Amz-Expires=300&X-Amz-Signature=07fc3e424cb2421d7dcf014af00d2180a687530ec65dc75ed2920137f2080f87&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtrain.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-07-06 16:12:04--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/77198f00-c145-11eb-83d1-11e647241ab6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230706%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230706T201204Z&X-Amz-Expires=300&X-Amz-Signature=07fc3e424cb2421d7dcf014af00d2180a687530ec65dc75ed2920137f2080f87&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtrain.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1607913 (1,5M) [application/octet-stream]\n",
            "Saving to: â€˜train.txtâ€™\n",
            "\n",
            "train.txt           100%[===================>]   1,53M  --.-KB/s    in 0,03s   \n",
            "\n",
            "2023-07-06 16:12:05 (59,9 MB/s) - â€˜train.txtâ€™ saved [1607913/1607913]\n",
            "\n",
            "--2023-07-06 16:12:05--  https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt\n",
            "Resolving github.com (github.com)... 20.201.28.151\n",
            "Connecting to github.com (github.com)|20.201.28.151|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/741e9e80-c145-11eb-813a-b9abac0d674c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230706%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230706T201206Z&X-Amz-Expires=300&X-Amz-Signature=464f43987443c58a6876a2ffdaf3c134511378796c6a3b9420755ea464045125&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Ddev.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-07-06 16:12:06--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/741e9e80-c145-11eb-813a-b9abac0d674c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230706%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230706T201206Z&X-Amz-Expires=300&X-Amz-Signature=464f43987443c58a6876a2ffdaf3c134511378796c6a3b9420755ea464045125&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Ddev.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 177166 (173K) [application/octet-stream]\n",
            "Saving to: â€˜dev.txtâ€™\n",
            "\n",
            "dev.txt             100%[===================>] 173,01K  --.-KB/s    in 0,007s  \n",
            "\n",
            "2023-07-06 16:12:06 (23,4 MB/s) - â€˜dev.txtâ€™ saved [177166/177166]\n",
            "\n",
            "--2023-07-06 16:12:06--  https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt\n",
            "Resolving github.com (github.com)... 20.201.28.151\n",
            "Connecting to github.com (github.com)|20.201.28.151|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/75e86200-c145-11eb-94f8-49517311d768?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230706%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230706T201207Z&X-Amz-Expires=300&X-Amz-Signature=0f9081000e50b86809578b37a23d03083d48a8749afe130b1d2af56af4cbc679&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtest.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-07-06 16:12:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/75e86200-c145-11eb-94f8-49517311d768?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230706%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230706T201207Z&X-Amz-Expires=300&X-Amz-Signature=0f9081000e50b86809578b37a23d03083d48a8749afe130b1d2af56af4cbc679&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtest.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 147052 (144K) [application/octet-stream]\n",
            "Saving to: â€˜test.txtâ€™\n",
            "\n",
            "test.txt            100%[===================>] 143,61K   922KB/s    in 0,2s    \n",
            "\n",
            "2023-07-06 16:12:08 (922 KB/s) - â€˜test.txtâ€™ saved [147052/147052]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#%%capture\n",
        "\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt -nc # Dataset de Entrenamiento\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt -nc    # Dataset de ValidaciÃ³n (Para probar y ajustar el modelo)\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt -nc  # Dataset de la Competencia. Estos datos solo contienen los tokens. Â¡Â¡SON LOS QUE DEBEN SER PREDICHOS!!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cl9RbgIKxQ4b"
      },
      "source": [
        "####  **Fields**\n",
        "\n",
        "Un `field`:\n",
        "\n",
        "* Define un tipo de datos junto con instrucciones para convertir el texto a Tensor.\n",
        "* Contiene un objeto `Vocab` que contiene el vocabulario (palabras posibles que puede tomar ese campo).\n",
        "* Contiene otros parÃ¡metros relacionados con la forma en que se debe numericalizar un tipo de datos, como un mÃ©todo de tokenizaciÃ³n y el tipo de Tensor que se debe producir.\n",
        "\n",
        "\n",
        "Analizemos el siguiente cuadro el cual contiene un ejemplo cualquiera de entrenamiento:\n",
        "\n",
        "\n",
        "```\n",
        "El O\n",
        "paciente O\n",
        "padece O\n",
        "de O\n",
        "cancer B-Disease\n",
        "de I-Disease\n",
        "colon I-Disease\n",
        ". O\n",
        "```\n",
        "\n",
        "Cada linea contiene un token y el tipo de entidad asociado en el formato IOB2 ya explicado. Para que `torchtext` pueda cargar estos datos, debemos definir como va a leer y separar los componentes de cada una de las lineas.\n",
        "Para esto, definiremos un field para cada uno de esos componentes: Las palabras (`TEXT`) y las etiquetas o categorÃ­as (`NER_TAGS`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0L3d2MewPkd"
      },
      "outputs": [],
      "source": [
        "# Primer Field: TEXT. Representan los tokens de la secuencia\n",
        "TEXT = legacy.data.Field(lower=False)\n",
        "\n",
        "# Segundo Field: NER_TAGS. Representan los Tags asociados a cada palabra.\n",
        "NER_TAGS = legacy.data.Field(unk_token=None)\n",
        "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd1K84uiwPp7",
        "outputId": "cc1aa8b4-38da-4903-b9e0-cae09ae48ca3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(('text', <torchtext.legacy.data.field.Field at 0x7f53614560d0>),\n",
              " ('nertags', <torchtext.legacy.data.field.Field at 0x7f53614561f0>))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fields"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q9PbEqH5xaOl"
      },
      "source": [
        "####  **SequenceTaggingDataset**\n",
        "\n",
        "`SequenceTaggingDataset` es una clase de torchtext diseÃ±ada para contener datasets de sequence labeling. Los ejemplos que se guarden en una instancia de estos serÃ¡n arreglos de palabras asociados con sus respectivos tags.\n",
        "\n",
        "Por ejemplo, para Part-of-speech tagging:\n",
        "\n",
        "[I, love, PyTorch, .] estarÃ¡ asociado con [PRON, VERB, PROPN, PUNCT]\n",
        "\n",
        "\n",
        "La idea es que usando los fields que definimos antes, le indiquemos a la clase cÃ³mo cargar los datasets de prueba, validaciÃ³n y test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH05e-BkxdGO"
      },
      "outputs": [],
      "source": [
        "train_data, valid_data, test_data = legacy.datasets.SequenceTaggingDataset.splits(\n",
        "    path=\"./\",\n",
        "    train=\"train.txt\",\n",
        "    validation=\"dev.txt\",\n",
        "    test=\"test.txt\",\n",
        "    fields=fields,\n",
        "    encoding=\"utf-8\",\n",
        "    separator=\" \"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7VHs9ioxgkM",
        "outputId": "8eb06e53-7a73-4c02-b433-24f18e076a45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torchtext.legacy.datasets.sequence_tagging.SequenceTaggingDataset at 0x7f5361cbe9a0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmkpl9Xhxim_",
        "outputId": "10d1c20a-c81c-4b61-9cb0-91392a03ff19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numero de ejemplos de entrenamiento: 8025\n",
            "NÃºmero de ejemplos de validaciÃ³n: 891\n",
            "NÃºmero de ejemplos de test (competencia): 992\n"
          ]
        }
      ],
      "source": [
        "print(f\"Numero de ejemplos de entrenamiento: {len(train_data)}\")\n",
        "print(f\"NÃºmero de ejemplos de validaciÃ³n: {len(valid_data)}\")\n",
        "print(f\"NÃºmero de ejemplos de test (competencia): {len(test_data)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cwiilzKVxnvB"
      },
      "source": [
        "Visualizemos un ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd5n1FIRxvUW",
        "outputId": "f5031ec6-87b3-4fdb-9db0-8f46004c55f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('M2', 'O'),\n",
              " ('HISTERECTONIZADA', 'O'),\n",
              " ('(', 'O'),\n",
              " ('2013', 'O'),\n",
              " ('*', 'O'),\n",
              " ('MIOMATOSIS', 'B-Disease'),\n",
              " (')', 'O'),\n",
              " ('OBS', 'O'),\n",
              " ('.', 'O')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "random_item_idx = random.randint(0, len(train_data))\n",
        "random_example = train_data.examples[random_item_idx]\n",
        "list(zip(random_example.text, random_example.nertags))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XULa23IAxyVd"
      },
      "source": [
        "#### **Construir los vocabularios para el texto y las etiquetas**\n",
        "\n",
        "Los vocabularios son los objetos que contienen todos los tokens (de entrenamiento) posibles para ambos fields. El siguiente paso consiste en construirlos. Para esto, hacemos uso del mÃ©todo `Field.build_vocab` sobre cada uno de nuestros `fields`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbwnA-vLx02F"
      },
      "outputs": [],
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "NER_TAGS.build_vocab(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTcpZtwyx3Nl",
        "outputId": "0351e7bd-3b78-405a-96bd-b68541a851f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens Ãºnicos en TEXT: 17591\n",
            "Tokens Ãºnicos en NER_TAGS: 12\n"
          ]
        }
      ],
      "source": [
        "print(f\"Tokens Ãºnicos en TEXT: {len(TEXT.vocab)}\")\n",
        "print(f\"Tokens Ãºnicos en NER_TAGS: {len(NER_TAGS.vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF_EKeOkx5Xf",
        "outputId": "6e36245c-2144-4e76-8251-6be44ea42132"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " 'O',\n",
              " 'I-Disease',\n",
              " 'B-Disease',\n",
              " 'I-Body_Part',\n",
              " 'B-Body_Part',\n",
              " 'B-Procedure',\n",
              " 'I-Procedure',\n",
              " 'B-Medication',\n",
              " 'B-Family_Member',\n",
              " 'I-Medication',\n",
              " 'I-Family_Member']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Veamos las posibles etiquetas que hemos cargado:\n",
        "NER_TAGS.vocab.itos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OP9EUPdcx79A"
      },
      "source": [
        "\n",
        "\n",
        "Observen que ademas de los tags NER, tenemos <pad>, el cual es generado por el dataloader para cumplir con el padding de cada oraciÃ³n.\n",
        "\n",
        "Veamos ahora los tokens mas frecuentes y especiales:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL2SVMcGx9-K",
        "outputId": "cea8dbb3-e86f-41ef-b67f-cec3da038705"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('.', 7396),\n",
              " (',', 6821),\n",
              " ('-', 4985),\n",
              " ('de', 3811),\n",
              " ('DE', 3645),\n",
              " ('/', 2317),\n",
              " (':', 2209),\n",
              " ('con', 1484),\n",
              " ('y', 1439),\n",
              " ('APS', 1429)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tokens mas frecuentes (SerÃ¡ necesario usar stopwords, eliminar sÃ­mbolos o nos entregan informaciÃ³n (?) )\n",
        "TEXT.vocab.freqs.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJiylfsbyAum"
      },
      "outputs": [],
      "source": [
        "# Seteamos algunas variables que nos serÃ¡n de utilidad mas adelante...\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "PAD_TAG_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "O_TAG_IDX = NER_TAGS.vocab.stoi['O']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kZBg5YWJyDSF"
      },
      "source": [
        "#### **Frecuencia de los Tags**\n",
        "\n",
        "Visualizemos rÃ¡pidamente las cantidades y frecuencias de cada tag:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EsxiaW7yFUA",
        "outputId": "d1cbe1a1-8372-46d9-edd4-57184d72fccf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tag Ocurrencia Porcentaje\n",
            "\n",
            "O\t101671\t68.1%\n",
            "I-Disease\t21629\t14.5%\n",
            "B-Disease\t8831\t 5.9%\n",
            "I-Body_Part\t6489\t 4.3%\n",
            "B-Body_Part\t3755\t 2.5%\n",
            "B-Procedure\t2891\t 1.9%\n",
            "I-Procedure\t2819\t 1.9%\n",
            "B-Medication\t784\t 0.5%\n",
            "B-Family_Member\t228\t 0.2%\n",
            "I-Medication\t116\t 0.1%\n",
            "I-Family_Member\t9\t 0.0%\n"
          ]
        }
      ],
      "source": [
        "def tag_percentage(tag_counts):\n",
        "\n",
        "    total_count = sum([count for tag, count in tag_counts])\n",
        "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
        "\n",
        "    return tag_counts_percentages\n",
        "\n",
        "print(\"Tag Ocurrencia Porcentaje\\n\")\n",
        "\n",
        "for tag, count, percent in tag_percentage(NER_TAGS.vocab.freqs.most_common()):\n",
        "    print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4YTIN8iyIJp"
      },
      "source": [
        "#### **Configuramos pytorch y dividimos los datos.**\n",
        "\n",
        "Importante: si tienes problemas con la ram de la gpu, disminuye el tamaÃ±o de los batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQauqu8zyKeT",
        "outputId": "ae000e61-f804-4caf-a139-be646998853c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 22  # disminuir si hay problemas de ram.\n",
        "\n",
        "# Usar cuda si es que estÃ¡ disponible.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using', device)\n",
        "\n",
        "# Dividir datos entre entrenamiento y test. Si van a hacer algÃºn sort no puede ser sobre\n",
        "# el conjunto de testing ya que al hacer sus predicciones sobre el conjunto de test sin etiquetas\n",
        "# debe conservar el orden original para ser comparado con los golden_labels.\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = legacy.data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort=False,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wAZAUwUD0QG2"
      },
      "source": [
        "#### **MÃ©tricas de evaluaciÃ³n**\n",
        "\n",
        "AdemÃ¡s, definiremos las mÃ©tricas que serÃ¡n usadas tanto para la competencia como para evaluar el modelo: `precision`, `recall` y `micro f1-score`.\n",
        "**Importante**: Noten que la evaluaciÃ³n solo se hace para las Named Entities (sin contar 'O'), toda esta funcionalidad nos la entrega la librerÃ­a seqeval, pueden revisar mÃ¡s documentaciÃ³n aquÃ­: https://github.com/chakki-works/seqeval. No utilicen el cÃ³digo entregado por sklearn para calcular las mÃ©tricas ya que esta lo hace a nivel de token y no a nivel de entidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2_9HAEH0TiX",
        "outputId": "57a640c0-36dd-4100-f863-a937bf4c5228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/site-packages (from seqeval) (1.22.3)\n",
            "Collecting scikit-learn>=0.21.3\n",
            "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.8 MB 29.0 MB/s \n",
            "\u001b[?25hCollecting scipy>=1.3.2\n",
            "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34.5 MB 24 kB/s \n",
            "\u001b[?25hCollecting joblib>=1.1.1\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 297 kB 52.5 MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=3a8b5ce2147ea8451ef967e490720b63ab88186476ebc5945a9412cd4f38a47f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "Successfully built seqeval\n",
            "Installing collected packages: scipy, joblib, threadpoolctl, scikit-learn, seqeval\n",
            "Successfully installed joblib-1.2.0 scikit-learn-1.2.2 scipy-1.10.1 seqeval-1.2.2 threadpoolctl-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxldlFdA0eIx"
      },
      "outputs": [],
      "source": [
        "# Definimos las mÃ©tricas\n",
        "\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
        "    \"\"\"\n",
        "    Calcula precision, recall y f1 de cada batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
        "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
        "\n",
        "    # filtramos <pad> para calcular los scores.\n",
        "    mask = [(y_true != pad_idx)]\n",
        "    y_pred = y_pred[mask]\n",
        "    y_true = y_true[mask]\n",
        "\n",
        "    # traemos a la cpu\n",
        "    y_pred = y_pred.view(-1).to('cpu').numpy()\n",
        "    y_true = y_true.to('cpu').numpy()\n",
        "    y_pred = [[NER_TAGS.vocab.itos[v] for v in y_pred]]\n",
        "    y_true = [[NER_TAGS.vocab.itos[v] for v in y_true]]\n",
        "\n",
        "    # calcular scores\n",
        "    f1 = f1_score(y_true, y_pred, mode='strict')\n",
        "    precision = precision_score(y_true, y_pred, mode='strict')\n",
        "    recall = recall_score(y_true, y_pred, mode='strict')\n",
        "\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UjK0WiQY0i-2"
      },
      "source": [
        "-------------------\n",
        "\n",
        "### **Modelo Baseline**\n",
        "\n",
        "Teniendo ya cargado los datos, toca definir nuestro modelo. Este baseline tendrÃ¡ una capa de embedding, unas cuantas LSTM y una capa de salida y usarÃ¡ dropout en el entrenamiento.\n",
        "\n",
        "Este constarÃ¡ de los siguientes pasos:\n",
        "\n",
        "1. Definir la clase que contendrÃ¡ la red.\n",
        "2. Definir los hiperparÃ¡metros e inicializar la red.\n",
        "3. Definir el nÃºmero de Ã©pocas de entrenamiento\n",
        "4. Definir la funciÃ³n de loss.\n",
        "\n",
        "\n",
        "\n",
        "Recomendamos que para experimentar, encapsules los modelos en una sola variable y luego la fijes en model para entrenarla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlCQC1xZ0m1F"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 embedding_dim,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout,\n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx,\n",
        "                                      )\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g6UD9WhO0t_e"
      },
      "source": [
        "#### **HiperparÃ¡metros de la red**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLaQOkkE0weV"
      },
      "outputs": [],
      "source": [
        "# tamaÃ±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300  # dimensiÃ³n de los embeddings.\n",
        "HIDDEN_DIM = 256  # dimensiÃ³n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # nÃºmero de clases\n",
        "\n",
        "N_LAYERS = 3  # nÃºmero de capas.\n",
        "DROPOUT = 0.6\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "baseline_model_name = 'baseline'  # nombre que tendrÃ¡ el modelo guardado..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMGivXyA0zIv"
      },
      "outputs": [],
      "source": [
        "baseline_n_epochs = 5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_JuKb3Ee01nQ"
      },
      "source": [
        "#### Definimos la funciÃ³n de loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NEqEzma03s3"
      },
      "outputs": [],
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RC-ywch_06ll"
      },
      "source": [
        "--------------------\n",
        "### Modelo 1\n",
        "\n",
        "En estas secciones pueden implementar nuevas redes al modificar los hiperparÃ¡metros, la cantidad de Ã©pocas de entrenamiento, el tamaÃ±o de los batches, loss, optimizador, etc... como tambiÃ©n definir nuevas arquitecturas de red (mediante la creaciÃ³n de clases nuevas)\n",
        "\n",
        "\n",
        "Al final de estas, hay 4 variables, las cuales deben setear con los modelos, Ã©pocas de entrenamiento, loss y optimizador que deseen probar."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zOvyL_Fs09hT"
      },
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdMOE5Xg0_m2"
      },
      "outputs": [],
      "source": [
        "# model_2 = ...\n",
        "# model_name_2 = ...\n",
        "# n_epochs_2 = ...\n",
        "# loss_2 = ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ocJ_7tPy1Aem"
      },
      "source": [
        "---------------\n",
        "\n",
        "\n",
        "### Modelo 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7thom2u1DQ1"
      },
      "outputs": [],
      "source": [
        "# modelo_3 = ...\n",
        "# model_name_3 = ...\n",
        "# n_epochs_3 = ...\n",
        "# loss_3 = ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HC-6L2mM1IS_"
      },
      "source": [
        "------\n",
        "### **Entrenamos y evaluamos**\n",
        "\n",
        "\n",
        "**Importante** : Fijen el modelo, el nÃºmero de Ã©pocas de entrenamiento, la loss y el optimizador que usarÃ¡n para entrenar y evaluar en las siguientes variables!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4l6rUW-1KEG"
      },
      "outputs": [],
      "source": [
        "model = baseline_model\n",
        "model_name = baseline_model_name\n",
        "criterion = baseline_criterion\n",
        "n_epochs = baseline_n_epochs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oa4KS7WJ1MzX"
      },
      "source": [
        "\n",
        "\n",
        "#### **Inicializamos la red**\n",
        "\n",
        "Iniciamos los pesos de la red de forma aleatoria (Usando una distribuciÃ³n normal).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaJzahg31PTg",
        "outputId": "c20da8fa-3f3a-4b61-9c00-1a6b4ad6afb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NER_RNN(\n",
              "  (embedding): Embedding(17591, 300, padding_idx=1)\n",
              "  (lstm): LSTM(300, 256, num_layers=3, dropout=0.6, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=12, bias=True)\n",
              "  (dropout): Dropout(p=0.6, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def init_weights(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1)\n",
        "\n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-JJG63s1S3X",
        "outputId": "4db918ed-198d-4431-f6ba-9c3d89f0165e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El modelo actual tiene 9,580,160 parÃ¡metros entrenables.\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parÃ¡metros entrenables.')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bWRtT6bG1WPG"
      },
      "source": [
        "Notar que definimos los embeddings que representan a \\<unk\\> y \\<pad\\>  como [0, 0, ..., 0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NgnqmouP1Y2G"
      },
      "source": [
        "#### **Definimos el optimizador**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lawgdTo_1avl"
      },
      "outputs": [],
      "source": [
        "# Optimizador\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAJAdQLx1dEp"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H7iBECZx1dUF"
      },
      "source": [
        "#### **Enviamos el modelo a cuda**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3cSflW71ffF"
      },
      "outputs": [],
      "source": [
        "# Enviamos el modelo y la loss a cuda (en el caso en que estÃ© disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "08L95wML1jd4"
      },
      "source": [
        "#### **Definimos el entrenamiento de la red**\n",
        "\n",
        "Algunos conceptos previos:\n",
        "\n",
        "- `epoch` : una pasada de entrenamiento completa de una dataset.\n",
        "- `batch`: una fracciÃ³n de la Ã©poca. Se utilizan para entrenar mas rÃ¡pidamente la red. (mas eficiente pasar n datos que uno en cada ejecuciÃ³n del backpropagation)\n",
        "\n",
        "Esta funciÃ³n estÃ¡ encargada de entrenar la red en una Ã©poca. Para esto, por cada batch de la Ã©poca actual, predice los tags del texto, calcula su loss y luego hace backpropagation para actualizar los pesos de la red.\n",
        "\n",
        "ObservaciÃ³n: En algunos comentarios aparecerÃ¡ el tamaÃ±o de los tensores entre corchetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTjQEOE71mY2"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Por cada batch del iterador de la Ã©poca:\n",
        "    for batch in iterator:\n",
        "\n",
        "        # Extraemos el texto y los tags del batch que estamos procesado\n",
        "        text = batch.text\n",
        "        tags = batch.nertags\n",
        "\n",
        "        # Reiniciamos los gradientes calculados en la iteraciÃ³n anterior\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Predecimos los tags del texto del batch.\n",
        "        predictions = model(text)\n",
        "\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "\n",
        "        # Reordenamos los datos para calcular la loss\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        tags = tags.view(-1)\n",
        "\n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "\n",
        "\n",
        "\n",
        "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "        loss = criterion(predictions, tags)\n",
        "\n",
        "        # Calculamos el accuracy\n",
        "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "        # Calculamos los gradientes\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizamos los parÃ¡metros de la red\n",
        "        optimizer.step()\n",
        "\n",
        "        # Actualizamos el loss y las mÃ©tricas\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_precision += precision\n",
        "        epoch_recall += recall\n",
        "        epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zanLdDAQ1phV"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H_ATC1ehxlZK"
      },
      "source": [
        "#### **Definimos la funciÃ³n de evaluaciÃ³n**\n",
        "\n",
        "Evalua el rendimiento actual de la red usando los datos de validaciÃ³n.\n",
        "\n",
        "Por cada batch de estos datos, calcula y reporta el loss y las mÃ©tricas asociadas al conjunto de validaciÃ³n.\n",
        "Ya que las mÃ©tricas son calculadas por cada batch, estas son retornadas promediadas por el nÃºmero de batches entregados. (ver linea del return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5Ac4jZ_1wo0"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Indicamos que ahora no guardaremos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # Por cada batch\n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text\n",
        "            tags = batch.nertags\n",
        "\n",
        "            # Predecimos\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            # Calculamos las mÃ©tricas\n",
        "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "            # Actualizamos el loss y las mÃ©tricas\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_precision += precision\n",
        "            epoch_recall += recall\n",
        "            epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQbLNLsf1y2N"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aNhdvEPq11EI"
      },
      "source": [
        "\n",
        "#### **Entrenamiento de la red**\n",
        "\n",
        "En este cuadro de cÃ³digo ejecutaremos el entrenamiento de la red.\n",
        "Para esto, primero definiremos el nÃºmero de Ã©pocas y luego por cada Ã©poca, ejecutaremos `train` y `evaluate`.\n",
        "\n",
        "**Importante: Reiniciar los pesos del modelo**\n",
        "\n",
        "Si ejecutas nuevamente esta celda, se seguira entrenando el mismo modelo una y otra vez.\n",
        "Para reiniciar el modelo se debe ejecutar nuevamente la celda que contiene la funciÃ³n `init_weights`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw2Yj2az13Xk",
        "outputId": "5e2693c1-0d1e-4681-9b39-16e5321a0bb7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.860 | Train f1: 0.38 | Train precision: 0.56 | Train recall: 0.30\n",
            "\t Val. Loss: 0.563 |  Val. f1: 0.59 |  Val. precision: 0.78 | Val. recall: 0.48\n",
            "Epoch: 02 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.495 | Train f1: 0.65 | Train precision: 0.74 | Train recall: 0.59\n",
            "\t Val. Loss: 0.408 |  Val. f1: 0.71 |  Val. precision: 0.78 | Val. recall: 0.66\n",
            "Epoch: 03 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.347 | Train f1: 0.76 | Train precision: 0.80 | Train recall: 0.73\n",
            "\t Val. Loss: 0.357 |  Val. f1: 0.74 |  Val. precision: 0.80 | Val. recall: 0.70\n",
            "Epoch: 04 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.269 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.79\n",
            "\t Val. Loss: 0.353 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.74\n",
            "Epoch: 05 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.212 | Train f1: 0.85 | Train precision: 0.86 | Train recall: 0.84\n",
            "\t Val. Loss: 0.338 |  Val. f1: 0.78 |  Val. precision: 0.79 | Val. recall: 0.77\n"
          ]
        }
      ],
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "    # Entrenar\n",
        "    train_loss, train_precision, train_recall, train_f1 = train(\n",
        "        model, train_iterator, optimizer, criterion)\n",
        "\n",
        "    # Evaluar (valid = validaciÃ³n)\n",
        "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "        model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de cÃ³digo.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "    # Si ya no mejoramos el loss de validaciÃ³n, terminamos de entrenar.\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(\n",
        "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "    )\n",
        "    print(\n",
        "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "    )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eTPzjpQb16-v"
      },
      "source": [
        "**Importante**: Recuerden que el Ãºltimo modelo entrenado no es el mejor (probablemente estÃ© *overfitteado*), si no el que guardamos con la menor loss del conjunto de validaciÃ³n. Este problema lo pueden solucionar con *early stopping*.\n",
        "Para cargar el mejor modelo entrenado, ejecuten la siguiente celda.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAsD01mL19-X",
        "outputId": "60c1ebae-f5f9-404e-d0a4-8eb407491669"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# cargar el mejor modelo entrenado.\n",
        "model.load_state_dict(torch.load('{}.pt'.format(model_name)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYxecBmt1_9B"
      },
      "outputs": [],
      "source": [
        "# Limpiar ram de cuda\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8JSy-Bn2Byp",
        "outputId": "e6867e9f-8671-40dd-b978-daa2b679b2c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val. Loss: 0.338 |  Val. f1: 0.78 | Val. precision: 0.79 | Val. recall: 0.77\n"
          ]
        }
      ],
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, valid_iterator, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iB6d0b5G2D_2"
      },
      "source": [
        "#### **Evaluamos el set de validaciÃ³n con el modelo final**\n",
        "\n",
        "Estos son los resultados de predecir el dataset de evaluaciÃ³n con el *mejor* modelo entrenado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDivR0Yz2Hvf",
        "outputId": "c26c3911-b484-40d4-e513-1176cf241a2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val. Loss: 0.338 |  Val. f1: 0.78 | Val. precision: 0.79 | Val. recall: 0.77\n"
          ]
        }
      ],
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, valid_iterator, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qrrRv9J42Kw6"
      },
      "source": [
        "### **Predecir datos para la competencia**\n",
        "\n",
        "Ahora, a partir de los datos de **test** y nuestro modelo entrenado, vamos a predecir las etiquetas que serÃ¡n evaluadas en la competencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHY9FMzq2MyR"
      },
      "outputs": [],
      "source": [
        "def predict_labels(model, iterator, criterion, fields=fields):\n",
        "\n",
        "    # Extraemos los vocabularios.\n",
        "    text_field = fields[0][1]\n",
        "    nertags_field = fields[1][1]\n",
        "    tags_vocab = nertags_field.vocab.itos\n",
        "    words_vocab = text_field.vocab.itos\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in iterator:\n",
        "\n",
        "            text_batch = batch.text\n",
        "            text_batch = torch.transpose(text_batch, 0, 1).tolist()\n",
        "\n",
        "            # Predecir los tags de las sentences del batch\n",
        "            predictions_batch = model(batch.text)\n",
        "            predictions_batch = torch.transpose(predictions_batch, 0, 1)\n",
        "\n",
        "            # por cada oraciÃ³n predicha:\n",
        "            for sentence, sentence_prediction in zip(text_batch,\n",
        "                                                     predictions_batch):\n",
        "                for word_idx, word_predictions in zip(sentence,\n",
        "                                                      sentence_prediction):\n",
        "                    # Obtener el indice del tag con la probabilidad mas alta.\n",
        "                    argmax_index = word_predictions.topk(1)[1]\n",
        "\n",
        "                    current_tag = tags_vocab[argmax_index]\n",
        "                    # Obtenemos la palabra\n",
        "                    current_word = words_vocab[word_idx]\n",
        "\n",
        "                    if current_word != '<pad>':\n",
        "                        predictions.append([current_word, current_tag])\n",
        "                predictions.append(['EOS', 'EOS'])\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "predictions = predict_labels(model, test_iterator, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRQa1bZQ2Of1",
        "outputId": "f54ec8d3-02a6-4d6d-bc6f-34faf8ce7d40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['FRENILLO', 'B-Body_Part'],\n",
              " ['LABIAL', 'I-Body_Part'],\n",
              " ['SUPERIOR', 'I-Body_Part'],\n",
              " ['<unk>', 'O'],\n",
              " ['EN', 'O'],\n",
              " ['ZONA', 'B-Body_Part'],\n",
              " ['SUPERIOR', 'I-Body_Part'],\n",
              " ['DEL', 'I-Body_Part'],\n",
              " ['REBORDE', 'I-Body_Part'],\n",
              " [',', 'O'],\n",
              " ['DIASTEMA', 'O'],\n",
              " [',', 'O'],\n",
              " ['POSIBLE', 'O'],\n",
              " ['DIFICULTAD', 'O'],\n",
              " ['EN', 'O'],\n",
              " ['ERUPCION', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['SE', 'O'],\n",
              " ['SOLICITA', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['pcte', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['presenta', 'O'],\n",
              " ['lesion', 'O'],\n",
              " ['compatible', 'O'],\n",
              " ['con', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['en', 'O'],\n",
              " ['cara', 'B-Body_Part'],\n",
              " ['interna', 'I-Body_Part'],\n",
              " ['de', 'I-Body_Part'],\n",
              " ['mejilla', 'I-Body_Part'],\n",
              " ['derecha', 'I-Body_Part'],\n",
              " [',', 'O'],\n",
              " ['cerca', 'O'],\n",
              " ['del', 'O'],\n",
              " ['borde', 'B-Body_Part'],\n",
              " ['<unk>', 'I-Body_Part'],\n",
              " ['de', 'I-Body_Part'],\n",
              " ['-', 'I-Body_Part'],\n",
              " ['3', 'I-Body_Part'],\n",
              " ['anos', 'O'],\n",
              " ['de', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " [',', 'O'],\n",
              " ['asintomatico', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['-', 'O'],\n",
              " ['TRASTORNO', 'B-Disease'],\n",
              " ['DE', 'I-Disease'],\n",
              " ['LA', 'I-Disease'],\n",
              " ['REFRACCIÃ“N', 'I-Disease'],\n",
              " [',', 'I-Disease'],\n",
              " ['NO', 'I-Disease'],\n",
              " ['ESPECIFICADO', 'I-Disease'],\n",
              " ['/', 'O'],\n",
              " ['-', 'O'],\n",
              " ['Fundamento', 'O'],\n",
              " ['ClÃ­nico', 'O'],\n",
              " ['APS', 'O'],\n",
              " [':', 'O'],\n",
              " ['Adolescente', 'O'],\n",
              " ['quien', 'O'],\n",
              " ['refiere', 'O'],\n",
              " ['presentar', 'O'],\n",
              " ['disminuciÃ³n', 'O'],\n",
              " ['de', 'O'],\n",
              " ['la', 'O'],\n",
              " ['agudeza', 'O'],\n",
              " ['visual', 'O'],\n",
              " ['Trastorno', 'B-Disease'],\n",
              " ['de', 'I-Disease'],\n",
              " ['la', 'I-Disease'],\n",
              " ['refraccion', 'I-Disease'],\n",
              " [',', 'I-Disease'],\n",
              " ['no', 'I-Disease'],\n",
              " ['especificado', 'I-Disease'],\n",
              " ['EOS', 'EOS'],\n",
              " ['USUARIO', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['68', 'O'],\n",
              " ['ANOS', 'O'],\n",
              " [',', 'O'],\n",
              " ['CON', 'O'],\n",
              " ['PATOLOGIAS', 'O'],\n",
              " ['CRONICAS', 'O'],\n",
              " ['HTA', 'B-Disease'],\n",
              " [',', 'O'],\n",
              " ['DM', 'B-Disease'],\n",
              " [',', 'O'],\n",
              " ['DISLIPIDEMIA', 'B-Disease'],\n",
              " [',', 'O'],\n",
              " ['CA', 'B-Disease'],\n",
              " ['DE', 'I-Disease'],\n",
              " ['COLON', 'I-Disease'],\n",
              " ['EN', 'O'],\n",
              " ['TRATAMIENTO', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['DESDENTADO', 'B-Disease'],\n",
              " ['TOTAL', 'I-Disease'],\n",
              " ['SUPERIOR', 'I-Disease'],\n",
              " ['E', 'I-Disease'],\n",
              " ['INFERIOR', 'I-Disease'],\n",
              " ['EOS', 'EOS'],\n",
              " ['PACIENTE', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['9', 'O'],\n",
              " ['ANOS', 'O'],\n",
              " ['SIN', 'O'],\n",
              " ['ANTECEDENTES', 'O'],\n",
              " ['MORBIDOS', 'O'],\n",
              " ['FAMILIARES', 'O'],\n",
              " ['NI', 'O'],\n",
              " ['PERSONALES', 'O'],\n",
              " ['INFORMADO', 'O'],\n",
              " ['POR', 'O'],\n",
              " ['MADRE', 'B-Family_Member'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['DENTICION', 'B-Body_Part'],\n",
              " ['MIXTA', 'O'],\n",
              " ['2', 'O'],\n",
              " ['FASE', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['PRESENCIA', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['MESIODENS', 'O'],\n",
              " ['UBICADO', 'O'],\n",
              " ['ENTRE', 'O'],\n",
              " ['PZAS', 'B-Body_Part'],\n",
              " ['8', 'I-Body_Part'],\n",
              " ['Y', 'I-Body_Part'],\n",
              " ['9', 'I-Body_Part'],\n",
              " [',', 'O'],\n",
              " ['ERUPCIONADO', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['SOLICITO', 'O'],\n",
              " ['EVALUACION', 'O'],\n",
              " ['Y', 'O'],\n",
              " ['MANEJO', 'O'],\n",
              " ['POR', 'O'],\n",
              " ['ESPECIALISTA', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['-', 'O'],\n",
              " ['ANOMALÃAS', 'B-Disease'],\n",
              " ['DE', 'I-Disease'],\n",
              " ['LA', 'I-Disease'],\n",
              " ['POSICIÃ“N', 'I-Disease'],\n",
              " ['DEL', 'I-Disease'],\n",
              " ['DIENTE', 'I-Disease'],\n",
              " ['/', 'O'],\n",
              " ['-', 'O'],\n",
              " ['Fundamento', 'O'],\n",
              " ['ClÃ­nico', 'O'],\n",
              " ['APS', 'O'],\n",
              " [':', 'O'],\n",
              " ['Paciente', 'O'],\n",
              " ['presenta', 'O'],\n",
              " ['3', 'B-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['8', 'I-Body_Part'],\n",
              " ['semierupcionado', 'O'],\n",
              " [',', 'O'],\n",
              " ['en', 'O'],\n",
              " ['posiciÃ³n', 'O'],\n",
              " ['horizontal', 'O'],\n",
              " [',', 'O'],\n",
              " ['asintomÃ¡tico', 'O'],\n",
              " ['(', 'O'],\n",
              " ['hallazgo', 'O'],\n",
              " ['radiogrÃ¡fico', 'I-Procedure'],\n",
              " [')', 'O'],\n",
              " ['Solicito', 'O'],\n",
              " ['extracciÃ³n', 'B-Procedure'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['Anomalias', 'B-Disease'],\n",
              " ['de', 'I-Disease'],\n",
              " ['la', 'I-Disease'],\n",
              " ['posicion', 'I-Disease'],\n",
              " ['del', 'I-Disease'],\n",
              " ['diente', 'I-Disease'],\n",
              " ['EOS', 'EOS'],\n",
              " ['-', 'O'],\n",
              " ['TRASTORNO', 'B-Disease'],\n",
              " ['DE', 'I-Disease'],\n",
              " ['LA', 'I-Disease'],\n",
              " ['CONDUCTA', 'I-Disease'],\n",
              " ['ALIMENTARIA', 'I-Disease'],\n",
              " ['/', 'O'],\n",
              " ['-', 'O'],\n",
              " ['Fundamento', 'O'],\n",
              " ['ClÃ­nico', 'O'],\n",
              " ['APS', 'O'],\n",
              " [':', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['SIN', 'O'],\n",
              " ['ANTECEDENTES', 'O'],\n",
              " ['MORBIDOS', 'O'],\n",
              " ['APARENTES', 'O'],\n",
              " ['MENOR', 'O'],\n",
              " ['ACUDE', 'O'],\n",
              " ['A', 'O'],\n",
              " ['CONSULTA', 'O'],\n",
              " ['EN', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['SU', 'O'],\n",
              " ['MADRE', 'B-Family_Member'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['ACTUALMENTE', 'O'],\n",
              " ['EN', 'O'],\n",
              " ['CONTROL', 'O'],\n",
              " ['POR', 'O'],\n",
              " ['BAJO', 'O'],\n",
              " ['PESO', 'O'],\n",
              " [',', 'O'],\n",
              " ['REFIERE', 'O'],\n",
              " ['TENER', 'O'],\n",
              " ['POCO', 'O'],\n",
              " ['APETITO', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['DIFICIL', 'O'],\n",
              " ['COMUNICACION', 'O'],\n",
              " ['CON', 'O'],\n",
              " ['PACIENTE', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['NO', 'O'],\n",
              " ['IDENTIFICA', 'O'],\n",
              " ['PROBLEMAS', 'O'],\n",
              " ['O', 'O'],\n",
              " ['RIESGO', 'O'],\n",
              " ['EN', 'O'],\n",
              " ['SU', 'O'],\n",
              " ['CONDUCTA', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['LA', 'O'],\n",
              " ['MADRE', 'B-Family_Member'],\n",
              " ['QUIEN', 'O'],\n",
              " ['SEÃ‘ALA', 'O'],\n",
              " ['QUE', 'O'],\n",
              " ['LA', 'O'],\n",
              " ['MENOR', 'O'],\n",
              " ['ES', 'O'],\n",
              " ['MUY', 'O'],\n",
              " ['SELECTIVA', 'O'],\n",
              " ['CON', 'O'],\n",
              " ['LOS', 'O'],\n",
              " ['ALIMENTOS', 'O'],\n",
              " ['ADEMAS', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['QUE', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['ACUSA', 'O'],\n",
              " ['MALESTAR', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['CON', 'O'],\n",
              " ['EL', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['LAS', 'O'],\n",
              " ['COMIDAS', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['<unk>', 'O'],\n",
              " ['QUE', 'O'],\n",
              " ['HA', 'O'],\n",
              " ['PRESENTADO', 'O'],\n",
              " ['VOMITOS', 'O'],\n",
              " ['EN', 'O'],\n",
              " ['OCASIONES', 'O'],\n",
              " ['PERO', 'O'],\n",
              " ['QUE', 'O'],\n",
              " ['TIENE', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['QUE', 'O'],\n",
              " ['ELLA', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['ESTAR', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['EL', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['P', 'B-Procedure'],\n",
              " [':', 'O'],\n",
              " ['<unk>', 'B-Procedure'],\n",
              " ['T', 'I-Procedure'],\n",
              " [':', 'O'],\n",
              " ['1', 'O'],\n",
              " ['.', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['IMC', 'O'],\n",
              " [':', 'O'],\n",
              " ['16', 'B-Body_Part'],\n",
              " ['.', 'O'],\n",
              " ['7', 'O'],\n",
              " [',', 'O'],\n",
              " ['SIN', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['PESO', 'O'],\n",
              " ['A', 'O'],\n",
              " ['PESAR', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['MULTIDISCIPLINARIA', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['POR', 'O'],\n",
              " ['TODO', 'O'],\n",
              " ['LO', 'O'],\n",
              " ['ANTERIOR', 'O'],\n",
              " ['SOLICITO', 'O'],\n",
              " ['ATENTAMENTE', 'O'],\n",
              " ['EVALUACION', 'O'],\n",
              " ['POR', 'O'],\n",
              " ['ESPECIALIDAD', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['-', 'O'],\n",
              " ['EXAMEN', 'O'],\n",
              " ['GINECOLÃ“GICO', 'O'],\n",
              " ['(', 'O'],\n",
              " ['GENERAL', 'O'],\n",
              " [')', 'O'],\n",
              " ['(', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['RUTINA', 'O'],\n",
              " [')', 'O'],\n",
              " ['/', 'O'],\n",
              " ['-', 'O'],\n",
              " ['Fundamento', 'O'],\n",
              " ['ClÃ­nico', 'O'],\n",
              " ['APS', 'O'],\n",
              " [':', 'O'],\n",
              " ['-', 'O'],\n",
              " ['G4P4A0', 'O'],\n",
              " ['-', 'O'],\n",
              " ['1', 'O'],\n",
              " ['CCA', 'B-Procedure'],\n",
              " ['-', 'O'],\n",
              " ['OBESIDAD', 'B-Disease'],\n",
              " ['MORBIDA', 'I-Disease'],\n",
              " ['-', 'O'],\n",
              " ['HTA', 'B-Disease'],\n",
              " ['CRONICA', 'I-Disease'],\n",
              " ['-', 'O'],\n",
              " ['ARTROSIS', 'B-Disease'],\n",
              " ['-', 'O'],\n",
              " ['METRORRAGIA', 'B-Disease'],\n",
              " ['POST', 'I-Disease'],\n",
              " ['<unk>', 'I-Disease'],\n",
              " ['Examen', 'O'],\n",
              " ['ginecologico', 'O'],\n",
              " ['(', 'O'],\n",
              " ['general', 'O'],\n",
              " [')', 'O'],\n",
              " ['(', 'O'],\n",
              " ['de', 'O'],\n",
              " ['rutina', 'O'],\n",
              " [')', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['-', 'O'],\n",
              " ['PACIENTE', 'O'],\n",
              " ['clase', 'O'],\n",
              " ['I', 'O'],\n",
              " ['MOLAR', 'O'],\n",
              " ['DERECHA', 'O'],\n",
              " ['E', 'O'],\n",
              " ['IZQUIERDA', 'O'],\n",
              " ['-', 'O'],\n",
              " ['APIÃ‘AMIENTO', 'O'],\n",
              " ['DENTARIO', 'O'],\n",
              " ['grupo', 'B-Body_Part'],\n",
              " ['II', 'I-Body_Part'],\n",
              " ['Y', 'O'],\n",
              " ['V', 'I-Body_Part'],\n",
              " ['-', 'O'],\n",
              " ['<unk>', 'B-Disease'],\n",
              " ['MEDIAS', 'I-Disease'],\n",
              " ['NO', 'I-Disease'],\n",
              " ['<unk>', 'I-Disease'],\n",
              " ['EOS', 'EOS'],\n",
              " ['-', 'O'],\n",
              " ['MACROCEFALIA', 'B-Disease'],\n",
              " ['-', 'O'],\n",
              " ['OTROS', 'B-Disease'],\n",
              " ['TRASTORNOS', 'I-Disease'],\n",
              " ['GENERALIZADOS', 'I-Disease'],\n",
              " ['DEL', 'I-Disease'],\n",
              " ['<unk>', 'I-Disease'],\n",
              " ['DEL', 'I-Disease'],\n",
              " ['<unk>', 'I-Disease'],\n",
              " ['PSICOMOTOR', 'I-Disease'],\n",
              " ['PACIENTE', 'O'],\n",
              " ['SIN', 'O'],\n",
              " ['ANTECEDENTES', 'O'],\n",
              " ['MORBIDOS', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['MACROCEFALIA', 'B-Disease'],\n",
              " ['<unk>', 'I-Disease'],\n",
              " ['Y', 'O'],\n",
              " ['DERIVADO', 'O'],\n",
              " ['EN', 'O'],\n",
              " ['AGOSTO', 'O'],\n",
              " ['DEL', 'O'],\n",
              " ['PRESENTE', 'O'],\n",
              " ['AÃ‘O', 'O'],\n",
              " ['A', 'O'],\n",
              " ['LOS', 'O'],\n",
              " ['8', 'O'],\n",
              " ['MESES', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['EDAD', 'O'],\n",
              " [',', 'O'],\n",
              " ['DESDE', 'O'],\n",
              " ['ENTONCES', 'O'],\n",
              " ['EN', 'O'],\n",
              " ['SALA', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['ESTIMULACION', 'O'],\n",
              " [',', 'O'],\n",
              " ['PRESENTA', 'O'],\n",
              " ['EN', 'O'],\n",
              " ['CONTROL', 'O'],\n",
              " ['NIÃ‘O', 'O'],\n",
              " ['SANO', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['+', 'O'],\n",
              " ['9', 'O'],\n",
              " ['MESES', 'O'],\n",
              " ['MESES', 'O'],\n",
              " ['PERIMETRO', 'O'],\n",
              " ['CRANEANO', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['51', 'O'],\n",
              " ['.', 'O'],\n",
              " ['5', 'O'],\n",
              " ['CM', 'O'],\n",
              " [',', 'O'],\n",
              " ['EN', 'O'],\n",
              " ['CONTROL', 'O'],\n",
              " ['PREVIO', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['LOS', 'O'],\n",
              " ['8', 'O'],\n",
              " ['MESES', 'O'],\n",
              " ['PRESENTO', 'O'],\n",
              " ['PERIMETRO', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['47', 'O'],\n",
              " ['CM', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['EL', 'O'],\n",
              " ['PERIMETRO', 'O'],\n",
              " ['CRANEANO', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['NACIMIENTO', 'O'],\n",
              " ['FUE', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['36', 'O'],\n",
              " ['.', 'O'],\n",
              " ['5CM', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['PACIENTE', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " [',', 'O'],\n",
              " ['PRESENTA', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " [':', 'O'],\n",
              " ['RETRASO', 'B-Disease'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['FAVOR', 'O'],\n",
              " ['EVALUAR', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['PACIENTE', 'O'],\n",
              " ['14', 'O'],\n",
              " ['ANOS', 'O'],\n",
              " [',', 'O'],\n",
              " ['RESISTENCIA', 'B-Disease'],\n",
              " ['A', 'I-Disease'],\n",
              " ['LA', 'I-Disease'],\n",
              " ['INSULINA', 'I-Disease'],\n",
              " ['SIN', 'O'],\n",
              " ['TRATAMIENTO', 'O'],\n",
              " [',', 'O'],\n",
              " ['GINGIVITIS', 'B-Disease'],\n",
              " [',', 'O'],\n",
              " ['POLIOBTURACIONES', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['PIEZA', 'B-Body_Part'],\n",
              " ['13', 'I-Body_Part'],\n",
              " ['NECROSIS', 'B-Disease'],\n",
              " ['PULPAR', 'I-Disease'],\n",
              " ['EOS', 'EOS'],\n",
              " ['COMPRESION', 'O'],\n",
              " ['MANDIBULAR', 'O'],\n",
              " ['MAS', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['ANTERIOR', 'O'],\n",
              " [',', 'O'],\n",
              " ['CON', 'O'],\n",
              " ['MORDIDA', 'O'],\n",
              " ['ABIERTA', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['(', 'O'],\n",
              " ['POR', 'O'],\n",
              " ['HABITO', 'B-Disease'],\n",
              " ['DISFUNCIONAL', 'I-Disease'],\n",
              " ['DE', 'O'],\n",
              " ['SUCCION', 'I-Disease'],\n",
              " ['DE', 'O'],\n",
              " ['MAMADERA', 'O'],\n",
              " [')', 'O'],\n",
              " [',', 'O'],\n",
              " ['EN', 'O'],\n",
              " ['CLASE', 'O'],\n",
              " ['I', 'O'],\n",
              " ['MOLAR', 'O'],\n",
              " ['BILATERAL', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['PACIENTE', 'O'],\n",
              " ['CON', 'O'],\n",
              " ['DEFECTOS', 'B-Disease'],\n",
              " ['DE', 'O'],\n",
              " ['<unk>', 'I-Disease'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['1', 'B-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['8', 'I-Body_Part'],\n",
              " [':', 'O'],\n",
              " ['SEMIERUPCIONADA', 'O'],\n",
              " ['MESIOVERSION', 'O'],\n",
              " [',', 'O'],\n",
              " ['IMPACTADA', 'O'],\n",
              " ['N', 'O'],\n",
              " ['PIEZA', 'B-Body_Part'],\n",
              " ['1', 'I-Body_Part'],\n",
              " [',', 'I-Body_Part'],\n",
              " ['7', 'I-Body_Part'],\n",
              " ['2', 'I-Body_Part'],\n",
              " [',', 'I-Body_Part'],\n",
              " ['8', 'I-Body_Part'],\n",
              " [':', 'O'],\n",
              " ['SEMIERUPCIONADA', 'O'],\n",
              " [',', 'O'],\n",
              " ['VERTICAL', 'O'],\n",
              " [',', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['RADICULAR', 'O'],\n",
              " ['A', 'O'],\n",
              " ['DISTAL', 'B-Body_Part'],\n",
              " ['3', 'I-Body_Part'],\n",
              " [',', 'I-Body_Part'],\n",
              " ['8', 'I-Body_Part'],\n",
              " ['SEMIINCLUIDA', 'O'],\n",
              " ['IMPACTADA', 'O'],\n",
              " ['3', 'O'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['7', 'I-Body_Part'],\n",
              " ['MESIOVERSION', 'O'],\n",
              " [',', 'O'],\n",
              " ['CLASE', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['Y', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " [',', 'O'],\n",
              " ['RAICES', 'B-Body_Part'],\n",
              " ['SOBRE', 'O'],\n",
              " ['CANAL', 'B-Body_Part'],\n",
              " ['MANDIBULAR', 'I-Body_Part'],\n",
              " ['4', 'I-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['8', 'I-Body_Part'],\n",
              " [':', 'O'],\n",
              " ['SEMIINCLUIDA', 'O'],\n",
              " ['CALSE', 'O'],\n",
              " ['II', 'O'],\n",
              " [',', 'O'],\n",
              " ['B', 'O'],\n",
              " [',', 'O'],\n",
              " ['DISTOVERSION', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['PACIENTE', 'O'],\n",
              " ['POSTRADO', 'O'],\n",
              " ['DESDENTADO', 'B-Disease'],\n",
              " ['PARCIAL', 'I-Disease'],\n",
              " ['<unk>', 'I-Disease'],\n",
              " ['E', 'I-Disease'],\n",
              " ['INFERIOR', 'I-Disease'],\n",
              " [',', 'O'],\n",
              " ['PERIODONTITIS', 'B-Disease'],\n",
              " ['CRONICO', 'I-Disease'],\n",
              " ['GENERALIZADA', 'I-Disease'],\n",
              " ['MODERADA', 'I-Disease'],\n",
              " ['SEVERA', 'I-Disease'],\n",
              " [',', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['PROTESIS', 'B-Procedure'],\n",
              " ['PARCIAL', 'I-Procedure'],\n",
              " ['REMOBIBLE', 'I-Procedure'],\n",
              " ['MAXILAR', 'I-Procedure'],\n",
              " ['EOS', 'EOS'],\n",
              " ['-', 'O'],\n",
              " ['CATARATA', 'B-Disease'],\n",
              " ['SENIL', 'I-Disease'],\n",
              " ['/', 'O'],\n",
              " ['-', 'O'],\n",
              " ['Fundamento', 'O'],\n",
              " ['ClÃ­nico', 'O'],\n",
              " ['APS', 'O'],\n",
              " [':', 'O'],\n",
              " ['paciente', 'O'],\n",
              " ['sin', 'O'],\n",
              " ['anatcedentes', 'O'],\n",
              " ['morbidos', 'O'],\n",
              " ['conocidos', 'O'],\n",
              " [',', 'O'],\n",
              " ['consulta', 'O'],\n",
              " ['por', 'O'],\n",
              " ['cuadro', 'O'],\n",
              " ['de', 'O'],\n",
              " ['mas', 'O'],\n",
              " ['menos', 'O'],\n",
              " ['4', 'O'],\n",
              " ['aÃ±os', 'O'],\n",
              " ['de', 'O'],\n",
              " ['evolucion', 'O'],\n",
              " ['caracterizado', 'O'],\n",
              " ['por', 'O'],\n",
              " ['dificultad', 'O'],\n",
              " ['visual', 'O'],\n",
              " ['con', 'O'],\n",
              " ['vision', 'O'],\n",
              " ['borrosa', 'O'],\n",
              " ['generalizada', 'O'],\n",
              " ['y', 'O'],\n",
              " ['que', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['con', 'O'],\n",
              " ['mucha', 'O'],\n",
              " ['luz', 'O'],\n",
              " ['o', 'O'],\n",
              " ['con', 'O'],\n",
              " ['poca', 'O'],\n",
              " ['luz', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['consulta', 'O'],\n",
              " ['en', 'O'],\n",
              " ['oftalmologo', 'O'],\n",
              " ['del', 'O'],\n",
              " ['extrasistema', 'O'],\n",
              " ['quien', 'O'],\n",
              " ['le', 'O'],\n",
              " ['diagnostica', 'O'],\n",
              " ['cataratas', 'B-Disease'],\n",
              " ['y', 'O'],\n",
              " ['le', 'O'],\n",
              " ['indica', 'O'],\n",
              " ['cirugia', 'O'],\n",
              " [',', 'O'],\n",
              " ['pero', 'O'],\n",
              " ['el', 'O'],\n",
              " ['paciente', 'O'],\n",
              " ['no', 'O'],\n",
              " ['puede', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['particular', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['se', 'O'],\n",
              " ['deriva', 'O'],\n",
              " ['para', 'O'],\n",
              " ['manejo', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['-', 'O'],\n",
              " ['TRASTORNOS', 'B-Disease'],\n",
              " ['DEL', 'I-Disease'],\n",
              " ['DESARROLLO', 'I-Disease'],\n",
              " ['Y', 'I-Disease'],\n",
              " ['DE', 'I-Disease'],\n",
              " ['LA', 'I-Disease'],\n",
              " ['ERUPCIÃ“N', 'I-Disease'],\n",
              " ['DE', 'I-Disease'],\n",
              " ['LOS', 'I-Disease'],\n",
              " ['DIENTES', 'I-Disease'],\n",
              " ['apiÃ±amiento', 'I-Disease'],\n",
              " ['/', 'O'],\n",
              " ['-', 'O'],\n",
              " ['Fundamento', 'O'],\n",
              " ['ClÃ­nico', 'O'],\n",
              " ['APS', 'O'],\n",
              " [':', 'O'],\n",
              " ['paciente', 'O'],\n",
              " ['de', 'O'],\n",
              " ['7', 'O'],\n",
              " ['aÃ±os', 'O'],\n",
              " ['con', 'O'],\n",
              " ['apiÃ±amiento', 'B-Disease'],\n",
              " ['antero', 'I-Disease'],\n",
              " ['superior', 'I-Disease'],\n",
              " ['e', 'I-Disease'],\n",
              " ['inferior', 'I-Disease'],\n",
              " ['EOS', 'EOS'],\n",
              " ['PACIENTE', 'O'],\n",
              " ['CON', 'O'],\n",
              " ['ANTECEDENTES', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['HTA', 'B-Disease'],\n",
              " [',', 'O'],\n",
              " ['IC', 'B-Disease'],\n",
              " [',', 'O'],\n",
              " ['Ac', 'B-Medication'],\n",
              " ['x', 'O'],\n",
              " ['FA', 'B-Disease'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['EN', 'O'],\n",
              " ['ESTUDIO', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['EPIGASTRALGIA', 'O'],\n",
              " ['SE', 'O'],\n",
              " ['EVIDENCIA', 'O'],\n",
              " ['EN', 'O'],\n",
              " ['ECOGRAFIA', 'B-Procedure'],\n",
              " ['ABDOMINAL', 'I-Procedure'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['PERIODONTITIS', 'B-Disease'],\n",
              " ['CRONICA', 'I-Disease'],\n",
              " ['MODERADA', 'I-Disease'],\n",
              " ['GENERALIZADA', 'I-Disease'],\n",
              " ['SEVERA', 'I-Disease'],\n",
              " ['EN', 'O'],\n",
              " ['1', 'B-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['1', 'I-Body_Part'],\n",
              " ['-', 'I-Body_Part'],\n",
              " ['2', 'I-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['1', 'I-Body_Part'],\n",
              " ['-', 'I-Body_Part'],\n",
              " ['4', 'I-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['1', 'I-Body_Part'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['RX', 'B-Procedure'],\n",
              " [':', 'O'],\n",
              " ['ROM', 'O'],\n",
              " ['AVANZADA', 'O'],\n",
              " ['EN', 'O'],\n",
              " ['GRUPOS', 'B-Body_Part'],\n",
              " ['II', 'I-Body_Part'],\n",
              " ['Y', 'I-Body_Part'],\n",
              " ['V', 'I-Body_Part'],\n",
              " ['ABUNDANTE', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['Y', 'O'],\n",
              " ['SANGRAMIENTO', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['PD', 'B-Body_Part'],\n",
              " ['1', 'I-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['1', 'I-Body_Part'],\n",
              " ['MOVILIDAD', 'O'],\n",
              " ['GRADO', 'O'],\n",
              " ['II', 'O'],\n",
              " ['PD', 'B-Body_Part'],\n",
              " ['2', 'I-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['1', 'I-Body_Part'],\n",
              " ['Y', 'I-Body_Part'],\n",
              " ['4', 'I-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['1', 'I-Body_Part'],\n",
              " ['MOVILIDAD', 'O'],\n",
              " ['GRADO', 'O'],\n",
              " ['I', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['-', 'O'],\n",
              " ['ANOMALÃAS', 'B-Disease'],\n",
              " ['DENTOFACIALES', 'I-Disease'],\n",
              " ['(', 'I-Disease'],\n",
              " ['INCLUSO', 'I-Disease'],\n",
              " ['LA', 'I-Disease'],\n",
              " ['MALOCLUSIÃ“N', 'I-Disease'],\n",
              " [')', 'O'],\n",
              " ['Diente', 'B-Body_Part'],\n",
              " ['1', 'I-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['8', 'I-Body_Part'],\n",
              " ['semierupcionado', 'O'],\n",
              " ['y', 'O'],\n",
              " ['con', 'O'],\n",
              " ['falta', 'O'],\n",
              " ['de', 'O'],\n",
              " ['espacio', 'O'],\n",
              " ['para', 'O'],\n",
              " ['la', 'O'],\n",
              " ['higiene', 'O'],\n",
              " ['y', 'O'],\n",
              " ['acceso', 'O'],\n",
              " [',', 'O'],\n",
              " ['diente', 'B-Body_Part'],\n",
              " ['2', 'I-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['8', 'I-Body_Part'],\n",
              " ['incluido', 'O'],\n",
              " ['en', 'O'],\n",
              " ['mal', 'O'],\n",
              " ['posicion', 'O'],\n",
              " ['inclinado', 'O'],\n",
              " ['hacia', 'O'],\n",
              " ['distal', 'O'],\n",
              " ['con', 'O'],\n",
              " ['imposibilidad', 'O'],\n",
              " ['de', 'O'],\n",
              " ['erupcion', 'O'],\n",
              " [',', 'O'],\n",
              " ['diente', 'B-Body_Part'],\n",
              " ['3', 'I-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['8', 'I-Body_Part'],\n",
              " ['incluido', 'O'],\n",
              " ['e', 'O'],\n",
              " ['impactado', 'O'],\n",
              " ['a', 'O'],\n",
              " ['diente', 'B-Body_Part'],\n",
              " ['3', 'I-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['7', 'I-Body_Part'],\n",
              " ['con', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['a', 'O'],\n",
              " ['canal', 'B-Body_Part'],\n",
              " ['mandibular', 'I-Body_Part'],\n",
              " [',', 'O'],\n",
              " ['diente', 'B-Body_Part'],\n",
              " ['4', 'I-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['8', 'I-Body_Part'],\n",
              " ['incluido', 'O'],\n",
              " ['e', 'O'],\n",
              " ['impactado', 'O'],\n",
              " ['en', 'O'],\n",
              " ['posicion', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['horizontal', 'O'],\n",
              " ['a', 'O'],\n",
              " ['diente', 'B-Body_Part'],\n",
              " ['4', 'I-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['7', 'I-Body_Part'],\n",
              " [',', 'O'],\n",
              " ['se', 'O'],\n",
              " ['deriva', 'O'],\n",
              " ['para', 'O'],\n",
              " ['realizar', 'O'],\n",
              " ['tratamiento', 'O'],\n",
              " ['en', 'O'],\n",
              " ['especialidad', 'O'],\n",
              " ['de', 'O'],\n",
              " ['cirugia', 'B-Procedure'],\n",
              " ['maxilofacial', 'I-Procedure'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['Paciente', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['desde', 'O'],\n",
              " ['hace', 'O'],\n",
              " ['3', 'O'],\n",
              " ['aÃ±os', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['orales', 'I-Disease'],\n",
              " ['con', 'O'],\n",
              " ['frecuencia', 'O'],\n",
              " ['aprox', 'O'],\n",
              " ['de', 'O'],\n",
              " ['3', 'O'],\n",
              " ['veces', 'O'],\n",
              " ['al', 'O'],\n",
              " ['mes', 'O'],\n",
              " [',', 'O'],\n",
              " ['con', 'O'],\n",
              " ['p', 'B-Body_Part'],\n",
              " ['.', 'O'],\n",
              " ['<unk>', 'I-Procedure'],\n",
              " ['negativo', 'O'],\n",
              " ['(', 'O'],\n",
              " ['ANA', 'B-Procedure'],\n",
              " [',', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " [',', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " [',', 'O'],\n",
              " ['FR', 'O'],\n",
              " [',', 'O'],\n",
              " ['VHS', 'B-Procedure'],\n",
              " ['y', 'O'],\n",
              " ['PCR', 'B-Disease'],\n",
              " ['negativos', 'O'],\n",
              " [')', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['Refiere', 'O'],\n",
              " ['artralgia', 'O'],\n",
              " ['en', 'O'],\n",
              " ['manos', 'B-Body_Part'],\n",
              " ['y', 'O'],\n",
              " ['pies', 'B-Body_Part'],\n",
              " ['(', 'O'],\n",
              " ['posterior', 'O'],\n",
              " ['a', 'O'],\n",
              " ['inicio', 'O'],\n",
              " ['de', 'O'],\n",
              " ['prednisona', 'B-Medication'],\n",
              " [',', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['Enf', 'B-Disease'],\n",
              " ['<unk>', 'I-Disease'],\n",
              " [')', 'O'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['Actualmente', 'O'],\n",
              " ['con', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " ['orales', 'I-Medication'],\n",
              " ['.', 'O'],\n",
              " ['EOS', 'EOS'],\n",
              " ['Se', 'O'],\n",
              " ['agradece', 'O'],\n",
              " ['evaluaciÃ³n', 'O'],\n",
              " ['por', 'O'],\n",
              " ['maxilo', 'O'],\n",
              " ['facial', 'O'],\n",
              " ['(', 'O'],\n",
              " ['Dr', 'O'],\n",
              " ['<unk>', 'O'],\n",
              " [')', 'O'],\n",
              " ['para', 'O'],\n",
              " ['toma', 'O'],\n",
              " ['de', 'O'],\n",
              " ['biopsia', 'B-Procedure'],\n",
              " ['de', 'I-Procedure'],\n",
              " ['<unk>', 'I-Procedure'],\n",
              " ['orales', 'I-Procedure'],\n",
              " ['SOSPECHA', 'O'],\n",
              " ['DE', 'O'],\n",
              " ['ENFERMEDAD', 'B-Disease'],\n",
              " ['AUTOINMUNE', 'I-Disease'],\n",
              " ['EOS', 'EOS'],\n",
              " ['-', 'O'],\n",
              " ['PERICORONARITIS', 'B-Disease'],\n",
              " ['/', 'O'],\n",
              " ['-', 'O'],\n",
              " ['DIENTES', 'B-Disease'],\n",
              " ['INCLUIDOS', 'I-Disease'],\n",
              " ['/', 'O'],\n",
              " ['-', 'O'],\n",
              " ['Fundamento', 'O'],\n",
              " ['ClÃ­nico', 'O'],\n",
              " ['APS', 'O'],\n",
              " [':', 'O'],\n",
              " ['Pza', 'B-Body_Part'],\n",
              " ['4', 'I-Body_Part'],\n",
              " ['.', 'I-Body_Part'],\n",
              " ['8', 'I-Body_Part'],\n",
              " ['con', 'O'],\n",
              " ['pericoronaritis', 'B-Disease'],\n",
              " ['.', 'O'],\n",
              " ['Se', 'O'],\n",
              " ...]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aIpdgiez2Q4g"
      },
      "source": [
        "### **Generar el archivo para la submission**\n",
        "\n",
        "No hay problema si aparecen unk en la salida. Estos no son relevantes para evaluarlos, usamos solo los tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIJu-xWy2TA1"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "\n",
        "if (os.path.isfile('./predictions.zip')):\n",
        "    os.remove('./predictions.zip')\n",
        "\n",
        "if (not os.path.isdir('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "f = open('predictions/predictions.txt', 'w')\n",
        "for i, (word, tag) in enumerate(predictions[:-1]):\n",
        "    if word=='EOS' and tag=='EOS': f.write('\\n')\n",
        "    else:\n",
        "      if i == len(predictions[:-1])-1:\n",
        "        f.write(word + ' ' + tag)\n",
        "      else: f.write(word + ' ' + tag + '\\n')\n",
        "\n",
        "f.close()\n",
        "\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AYW1OpT82U2H"
      },
      "source": [
        "## **Conclusiones**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fjwKDxDm2Vsd"
      },
      "source": [
        "    Escriba aquÃ­ sus conclusiones"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
