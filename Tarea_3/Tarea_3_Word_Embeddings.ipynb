{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckbt7VPDhBwb"
      },
      "source": [
        "# **Tarea 3 - Word Embeddings üìö**\n",
        "\n",
        "**Integrantes:**\n",
        "\n",
        "**Fecha l√≠mite de entrega üìÜ:** 16 de mayo.\n",
        "\n",
        "**Tiempo estimado de dedicaci√≥n:**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-19T18:30:18.109327Z",
          "start_time": "2020-03-19T18:30:18.103344Z"
        },
        "id": "q5CSRY4oNCHK"
      },
      "source": [
        "\n",
        "**Instrucciones:**\n",
        "- El ejercicio consiste en:\n",
        "    - Responder preguntas relativas a los contenidos vistos en los v√≠deos y slides de las clases.\n",
        "    - Implementar el m√©todo de la Word Context Matrix. \n",
        "    - Entrenar Word2Vec y FastText sobre un peque√±o corpus.\n",
        "    - Evaluar los embeddings obtenidos en una tarea de clasificaci√≥n.\n",
        "- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo **Jupyter Notebook**.\n",
        "- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n",
        "\n",
        "\n",
        "**Referencias**\n",
        "\n",
        "V√≠deos: \n",
        "\n",
        "- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n",
        "- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n",
        "- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G4wYf0vgnbTv"
      },
      "source": [
        "## **Preguntas te√≥ricas üìï (3 puntos).** ##\n",
        "Para estas preguntas no es necesario implementar c√≥digo, pero pueden utilizar pseudo c√≥digo."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B5hUG6-8ngoK"
      },
      "source": [
        "### **Parte 1: Modelos Lineales (1.5 ptos)**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5yRvZbhsoi8f"
      },
      "source": [
        "Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categor√≠as: pol√≠tica, deporte, negocios y otros. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "irsqBVmCnx3M"
      },
      "source": [
        "**Pregunta 1**: Dise√±e un modelo lineal capaz de clasificar un documento seg√∫n estas categor√≠as donde el output sea un vector con una distribuci√≥n de probabilidad con la pertenencia a cada clase. \n",
        "\n",
        "Especifique: representaci√≥n de los documentos de entrada, par√°metros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y funci√≥n de p√©rdida escogida. **(0.75 puntos)**\n",
        "\n",
        "**Respuesta**: \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G5FaWqBVvL90"
      },
      "source": [
        "**Pregunta 2**: Explique c√≥mo funciona el proceso de entrenamiento en este tipo de modelos y su evaluaci√≥n. **(0.75 puntos)**\n",
        "\n",
        "**Respuesta**: "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XkK7pc54njZq"
      },
      "source": [
        "### **Parte 2: Redes Neuronales (1.5 ptos)** "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VUbJjlj_9AFC"
      },
      "source": [
        "Supongamos que tenemos la siguiente red neuronal."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "obUfuOYB_TOC"
      },
      "source": [
        "![image.png](https://drive.google.com/uc?export=view&id=1nV1G0dOeVGPn40qGcGF9l_pVEFNtLU-w)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s2z-8zKW0_6q"
      },
      "source": [
        "**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matem√°tica. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en funci√≥n del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n",
        "\n",
        "Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.75 Puntos)**\n",
        "\n",
        "**Respuesta**: \n",
        "\n",
        "Formula:\n",
        "$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) =$\n",
        "\n",
        "Dimensiones: \n",
        "\n",
        "**Pregunta 2**: Explique qu√© es backpropagation. ¬øCuales ser√≠an los par√°metros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n",
        "\n",
        "**Respuesta**:\n",
        "\n",
        "**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.5 puntos)**\n",
        "\n",
        "**Respuesta**:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ocS_vQhR1gcU"
      },
      "source": [
        "## **Preguntas pr√°cticas üíª (3 puntos).** ##"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D0wk5GBkSE73"
      },
      "source": [
        "### Parte 3 A (1 Punto): Word Contex Matrix"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e_mh12Z9SF-J"
      },
      "source": [
        "\n",
        "\n",
        "En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librer√≠as ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```\n",
        "\n",
        "```python\n",
        "class WordContextMatrix:\n",
        "\n",
        "  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Utilice el constructor para definir los parametros.\n",
        "    \"\"\"\n",
        "\n",
        "    # se sugiere agregar un una estructura de datos para guardar las\n",
        "    # palabras del vocab y para guardar el conteo de coocurrencia\n",
        "    # si lo necesita puede agregar m√°s parametros pero no puede cambiar el resto\n",
        "    ...\n",
        "    \n",
        "  def build_vocab(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para construir el vocabulario\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    # Le puede ser √∫til considerar un token unk al vocab\n",
        "    # para palabras fuera del vocab\n",
        "    ...\n",
        "  \n",
        "  def build_matrix(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para crear la palabra contexto\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  def get_matrix(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para obtener la matriz palabra contexto. \n",
        "    \"\"\"\n",
        "\n",
        "    # se recomienda transformar la matrix a un diccionario de embedding.\n",
        "    # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n",
        "    ...\n",
        "\n",
        "```\n",
        "\n",
        "puede modificar los par√°metros o m√©todos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n",
        "\n",
        "```python\n",
        "corpus = [\n",
        "  \"I like deep learning.\",\n",
        "  \"I like NLP.\",\n",
        "  \"I enjoy flying.\"\n",
        "]\n",
        "```\n",
        "\n",
        "Obteniendo una matriz parecia a esta:\n",
        "\n",
        "***Resultado esperado***: \n",
        "\n",
        "| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n",
        "|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n",
        "| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n",
        "| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n",
        "| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n",
        "| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n",
        "| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n",
        "| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n",
        "| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n",
        "| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n",
        "\n",
        "``\n",
        "\n",
        "Verifique si su matrix es igual a esta utilizando el corpus de ejemplo. Ojo que este es s√≥lo un ejemplo, su algoritmo debe **generalizar** a otros ejemplos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = [\n",
        "  \"I like deep learning.\",\n",
        "  \"I like NLP.\",\n",
        "  \"I enjoy flying.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "#definimos duncion tokenizer for every sentence:\n",
        "def tokenizer(sentence):\n",
        "\n",
        "    tokens = sentence.split()\n",
        "\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class WordContextMatrix:\n",
        "\n",
        "  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.window_size = window_size\n",
        "    self.dataset = dataset\n",
        "    self.tokenizer = tokenizer\n",
        "    self.matrix = np.zeros((vocab_size, vocab_size))\n",
        "\n",
        "  #Definimos la funci√≥n para construir el vocabulario:\n",
        "  def build_vocab(self):\n",
        "    vocab = []\n",
        "    for sentence in self.dataset:\n",
        "      for word in self.tokenizer(sentence):\n",
        "        vocab.append(word)\n",
        "    return list(set(vocab))\n",
        "  \n",
        "  #Definimos la funci√≥n para construir la matriz de contexto:\n",
        "  def build_matrix(self):\n",
        "    vocab = self.build_vocab()\n",
        "    for sentence in self.dataset:\n",
        "      for i in range(0, len(self.tokenizer(sentence))):\n",
        "        for j in range(i - self.window_size, i + self.window_size + 1):\n",
        "          if j >= 0 and j < len(self.tokenizer(sentence)) and j != i:\n",
        "            #print('w=', self.tokenizer(sentence)[i], 'context=', self.tokenizer(sentence)[j], i, j)\n",
        "            self.matrix[vocab.index(self.tokenizer(sentence)[i])][vocab.index(self.tokenizer(sentence)[j])] += 1\n",
        "    return self.matrix\n",
        "  \n",
        "  #Definimos la funci√≥n para obtener la matriz de contexto:\n",
        "  def get_matrix(self):\n",
        "    return self.matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'enjoy', 'like', 'flying.', 'NLP.', 'learning.', 'deep']\n",
            "14\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>I</th>\n",
              "      <th>enjoy</th>\n",
              "      <th>like</th>\n",
              "      <th>flying.</th>\n",
              "      <th>NLP.</th>\n",
              "      <th>learning.</th>\n",
              "      <th>deep</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>I</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>enjoy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>like</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flying.</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NLP.</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>learning.</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>deep</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           I  enjoy  like  flying.  NLP.  learning.  deep\n",
              "I          0      1     2        0     0          0     0\n",
              "enjoy      1      0     0        1     0          0     0\n",
              "like       2      0     0        0     1          0     1\n",
              "flying.    0      1     0        0     0          0     0\n",
              "NLP.       0      0     1        0     0          0     0\n",
              "learning.  0      0     0        0     0          0     1\n",
              "deep       0      0     1        0     0          1     0"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context_matrix = WordContextMatrix(7, 1, corpus, tokenizer) # Tama√±o del volabulario se define manualmente ej: 7\n",
        "\n",
        "print(context_matrix.build_vocab()[0:10])\n",
        "\n",
        "CM = context_matrix.build_matrix()\n",
        "#Imprimimos la suma de elentos de CM en int:\n",
        "print(np.sum(CM, dtype=int))\n",
        "\n",
        "#add the matrix to a dataframe including vicabulary as index and columns:\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(CM, index=context_matrix.build_vocab()[0:10], columns=context_matrix.build_vocab()[0:10], dtype=int)\n",
        "df\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol82nJ0FnmcP"
      },
      "source": [
        "### **Parte 3 B (1 Punto): Word Embeddings**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OgmeSFqKLpFL"
      },
      "source": [
        "En la auxiliar 2 aprendieron como entrenar Word2Vec utilizando gensim. El objetivo de esta parte es comparar los embeddings obtenidos con dos modelos \n",
        "diferentes: Word2Vec y [FastText](https://radimrehurek.com/gensim/models/fasttext.html) (utilizen size=200 en FastText) entrenados en el mismo dataset de di√°logos de los Simpson. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CgSlIxJW36ZA"
      },
      "source": [
        "# Secci√≥n nueva"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ecCvnryeQiG7"
      },
      "outputs": [],
      "source": [
        "import re  \n",
        "import pandas as pd \n",
        "from time import time  \n",
        "from collections import defaultdict \n",
        "import string \n",
        "import multiprocessing\n",
        "import os\n",
        "import gensim\n",
        "import sklearn\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
        "\n",
        "# word2vec\n",
        "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tZgN06q4QPi3"
      },
      "source": [
        "Utilizando el dataset adjunto con la tarea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY3kmg4onnsu",
        "outputId": "d3525a54-0c10-401e-b3e2-9c6e9e714a2c"
      },
      "outputs": [],
      "source": [
        "data_file = \"data/simpsons_dataset.csv\"\n",
        "df = pd.read_csv(data_file)\n",
        "stopwords = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
        ").values\n",
        "stopwords = Counter(stopwords.flatten().tolist())\n",
        "df = df.dropna().reset_index(drop=True) # Quitar filas vacias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>raw_character_text</th>\n",
              "      <th>spoken_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Miss Hoover</td>\n",
              "      <td>No, actually, it was a little of both. Sometim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lisa Simpson</td>\n",
              "      <td>Where's Mr. Bergstrom?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Miss Hoover</td>\n",
              "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lisa Simpson</td>\n",
              "      <td>That life is worth living.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Edna Krabappel-Flanders</td>\n",
              "      <td>The polls will be open from now until the end ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131848</th>\n",
              "      <td>Miss Hoover</td>\n",
              "      <td>I'm back.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131849</th>\n",
              "      <td>Miss Hoover</td>\n",
              "      <td>You see, class, my Lyme disease turned out to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131850</th>\n",
              "      <td>Miss Hoover</td>\n",
              "      <td>Psy-cho-so-ma-tic.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131851</th>\n",
              "      <td>Ralph Wiggum</td>\n",
              "      <td>Does that mean you were crazy?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131852</th>\n",
              "      <td>JANEY</td>\n",
              "      <td>No, that means she was faking it.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>131853 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             raw_character_text  \\\n",
              "0                   Miss Hoover   \n",
              "1                  Lisa Simpson   \n",
              "2                   Miss Hoover   \n",
              "3                  Lisa Simpson   \n",
              "4       Edna Krabappel-Flanders   \n",
              "...                         ...   \n",
              "131848              Miss Hoover   \n",
              "131849              Miss Hoover   \n",
              "131850              Miss Hoover   \n",
              "131851             Ralph Wiggum   \n",
              "131852                    JANEY   \n",
              "\n",
              "                                             spoken_words  \n",
              "0       No, actually, it was a little of both. Sometim...  \n",
              "1                                  Where's Mr. Bergstrom?  \n",
              "2       I don't know. Although I'd sure like to talk t...  \n",
              "3                              That life is worth living.  \n",
              "4       The polls will be open from now until the end ...  \n",
              "...                                                   ...  \n",
              "131848                                          I'm back.  \n",
              "131849  You see, class, my Lyme disease turned out to ...  \n",
              "131850                                 Psy-cho-so-ma-tic.  \n",
              "131851                     Does that mean you were crazy?  \n",
              "131852                  No, that means she was faking it.  \n",
              "\n",
              "[131853 rows x 2 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VAg5a5bmWk3T"
      },
      "source": [
        "**Pregunta 1**: Ayud√°ndose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec y FastText sobre el dataset anterior. **(1 punto)** (Hint, le puede servir explorar un poco los datos)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MWw2fXFRXe5Y"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "Bvwplz7yTNcr"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(131853,)"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# unir titulo con contenido de la noticia\n",
        "content = df['raw_character_text'] + df['spoken_words']\n",
        "content.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# limpiar puntuaciones y separar por tokens.\n",
        "punctuation = string.punctuation + \"¬´¬ª‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî\"\n",
        "stopwords = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt'\n",
        ").values\n",
        "stopwords = Counter(stopwords.flatten().tolist())\n",
        "\n",
        "def simple_tokenizer(doc, lower=False):\n",
        "    if lower:\n",
        "        tokenized_doc = doc.translate(str.maketrans(\n",
        "            '', '', punctuation)).lower().split()\n",
        "\n",
        "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
        "    tokenized_doc = [\n",
        "        token for token in tokenized_doc if token.lower() not in stopwords\n",
        "    ]\n",
        "    return tokenized_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ejemplo de alguna noticia: ['Lisa', 'SimpsonI', 'see', 'touched', 'you', 'too']\n"
          ]
        }
      ],
      "source": [
        "cleaned_content = [simple_tokenizer(doc) for doc in content.values]\n",
        "print(\"Ejemplo de alguna noticia: {}\".format(cleaned_content[14]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-19 16:17:02,475 : INFO : collecting all words and their counts\n",
            "2023-05-19 16:17:02,476 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
            "2023-05-19 16:17:02,543 : INFO : PROGRESS: at sentence #5000, processed 52993 words and 41090 word types\n",
            "2023-05-19 16:17:02,609 : INFO : PROGRESS: at sentence #10000, processed 104443 words and 72630 word types\n",
            "2023-05-19 16:17:02,676 : INFO : PROGRESS: at sentence #15000, processed 153553 words and 100125 word types\n",
            "2023-05-19 16:17:02,753 : INFO : PROGRESS: at sentence #20000, processed 210893 words and 128923 word types\n",
            "2023-05-19 16:17:02,831 : INFO : PROGRESS: at sentence #25000, processed 267475 words and 157025 word types\n",
            "2023-05-19 16:17:02,915 : INFO : PROGRESS: at sentence #30000, processed 327248 words and 185229 word types\n",
            "2023-05-19 16:17:02,993 : INFO : PROGRESS: at sentence #35000, processed 383895 words and 210128 word types\n",
            "2023-05-19 16:17:03,064 : INFO : PROGRESS: at sentence #40000, processed 434936 words and 231912 word types\n",
            "2023-05-19 16:17:03,135 : INFO : PROGRESS: at sentence #45000, processed 485343 words and 253492 word types\n",
            "2023-05-19 16:17:03,210 : INFO : PROGRESS: at sentence #50000, processed 534712 words and 274693 word types\n",
            "2023-05-19 16:17:03,280 : INFO : PROGRESS: at sentence #55000, processed 582859 words and 294866 word types\n",
            "2023-05-19 16:17:03,347 : INFO : PROGRESS: at sentence #60000, processed 628592 words and 313505 word types\n",
            "2023-05-19 16:17:03,421 : INFO : PROGRESS: at sentence #65000, processed 677609 words and 333043 word types\n",
            "2023-05-19 16:17:03,511 : INFO : PROGRESS: at sentence #70000, processed 732677 words and 355604 word types\n",
            "2023-05-19 16:17:03,590 : INFO : PROGRESS: at sentence #75000, processed 787361 words and 376972 word types\n",
            "2023-05-19 16:17:03,671 : INFO : PROGRESS: at sentence #80000, processed 842358 words and 397923 word types\n",
            "2023-05-19 16:17:03,749 : INFO : PROGRESS: at sentence #85000, processed 896991 words and 418791 word types\n",
            "2023-05-19 16:17:03,826 : INFO : PROGRESS: at sentence #90000, processed 949703 words and 438958 word types\n",
            "2023-05-19 16:17:03,906 : INFO : PROGRESS: at sentence #95000, processed 1004405 words and 458631 word types\n",
            "2023-05-19 16:17:03,988 : INFO : PROGRESS: at sentence #100000, processed 1058029 words and 478687 word types\n",
            "2023-05-19 16:17:04,091 : INFO : PROGRESS: at sentence #105000, processed 1112821 words and 498616 word types\n",
            "2023-05-19 16:17:04,177 : INFO : PROGRESS: at sentence #110000, processed 1168376 words and 519127 word types\n",
            "2023-05-19 16:17:04,253 : INFO : PROGRESS: at sentence #115000, processed 1221692 words and 537672 word types\n",
            "2023-05-19 16:17:04,337 : INFO : PROGRESS: at sentence #120000, processed 1275993 words and 556857 word types\n",
            "2023-05-19 16:17:04,416 : INFO : PROGRESS: at sentence #125000, processed 1329611 words and 575455 word types\n",
            "2023-05-19 16:17:04,497 : INFO : PROGRESS: at sentence #130000, processed 1383007 words and 591195 word types\n",
            "2023-05-19 16:17:04,527 : INFO : collected 597019 token types (unigram + bigrams) from a corpus of 1402728 words and 131853 sentences\n",
            "2023-05-19 16:17:04,528 : INFO : merged Phrases<597019 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n",
            "2023-05-19 16:17:04,529 : INFO : Phrases lifecycle event {'msg': 'built Phrases<597019 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000> in 2.05s', 'datetime': '2023-05-19T16:17:04.529107', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "# Phrases recibe una lista de oraciones, y junta bigramas que est√©n al menos 100 veces repetidos\n",
        "# como un √∫nico token. Detr√°s de esto hay un modelo estad√≠stico basado en frecuencias, probabilidades, etc\n",
        "# pero en t√©rminos simples ese es el resultado\n",
        "phrases = Phrases(cleaned_content, min_count=100, progress_per=5000) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-19 16:17:09,461 : INFO : exporting phrases from Phrases<597019 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n",
            "2023-05-19 16:17:11,251 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<91 phrases, min_count=100, threshold=10.0> from Phrases<597019 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000> in 1.79s', 'datetime': '2023-05-19T16:17:11.251624', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "bigram = Phraser(phrases)\n",
        "sentences = bigram[cleaned_content]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Miss', 'HooverNo', 'actually', 'it', 'was', 'a', 'little', 'of', 'both', 'Sometimes', 'when', 'a', 'disease', 'is', 'in', 'all', 'the', 'magazines', 'and', 'all', 'the', 'news', 'shows', 'its', 'only', 'natural', 'that', 'you', 'think', 'you', 'have', 'it'], ['Lisa', 'SimpsonWheres', 'Mr', 'Bergstrom'], ['Miss', 'HooverI', 'dont_know', 'Although', 'Id', 'sure', 'like', 'to', 'talk', 'to', 'him', 'didnt', 'touch', 'my', 'lesson', 'plan', 'What', 'did', 'teach', 'you'], ['Lisa', 'SimpsonThat', 'life', 'is', 'worth', 'living'], ['Edna', 'KrabappelFlandersThe', 'polls', 'will_be', 'open', 'from', 'now', 'until', 'the', 'end', 'of', 'recess', 'Now', 'just', 'in', 'case', 'any', 'of', 'you', 'have', 'decided', 'to', 'put', 'any', 'thought', 'into', 'this', 'well', 'have', 'our', 'final', 'statements', 'Martin'], ['Martin', 'PrinceI', 'dont', 'think', 'theres', 'anything', 'left', 'to', 'say'], ['Edna', 'KrabappelFlandersBart'], ['Bart', 'SimpsonVictory', 'party', 'under', 'the', 'slide'], ['Lisa', 'SimpsonMr', 'Bergstrom', 'Mr', 'Bergstrom'], ['LandladyHey', 'hey', 'Moved', 'out', 'this', 'morning', 'must', 'have', 'a', 'new', 'job', 'took', 'his', 'Copernicus', 'costume']]\n"
          ]
        }
      ],
      "source": [
        "# para ver como quedan los textos tokenizadas, quitar comentario a la siguiente linea:\n",
        "print(list(sentences)[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-19 16:17:26,611 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=200, alpha=0.03>', 'datetime': '2023-05-19T16:17:26.611431', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "Simpsons = Word2Vec(min_count=10,\n",
        "                      window=4,\n",
        "                      vector_size=200,\n",
        "                      sample=6e-5,\n",
        "                      alpha=0.03,\n",
        "                      min_alpha=0.0007,\n",
        "                      negative=20,\n",
        "                      workers=multiprocessing.cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-19 16:17:33,623 : INFO : collecting all words and their counts\n",
            "2023-05-19 16:17:33,625 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2023-05-19 16:17:33,877 : INFO : PROGRESS: at sentence #10000, processed 101844 words, keeping 15730 word types\n",
            "2023-05-19 16:17:34,110 : INFO : PROGRESS: at sentence #20000, processed 205879 words, keeping 25594 word types\n",
            "2023-05-19 16:17:34,362 : INFO : PROGRESS: at sentence #30000, processed 319516 words, keeping 34491 word types\n",
            "2023-05-19 16:17:34,602 : INFO : PROGRESS: at sentence #40000, processed 424692 words, keeping 41294 word types\n",
            "2023-05-19 16:17:34,836 : INFO : PROGRESS: at sentence #50000, processed 522288 words, keeping 47863 word types\n",
            "2023-05-19 16:17:35,054 : INFO : PROGRESS: at sentence #60000, processed 614063 words, keeping 53932 word types\n",
            "2023-05-19 16:17:35,290 : INFO : PROGRESS: at sentence #70000, processed 715810 words, keeping 60223 word types\n",
            "2023-05-19 16:17:35,534 : INFO : PROGRESS: at sentence #80000, processed 823239 words, keeping 66326 word types\n",
            "2023-05-19 16:17:35,778 : INFO : PROGRESS: at sentence #90000, processed 928559 words, keeping 72293 word types\n",
            "2023-05-19 16:17:36,025 : INFO : PROGRESS: at sentence #100000, processed 1034630 words, keeping 77908 word types\n",
            "2023-05-19 16:17:36,286 : INFO : PROGRESS: at sentence #110000, processed 1142844 words, keeping 83922 word types\n",
            "2023-05-19 16:17:36,538 : INFO : PROGRESS: at sentence #120000, processed 1248271 words, keeping 89307 word types\n",
            "2023-05-19 16:17:36,780 : INFO : PROGRESS: at sentence #130000, processed 1352861 words, keeping 93841 word types\n",
            "2023-05-19 16:17:36,826 : INFO : collected 94548 word types from a corpus of 1372110 raw words and 131853 sentences\n",
            "2023-05-19 16:17:36,827 : INFO : Creating a fresh vocabulary\n",
            "2023-05-19 16:17:36,962 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 8907 unique words (9.42% of original 94548, drops 85641)', 'datetime': '2023-05-19T16:17:36.962281', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
            "2023-05-19 16:17:36,963 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 1214250 word corpus (88.50% of original 1372110, drops 157860)', 'datetime': '2023-05-19T16:17:36.963004', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
            "2023-05-19 16:17:37,030 : INFO : deleting the raw counts dictionary of 94548 items\n",
            "2023-05-19 16:17:37,032 : INFO : sample=6e-05 downsamples 805 most-common words\n",
            "2023-05-19 16:17:37,033 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 528492.7114590418 word corpus (43.5%% of prior 1214250)', 'datetime': '2023-05-19T16:17:37.033372', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
            "2023-05-19 16:17:37,107 : INFO : estimated required memory for 8907 words and 200 dimensions: 18704700 bytes\n",
            "2023-05-19 16:17:37,107 : INFO : resetting layer weights\n",
            "2023-05-19 16:17:37,113 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-05-19T16:17:37.113462', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'build_vocab'}\n"
          ]
        }
      ],
      "source": [
        "#Construimos el vocabulario:\n",
        "Simpsons.build_vocab(sentences, progress_per=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-19 16:17:43,191 : INFO : Word2Vec lifecycle event {'msg': 'training model with 8 workers on 8907 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4 shrink_windows=True', 'datetime': '2023-05-19T16:17:43.191218', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'train'}\n",
            "2023-05-19 16:17:44,211 : INFO : EPOCH 0 - PROGRESS: at 21.40% examples, 114357 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:17:46,849 : INFO : EPOCH 0: training on 1372110 raw words (528599 effective words) took 3.7s, 144718 effective words/s\n",
            "2023-05-19 16:17:47,871 : INFO : EPOCH 1 - PROGRESS: at 28.46% examples, 152959 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:17:50,182 : INFO : EPOCH 1: training on 1372110 raw words (528640 effective words) took 3.3s, 159026 effective words/s\n",
            "2023-05-19 16:17:51,215 : INFO : EPOCH 2 - PROGRESS: at 29.23% examples, 155229 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:17:53,617 : INFO : EPOCH 2: training on 1372110 raw words (528614 effective words) took 3.4s, 154302 effective words/s\n",
            "2023-05-19 16:17:54,631 : INFO : EPOCH 3 - PROGRESS: at 29.94% examples, 161712 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:17:57,286 : INFO : EPOCH 3: training on 1372110 raw words (528102 effective words) took 3.7s, 144292 effective words/s\n",
            "2023-05-19 16:17:58,327 : INFO : EPOCH 4 - PROGRESS: at 26.93% examples, 142456 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:00,827 : INFO : EPOCH 4: training on 1372110 raw words (528214 effective words) took 3.5s, 149541 effective words/s\n",
            "2023-05-19 16:18:01,866 : INFO : EPOCH 5 - PROGRESS: at 29.23% examples, 153581 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:04,256 : INFO : EPOCH 5: training on 1372110 raw words (528680 effective words) took 3.4s, 154364 effective words/s\n",
            "2023-05-19 16:18:05,274 : INFO : EPOCH 6 - PROGRESS: at 28.46% examples, 153391 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:07,884 : INFO : EPOCH 6: training on 1372110 raw words (528410 effective words) took 3.6s, 145997 effective words/s\n",
            "2023-05-19 16:18:08,893 : INFO : EPOCH 7 - PROGRESS: at 28.46% examples, 154053 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:11,237 : INFO : EPOCH 7: training on 1372110 raw words (528027 effective words) took 3.3s, 157662 effective words/s\n",
            "2023-05-19 16:18:12,266 : INFO : EPOCH 8 - PROGRESS: at 29.23% examples, 154669 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:14,740 : INFO : EPOCH 8: training on 1372110 raw words (527997 effective words) took 3.5s, 150870 effective words/s\n",
            "2023-05-19 16:18:15,759 : INFO : EPOCH 9 - PROGRESS: at 28.46% examples, 153484 words/s, in_qsize 1, out_qsize 0\n",
            "2023-05-19 16:18:18,156 : INFO : EPOCH 9: training on 1372110 raw words (528449 effective words) took 3.4s, 155157 effective words/s\n",
            "2023-05-19 16:18:19,184 : INFO : EPOCH 10 - PROGRESS: at 29.23% examples, 155574 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:21,716 : INFO : EPOCH 10: training on 1372110 raw words (527812 effective words) took 3.6s, 148610 effective words/s\n",
            "2023-05-19 16:18:22,724 : INFO : EPOCH 11 - PROGRESS: at 27.71% examples, 150805 words/s, in_qsize 1, out_qsize 0\n",
            "2023-05-19 16:18:25,196 : INFO : EPOCH 11: training on 1372110 raw words (528719 effective words) took 3.5s, 152118 effective words/s\n",
            "2023-05-19 16:18:26,200 : INFO : EPOCH 12 - PROGRESS: at 27.71% examples, 150625 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:28,623 : INFO : EPOCH 12: training on 1372110 raw words (528351 effective words) took 3.4s, 154346 effective words/s\n",
            "2023-05-19 16:18:29,639 : INFO : EPOCH 13 - PROGRESS: at 28.46% examples, 153207 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:32,195 : INFO : EPOCH 13: training on 1372110 raw words (528661 effective words) took 3.6s, 148164 effective words/s\n",
            "2023-05-19 16:18:33,207 : INFO : EPOCH 14 - PROGRESS: at 29.23% examples, 158313 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:35,548 : INFO : EPOCH 14: training on 1372110 raw words (528967 effective words) took 3.3s, 158198 effective words/s\n",
            "2023-05-19 16:18:36,577 : INFO : EPOCH 15 - PROGRESS: at 28.46% examples, 151127 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:38,907 : INFO : EPOCH 15: training on 1372110 raw words (527993 effective words) took 3.4s, 157386 effective words/s\n",
            "2023-05-19 16:18:39,934 : INFO : EPOCH 16 - PROGRESS: at 29.23% examples, 155799 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:42,315 : INFO : EPOCH 16: training on 1372110 raw words (528058 effective words) took 3.4s, 155351 effective words/s\n",
            "2023-05-19 16:18:43,342 : INFO : EPOCH 17 - PROGRESS: at 21.40% examples, 114002 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:46,145 : INFO : EPOCH 17: training on 1372110 raw words (528263 effective words) took 3.8s, 138260 effective words/s\n",
            "2023-05-19 16:18:47,163 : INFO : EPOCH 18 - PROGRESS: at 24.14% examples, 129770 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:50,292 : INFO : EPOCH 18: training on 1372110 raw words (528225 effective words) took 4.1s, 127540 effective words/s\n",
            "2023-05-19 16:18:51,334 : INFO : EPOCH 19 - PROGRESS: at 20.78% examples, 108593 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:54,722 : INFO : EPOCH 19: training on 1372110 raw words (528290 effective words) took 4.4s, 119488 effective words/s\n",
            "2023-05-19 16:18:55,735 : INFO : EPOCH 20 - PROGRESS: at 19.53% examples, 104255 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:18:58,827 : INFO : EPOCH 20: training on 1372110 raw words (527875 effective words) took 4.1s, 128899 effective words/s\n",
            "2023-05-19 16:18:59,850 : INFO : EPOCH 21 - PROGRESS: at 25.46% examples, 137623 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:19:02,653 : INFO : EPOCH 21: training on 1372110 raw words (528528 effective words) took 3.8s, 138472 effective words/s\n",
            "2023-05-19 16:19:03,669 : INFO : EPOCH 22 - PROGRESS: at 24.81% examples, 134324 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:19:06,565 : INFO : EPOCH 22: training on 1372110 raw words (528514 effective words) took 3.9s, 135437 effective words/s\n",
            "2023-05-19 16:19:07,580 : INFO : EPOCH 23 - PROGRESS: at 20.12% examples, 106934 words/s, in_qsize 0, out_qsize 0\n",
            "2023-05-19 16:19:10,567 : INFO : EPOCH 23: training on 1372110 raw words (528839 effective words) took 4.0s, 132279 effective words/s\n",
            "2023-05-19 16:19:11,576 : INFO : EPOCH 24 - PROGRESS: at 24.14% examples, 131326 words/s, in_qsize 0, out_qsize 1\n",
            "2023-05-19 16:19:14,487 : INFO : EPOCH 24: training on 1372110 raw words (527962 effective words) took 3.9s, 135017 effective words/s\n",
            "2023-05-19 16:19:14,487 : INFO : Word2Vec lifecycle event {'msg': 'training on 34302750 raw words (13208789 effective words) took 91.3s, 144681 effective words/s', 'datetime': '2023-05-19T16:19:14.487660', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'train'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to train the model: 1.52 mins\n"
          ]
        }
      ],
      "source": [
        "t = time()\n",
        "Simpsons.train(sentences, total_examples=Simpsons.corpus_count, epochs=25, report_delay=10)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### NOTA: Falta entrenar FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100396/125515796.py:2: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
            "  Simpsons.init_sims(replace=True)\n",
            "2023-05-19 16:19:20,248 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
          ]
        }
      ],
      "source": [
        "#Ahora que terminamos de entrenar el modelo, le indicamos que no lo entrenaremos mas. Esto nos permitir√° ejecutar eficientemente las tareas que realizaremos.\n",
        "Simpsons.init_sims(replace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lr8U5wOTNcr"
      },
      "source": [
        "**Pregunta 2**: Encuentre las palabras mas similares a las siguientes: Lisa, Bart, Homer, Marge. C√∫al es la diferencia entre ambos resultados? Por qu√© ocurre esto? Intente comparar ahora Liisa en ambos modelos (doble i). Cuando escoger√≠a uno vs el otro? **(0.5 puntos)**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yMLyGffVTNcs"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "w6RvJGpbTNcs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Bart', 0.7668272852897644),\n",
              " ('Marge', 0.7249565720558167),\n",
              " ('Homer', 0.7183125615119934),\n",
              " ('Grampa', 0.661527693271637),\n",
              " ('Dad', 0.5695444345474243),\n",
              " ('Mom', 0.5656762719154358),\n",
              " ('Mona', 0.5489301085472107),\n",
              " ('Macbeth', 0.4326525926589966),\n",
              " ('Maggie', 0.43218734860420227),\n",
              " ('hippo', 0.4212038516998291)]"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Simpsons.wv.most_similar(positive=[\"Lisa\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Lisa', 0.7668273448944092),\n",
              " ('Homer', 0.7004038095474243),\n",
              " ('Grampa', 0.6456120014190674),\n",
              " ('Marge', 0.596946120262146),\n",
              " ('Dad', 0.5533592700958252),\n",
              " ('Mom', 0.5386731624603271),\n",
              " ('Whatd', 0.45966455340385437),\n",
              " ('Lis', 0.45413124561309814),\n",
              " ('Mona', 0.45376166701316833),\n",
              " ('dentist', 0.43352335691452026)]"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Simpsons.wv.most_similar(positive=[\"Bart\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Marge', 0.7615135908126831),\n",
              " ('Grampa', 0.750906229019165),\n",
              " ('Lisa', 0.7183126211166382),\n",
              " ('Bart', 0.7004038095474243),\n",
              " ('Mona', 0.4953669309616089),\n",
              " ('honey', 0.45786911249160767),\n",
              " ('hippo', 0.428343802690506),\n",
              " ('brownie', 0.42758703231811523),\n",
              " ('Herb', 0.4199666976928711),\n",
              " ('Doc', 0.4162862300872803)]"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Simpsons.wv.most_similar(positive=[\"Homer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Homer', 0.7615135908126831),\n",
              " ('Lisa', 0.7249566316604614),\n",
              " ('Grampa', 0.6683892011642456),\n",
              " ('Homie', 0.6052965521812439),\n",
              " ('Mona', 0.5997169017791748),\n",
              " ('Bart', 0.596946120262146),\n",
              " ('honey', 0.5731984972953796),\n",
              " ('sweetie', 0.4978075623512268),\n",
              " ('Honey', 0.4702324867248535),\n",
              " ('Maggie', 0.4505022466182709)]"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Simpsons.wv.most_similar(positive=[\"Marge\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Podemos hacer un analisis pca de algunas plabras clave para ver como se distribuyen:\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_pca_scatterplot(model, words=None, sample=0):\n",
        "    if words == None:\n",
        "        if sample > 0:\n",
        "            words = np.random.choice(list(model.wv.index_to_key), sample)\n",
        "        else:\n",
        "            words = [word for word in model.wv.index_to_key]\n",
        "    \n",
        "    word_vectors = np.array([model.wv[word] for word in words])\n",
        "    \n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "    \n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)\n",
        "    plt.title(\"PCA Simpsons\")\n",
        "    plt.show()\n",
        "\n",
        "display_pca_scatterplot(Simpsons,\n",
        "                        [\"Homer\", \"Marge\", \"Bart\", \"Lisa\", \"Maggie\", \"Ned\", \"Milhouse\", \"Krusty\", \"Burns\", \"Moe\"])\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTA: En el analisis pca puede observarse que Lisa esta cerca de Bart y Marge esta cerca de Homer. Ademas los miembros de la familia Simpsons esta cerca entre ellos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IRCB-jqgTNcs"
      },
      "source": [
        "### **Parte 4 (1 Punto): Aplicar embeddings para clasificar**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zlqzlJRSTNcs"
      },
      "source": [
        "Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n",
        "\n",
        "Para esto ocuparemos el lexic√≥n AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotaci√≥n es positiva y un -1 si es negativa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "CMskFDmHTNcs"
      },
      "outputs": [],
      "source": [
        "AFINN = 'data/AFINN_full.csv'\n",
        "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uaKl8hsCTNcs"
      },
      "source": [
        "Hint: Para w2v son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendr√°n una representaci√≥n en AFINN. Pueden utilizar esta funci√≥n auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "tWSSuctiTNcs"
      },
      "outputs": [],
      "source": [
        "def try_apply(model,word):\n",
        "    try:\n",
        "        aux = model[word]\n",
        "        return True\n",
        "    except KeyError:\n",
        "        #logger.error('Word {} not in dictionary'.format(word))\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>tops</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wasting</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>complaining</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>super</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>loving</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>awaits</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fool</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>attractive</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>admire</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bold</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>899 rows √ó 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             score\n",
              "word              \n",
              "tops             1\n",
              "wasting         -1\n",
              "complaining     -1\n",
              "super            1\n",
              "loving           1\n",
              "...            ...\n",
              "awaits          -1\n",
              "fool            -1\n",
              "attractive       1\n",
              "admire           1\n",
              "bold             1\n",
              "\n",
              "[899 rows x 1 columns]"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Aplicamos la funcion try_apply para filtrar las palabras que no estan en el modelo de word2vec Simpsons:\n",
        "df_afinn['in_model'] = df_afinn[0].apply(lambda x: try_apply(Simpsons.wv,x))\n",
        "df_afinn = df_afinn[df_afinn['in_model'] == True]\n",
        "df_afinn = df_afinn.drop('in_model', axis=1)\n",
        "df_afinn.columns = ['word', 'score']\n",
        "df_afinn = df_afinn.set_index('word')\n",
        "df_afinn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LrVPeEzgTNcs"
      },
      "source": [
        "**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representaci√≥n en embedding que acabamos de calcular (con ambos modelos). \n",
        "\n",
        "Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n",
        "\n",
        "Para ambos modelos, separar train y test de acuerdo a la siguiente funci√≥n. **(0.75 puntos)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "0Bkt26BwTNcs"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_afinn.index.values, df_afinn['score'].values, random_state=0, test_size=0.1, stratify=df_afinn['score'].values)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iDcq5czXTNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kDKe4gA3TNct"
      },
      "source": [
        "**Pregunta 2**: Entrenar una regresi√≥n log√≠stica (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qu√© se obtienen estos resultados? C√≥mo los mejorar√≠as? **(0.75 puntos)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.75      0.86      0.80        51\n",
            "           1       0.77      0.62      0.69        39\n",
            "\n",
            "    accuracy                           0.76        90\n",
            "   macro avg       0.76      0.74      0.74        90\n",
            "weighted avg       0.76      0.76      0.75        90\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Entrenamos un modelo de clasificacion con regresion logistica, suando como features los vectores de word2vec:\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
        "\n",
        "def train_model(model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "model = LogisticRegression()\n",
        "train_model(model, Simpsons.wv[X_train], y_train, Simpsons.wv[X_test], y_test)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hJMzq_dETNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "izppruGQTNct"
      },
      "source": [
        "# Bonus: +0.25 puntos en cualquier pregunta"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YW0aeK2KTNct"
      },
      "source": [
        "**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset m√°s grande y obtener mejores resultados. Les puede servir [√©sta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qvHcVS3sTNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSc8p-T8TNcu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
