{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckbt7VPDhBwb"
      },
      "source": [
        "# **Tarea 3 - Word Embeddings üìö**\n",
        "\n",
        "**Integrantes:**\n",
        "\n",
        "**Fecha l√≠mite de entrega üìÜ:** 16 de mayo.\n",
        "\n",
        "**Tiempo estimado de dedicaci√≥n:**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-19T18:30:18.109327Z",
          "start_time": "2020-03-19T18:30:18.103344Z"
        },
        "id": "q5CSRY4oNCHK"
      },
      "source": [
        "\n",
        "**Instrucciones:**\n",
        "- El ejercicio consiste en:\n",
        "    - Responder preguntas relativas a los contenidos vistos en los v√≠deos y slides de las clases.\n",
        "    - Implementar el m√©todo de la Word Context Matrix. \n",
        "    - Entrenar Word2Vec y FastText sobre un peque√±o corpus.\n",
        "    - Evaluar los embeddings obtenidos en una tarea de clasificaci√≥n.\n",
        "- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo **Jupyter Notebook**.\n",
        "- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n",
        "\n",
        "\n",
        "**Referencias**\n",
        "\n",
        "V√≠deos: \n",
        "\n",
        "- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n",
        "- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n",
        "- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G4wYf0vgnbTv"
      },
      "source": [
        "## **Preguntas te√≥ricas üìï (3 puntos).** ##\n",
        "Para estas preguntas no es necesario implementar c√≥digo, pero pueden utilizar pseudo c√≥digo."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B5hUG6-8ngoK"
      },
      "source": [
        "### **Parte 1: Modelos Lineales (1.5 ptos)**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5yRvZbhsoi8f"
      },
      "source": [
        "Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categor√≠as: pol√≠tica, deporte, negocios y otros. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "irsqBVmCnx3M"
      },
      "source": [
        "**Pregunta 1**: Dise√±e un modelo lineal capaz de clasificar un documento seg√∫n estas categor√≠as donde el output sea un vector con una distribuci√≥n de probabilidad con la pertenencia a cada clase. \n",
        "\n",
        "Especifique: representaci√≥n de los documentos de entrada, par√°metros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y funci√≥n de p√©rdida escogida. **(0.75 puntos)**\n",
        "\n",
        "**Respuesta**: \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G5FaWqBVvL90"
      },
      "source": [
        "**Pregunta 2**: Explique c√≥mo funciona el proceso de entrenamiento en este tipo de modelos y su evaluaci√≥n. **(0.75 puntos)**\n",
        "\n",
        "**Respuesta**: "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XkK7pc54njZq"
      },
      "source": [
        "### **Parte 2: Redes Neuronales (1.5 ptos)** "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VUbJjlj_9AFC"
      },
      "source": [
        "Supongamos que tenemos la siguiente red neuronal."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "obUfuOYB_TOC"
      },
      "source": [
        "![image.png](https://drive.google.com/uc?export=view&id=1nV1G0dOeVGPn40qGcGF9l_pVEFNtLU-w)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s2z-8zKW0_6q"
      },
      "source": [
        "**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matem√°tica. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en funci√≥n del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n",
        "\n",
        "Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.75 Puntos)**\n",
        "\n",
        "**Respuesta**: \n",
        "\n",
        "Formula:\n",
        "$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) =$\n",
        "\n",
        "Dimensiones: \n",
        "\n",
        "**Pregunta 2**: Explique qu√© es backpropagation. ¬øCuales ser√≠an los par√°metros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n",
        "\n",
        "**Respuesta**:\n",
        "\n",
        "**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.5 puntos)**\n",
        "\n",
        "**Respuesta**:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ocS_vQhR1gcU"
      },
      "source": [
        "## **Preguntas pr√°cticas üíª (3 puntos).** ##"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D0wk5GBkSE73"
      },
      "source": [
        "### Parte 3 A (1 Punto): Word Contex Matrix"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e_mh12Z9SF-J"
      },
      "source": [
        "\n",
        "\n",
        "En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librer√≠as ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```\n",
        "\n",
        "```python\n",
        "class WordContextMatrix:\n",
        "\n",
        "  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Utilice el constructor para definir los parametros.\n",
        "    \"\"\"\n",
        "\n",
        "    # se sugiere agregar un una estructura de datos para guardar las\n",
        "    # palabras del vocab y para guardar el conteo de coocurrencia\n",
        "    # si lo necesita puede agregar m√°s parametros pero no puede cambiar el resto\n",
        "    ...\n",
        "    \n",
        "  def build_vocab(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para construir el vocabulario\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    # Le puede ser √∫til considerar un token unk al vocab\n",
        "    # para palabras fuera del vocab\n",
        "    ...\n",
        "  \n",
        "  def build_matrix(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para crear la palabra contexto\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  def get_matrix(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para obtener la matriz palabra contexto. \n",
        "    \"\"\"\n",
        "\n",
        "    # se recomienda transformar la matrix a un diccionario de embedding.\n",
        "    # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n",
        "    ...\n",
        "\n",
        "```\n",
        "\n",
        "puede modificar los par√°metros o m√©todos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n",
        "\n",
        "```python\n",
        "corpus = [\n",
        "  \"I like deep learning.\",\n",
        "  \"I like NLP.\",\n",
        "  \"I enjoy flying.\"\n",
        "]\n",
        "```\n",
        "\n",
        "Obteniendo una matriz parecia a esta:\n",
        "\n",
        "***Resultado esperado***: \n",
        "\n",
        "| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n",
        "|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n",
        "| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n",
        "| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n",
        "| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n",
        "| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n",
        "| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n",
        "| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n",
        "| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n",
        "| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n",
        "\n",
        "``\n",
        "\n",
        "Verifique si su matrix es igual a esta utilizando el corpus de ejemplo. Ojo que este es s√≥lo un ejemplo, su algoritmo debe **generalizar** a otros ejemplos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = [\n",
        "  \"I like deep learning.\",\n",
        "  \"I like NLP.\",\n",
        "  \"I enjoy flying.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "#definimos duncion tokenizer for every sentence:\n",
        "def tokenizer(sentence):\n",
        "\n",
        "    tokens = sentence.split()\n",
        "\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class WordContextMatrix:\n",
        "\n",
        "  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.window_size = window_size\n",
        "    self.dataset = dataset\n",
        "    self.tokenizer = tokenizer\n",
        "    self.matrix = np.zeros((vocab_size, vocab_size))\n",
        "\n",
        "  #Definimos la funci√≥n para construir el vocabulario:\n",
        "  def build_vocab(self):\n",
        "    vocab = []\n",
        "    for sentence in self.dataset:\n",
        "      for word in self.tokenizer(sentence):\n",
        "        vocab.append(word)\n",
        "    return list(set(vocab))\n",
        "  \n",
        "  #Definimos la funci√≥n para construir la matriz de contexto:\n",
        "  def build_matrix(self):\n",
        "    vocab = self.build_vocab()\n",
        "    for sentence in self.dataset:\n",
        "      for i in range(0, len(self.tokenizer(sentence))):\n",
        "        for j in range(i - self.window_size, i + self.window_size + 1):\n",
        "          if j >= 0 and j < len(self.tokenizer(sentence)) and j != i:\n",
        "            #print('w=', self.tokenizer(sentence)[i], 'context=', self.tokenizer(sentence)[j], i, j)\n",
        "            self.matrix[vocab.index(self.tokenizer(sentence)[i])][vocab.index(self.tokenizer(sentence)[j])] += 1\n",
        "    return self.matrix\n",
        "  \n",
        "  #Definimos la funci√≥n para obtener la matriz de contexto:\n",
        "  def get_matrix(self):\n",
        "    return self.matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'enjoy', 'like', 'flying.', 'NLP.', 'learning.', 'deep']\n",
            "14\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>I</th>\n",
              "      <th>enjoy</th>\n",
              "      <th>like</th>\n",
              "      <th>flying.</th>\n",
              "      <th>NLP.</th>\n",
              "      <th>learning.</th>\n",
              "      <th>deep</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>I</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>enjoy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>like</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flying.</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NLP.</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>learning.</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>deep</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           I  enjoy  like  flying.  NLP.  learning.  deep\n",
              "I          0      1     2        0     0          0     0\n",
              "enjoy      1      0     0        1     0          0     0\n",
              "like       2      0     0        0     1          0     1\n",
              "flying.    0      1     0        0     0          0     0\n",
              "NLP.       0      0     1        0     0          0     0\n",
              "learning.  0      0     0        0     0          0     1\n",
              "deep       0      0     1        0     0          1     0"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context_matrix = WordContextMatrix(7, 1, corpus, tokenizer) # Tama√±o del volabulario se define manualmente ej: 7\n",
        "\n",
        "print(context_matrix.build_vocab()[0:10])\n",
        "\n",
        "CM = context_matrix.build_matrix()\n",
        "#Imprimimos la suma de elentos de CM en int:\n",
        "print(np.sum(CM, dtype=int))\n",
        "\n",
        "#add the matrix to a dataframe including vicabulary as index and columns:\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(CM, index=context_matrix.build_vocab()[0:10], columns=context_matrix.build_vocab()[0:10], dtype=int)\n",
        "df\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol82nJ0FnmcP"
      },
      "source": [
        "### **Parte 3 B (1 Punto): Word Embeddings**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OgmeSFqKLpFL"
      },
      "source": [
        "En la auxiliar 2 aprendieron como entrenar Word2Vec utilizando gensim. El objetivo de esta parte es comparar los embeddings obtenidos con dos modelos \n",
        "diferentes: Word2Vec y [FastText](https://radimrehurek.com/gensim/models/fasttext.html) (utilizen size=200 en FastText) entrenados en el mismo dataset de di√°logos de los Simpson. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CgSlIxJW36ZA"
      },
      "source": [
        "# Secci√≥n nueva"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ecCvnryeQiG7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
            "  \"class\": algorithms.Blowfish,\n"
          ]
        }
      ],
      "source": [
        "import re  \n",
        "import pandas as pd \n",
        "from time import time  \n",
        "from collections import defaultdict \n",
        "import string \n",
        "import multiprocessing\n",
        "import os\n",
        "import gensim\n",
        "import sklearn\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
        "\n",
        "# word2vec\n",
        "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tZgN06q4QPi3"
      },
      "source": [
        "Utilizando el dataset adjunto con la tarea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY3kmg4onnsu",
        "outputId": "d3525a54-0c10-401e-b3e2-9c6e9e714a2c"
      },
      "outputs": [],
      "source": [
        "data_file = \"data/simpsons_dataset.csv\"\n",
        "df = pd.read_csv(data_file)\n",
        "stopwords = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
        ").values\n",
        "stopwords = Counter(stopwords.flatten().tolist())\n",
        "df = df.dropna().reset_index(drop=True) # Quitar filas vacias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>raw_character_text</th>\n",
              "      <th>spoken_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Miss Hoover</td>\n",
              "      <td>No, actually, it was a little of both. Sometim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lisa Simpson</td>\n",
              "      <td>Where's Mr. Bergstrom?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Miss Hoover</td>\n",
              "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lisa Simpson</td>\n",
              "      <td>That life is worth living.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Edna Krabappel-Flanders</td>\n",
              "      <td>The polls will be open from now until the end ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131848</th>\n",
              "      <td>Miss Hoover</td>\n",
              "      <td>I'm back.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131849</th>\n",
              "      <td>Miss Hoover</td>\n",
              "      <td>You see, class, my Lyme disease turned out to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131850</th>\n",
              "      <td>Miss Hoover</td>\n",
              "      <td>Psy-cho-so-ma-tic.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131851</th>\n",
              "      <td>Ralph Wiggum</td>\n",
              "      <td>Does that mean you were crazy?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131852</th>\n",
              "      <td>JANEY</td>\n",
              "      <td>No, that means she was faking it.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>131853 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             raw_character_text  \\\n",
              "0                   Miss Hoover   \n",
              "1                  Lisa Simpson   \n",
              "2                   Miss Hoover   \n",
              "3                  Lisa Simpson   \n",
              "4       Edna Krabappel-Flanders   \n",
              "...                         ...   \n",
              "131848              Miss Hoover   \n",
              "131849              Miss Hoover   \n",
              "131850              Miss Hoover   \n",
              "131851             Ralph Wiggum   \n",
              "131852                    JANEY   \n",
              "\n",
              "                                             spoken_words  \n",
              "0       No, actually, it was a little of both. Sometim...  \n",
              "1                                  Where's Mr. Bergstrom?  \n",
              "2       I don't know. Although I'd sure like to talk t...  \n",
              "3                              That life is worth living.  \n",
              "4       The polls will be open from now until the end ...  \n",
              "...                                                   ...  \n",
              "131848                                          I'm back.  \n",
              "131849  You see, class, my Lyme disease turned out to ...  \n",
              "131850                                 Psy-cho-so-ma-tic.  \n",
              "131851                     Does that mean you were crazy?  \n",
              "131852                  No, that means she was faking it.  \n",
              "\n",
              "[131853 rows x 2 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VAg5a5bmWk3T"
      },
      "source": [
        "**Pregunta 1**: Ayud√°ndose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec y FastText sobre el dataset anterior. **(1 punto)** (Hint, le puede servir explorar un poco los datos)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MWw2fXFRXe5Y"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Bvwplz7yTNcr"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(131853,)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# unir titulo con contenido de la noticia\n",
        "content = df['raw_character_text'] + df['spoken_words']\n",
        "content.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# limpiar puntuaciones y separar por tokens.\n",
        "punctuation = string.punctuation + \"¬´¬ª‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî\"\n",
        "stopwords = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt'\n",
        ").values\n",
        "stopwords = Counter(stopwords.flatten().tolist())\n",
        "\n",
        "def simple_tokenizer(doc, lower=False):\n",
        "    if lower:\n",
        "        tokenized_doc = doc.translate(str.maketrans(\n",
        "            '', '', punctuation)).lower().split()\n",
        "\n",
        "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
        "    tokenized_doc = [\n",
        "        token for token in tokenized_doc if token.lower() not in stopwords\n",
        "    ]\n",
        "    return tokenized_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ejemplo de alguna noticia: ['Lisa', 'SimpsonI', 'see', 'touched', 'you', 'too']\n"
          ]
        }
      ],
      "source": [
        "cleaned_content = [simple_tokenizer(doc) for doc in content.values]\n",
        "print(\"Ejemplo de alguna noticia: {}\".format(cleaned_content[14]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phrases recibe una lista de oraciones, y junta bigramas que est√©n al menos 100 veces repetidos como un √∫nico token. Por ejemplo, \"new york\" se convierte en \"new_york\".\n",
        "phrases = Phrases(cleaned_content, min_count=100, progress_per=5000) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-19 16:53:13,451 : INFO : exporting phrases from Phrases<597019 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n",
            "2023-05-19 16:53:14,204 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<91 phrases, min_count=100, threshold=10.0> from Phrases<597019 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000> in 0.75s', 'datetime': '2023-05-19T16:53:14.204855', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "bigram = Phraser(phrases)\n",
        "sentences = bigram[cleaned_content]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Miss', 'HooverNo', 'actually', 'it', 'was', 'a', 'little', 'of', 'both', 'Sometimes', 'when', 'a', 'disease', 'is', 'in', 'all', 'the', 'magazines', 'and', 'all', 'the', 'news', 'shows', 'its', 'only', 'natural', 'that', 'you', 'think', 'you', 'have', 'it'], ['Lisa', 'SimpsonWheres', 'Mr', 'Bergstrom'], ['Miss', 'HooverI', 'dont_know', 'Although', 'Id', 'sure', 'like', 'to', 'talk', 'to', 'him', 'didnt', 'touch', 'my', 'lesson', 'plan', 'What', 'did', 'teach', 'you'], ['Lisa', 'SimpsonThat', 'life', 'is', 'worth', 'living'], ['Edna', 'KrabappelFlandersThe', 'polls', 'will_be', 'open', 'from', 'now', 'until', 'the', 'end', 'of', 'recess', 'Now', 'just', 'in', 'case', 'any', 'of', 'you', 'have', 'decided', 'to', 'put', 'any', 'thought', 'into', 'this', 'well', 'have', 'our', 'final', 'statements', 'Martin'], ['Martin', 'PrinceI', 'dont', 'think', 'theres', 'anything', 'left', 'to', 'say'], ['Edna', 'KrabappelFlandersBart'], ['Bart', 'SimpsonVictory', 'party', 'under', 'the', 'slide'], ['Lisa', 'SimpsonMr', 'Bergstrom', 'Mr', 'Bergstrom'], ['LandladyHey', 'hey', 'Moved', 'out', 'this', 'morning', 'must', 'have', 'a', 'new', 'job', 'took', 'his', 'Copernicus', 'costume']]\n"
          ]
        }
      ],
      "source": [
        "# para ver como quedan los textos tokenizadas, quitar comentario a la siguiente linea:\n",
        "print(list(sentences)[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-19 16:53:16,790 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=200, alpha=0.03>', 'datetime': '2023-05-19T16:53:16.790638', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'created'}\n",
            "2023-05-19 16:53:16,791 : INFO : FastText lifecycle event {'params': 'FastText<vocab=0, vector_size=200, alpha=0.03>', 'datetime': '2023-05-19T16:53:16.791793', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "#Modelo woed2vec:\n",
        "Simpsons_w2v = Word2Vec(min_count=10,\n",
        "                      window=4,\n",
        "                      vector_size=200,\n",
        "                      sample=6e-5,\n",
        "                      alpha=0.03,\n",
        "                      min_alpha=0.0007,\n",
        "                      negative=20,\n",
        "                      workers=multiprocessing.cpu_count())\n",
        "\n",
        "\n",
        "#Modelo FastText:\n",
        "Simpsons_ft = FastText(min_count=10,\n",
        "                        window=4,\n",
        "                        vector_size=200,\n",
        "                        sample=6e-5,\n",
        "                        alpha=0.03,\n",
        "                        min_alpha=0.0007,\n",
        "                        negative=20,\n",
        "                        workers=multiprocessing.cpu_count())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Construimos el vocabulario:\n",
        "Simpsons_w2v.build_vocab(sentences, progress_per=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-19 16:50:21,343 : INFO : Word2Vec lifecycle event {'msg': 'training model with 8 workers on 8907 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4 shrink_windows=True', 'datetime': '2023-05-19T16:50:21.343312', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'train'}\n",
            "2023-05-19 16:50:22,361 : INFO : EPOCH 0 - PROGRESS: at 59.67% examples, 309672 words/s, in_qsize 16, out_qsize 0\n",
            "2023-05-19 16:50:22,756 : INFO : EPOCH 0: training on 1372110 raw words (527877 effective words) took 1.4s, 375578 effective words/s\n",
            "2023-05-19 16:50:23,782 : INFO : EPOCH 1 - PROGRESS: at 58.96% examples, 303453 words/s, in_qsize 14, out_qsize 1\n",
            "2023-05-19 16:50:24,159 : INFO : EPOCH 1: training on 1372110 raw words (528297 effective words) took 1.4s, 378370 effective words/s\n",
            "2023-05-19 16:50:25,186 : INFO : EPOCH 2 - PROGRESS: at 60.40% examples, 310608 words/s, in_qsize 15, out_qsize 0\n",
            "2023-05-19 16:50:25,563 : INFO : EPOCH 2: training on 1372110 raw words (528410 effective words) took 1.4s, 378068 effective words/s\n",
            "2023-05-19 16:50:26,608 : INFO : EPOCH 3 - PROGRESS: at 63.91% examples, 324223 words/s, in_qsize 15, out_qsize 0\n",
            "2023-05-19 16:50:27,029 : INFO : EPOCH 3: training on 1372110 raw words (528600 effective words) took 1.5s, 362398 effective words/s\n",
            "2023-05-19 16:50:28,083 : INFO : EPOCH 4 - PROGRESS: at 61.10% examples, 306438 words/s, in_qsize 13, out_qsize 4\n",
            "2023-05-19 16:50:28,424 : INFO : EPOCH 4: training on 1372110 raw words (527933 effective words) took 1.4s, 380161 effective words/s\n",
            "2023-05-19 16:50:29,431 : INFO : EPOCH 5 - PROGRESS: at 50.52% examples, 262569 words/s, in_qsize 15, out_qsize 0\n",
            "2023-05-19 16:50:29,966 : INFO : EPOCH 5: training on 1372110 raw words (528304 effective words) took 1.5s, 344057 effective words/s\n",
            "2023-05-19 16:50:30,985 : INFO : EPOCH 6 - PROGRESS: at 56.20% examples, 290551 words/s, in_qsize 12, out_qsize 4\n",
            "2023-05-19 16:50:31,425 : INFO : EPOCH 6: training on 1372110 raw words (528625 effective words) took 1.5s, 364054 effective words/s\n",
            "2023-05-19 16:50:32,433 : INFO : EPOCH 7 - PROGRESS: at 54.81% examples, 285789 words/s, in_qsize 11, out_qsize 0\n",
            "2023-05-19 16:50:32,919 : INFO : EPOCH 7: training on 1372110 raw words (528404 effective words) took 1.5s, 355276 effective words/s\n",
            "2023-05-19 16:50:33,939 : INFO : EPOCH 8 - PROGRESS: at 58.95% examples, 305421 words/s, in_qsize 13, out_qsize 0\n",
            "2023-05-19 16:50:34,477 : INFO : EPOCH 8: training on 1372110 raw words (528295 effective words) took 1.6s, 340627 effective words/s\n",
            "2023-05-19 16:50:35,510 : INFO : EPOCH 9 - PROGRESS: at 58.26% examples, 297834 words/s, in_qsize 15, out_qsize 0\n",
            "2023-05-19 16:50:35,902 : INFO : EPOCH 9: training on 1372110 raw words (529280 effective words) took 1.4s, 373179 effective words/s\n",
            "2023-05-19 16:50:36,908 : INFO : EPOCH 10 - PROGRESS: at 59.68% examples, 313302 words/s, in_qsize 15, out_qsize 0\n",
            "2023-05-19 16:50:37,275 : INFO : EPOCH 10: training on 1372110 raw words (528592 effective words) took 1.4s, 386795 effective words/s\n",
            "2023-05-19 16:50:38,309 : INFO : EPOCH 11 - PROGRESS: at 60.37% examples, 308425 words/s, in_qsize 11, out_qsize 5\n",
            "2023-05-19 16:50:38,654 : INFO : EPOCH 11: training on 1372110 raw words (528556 effective words) took 1.4s, 385098 effective words/s\n",
            "2023-05-19 16:50:39,666 : INFO : EPOCH 12 - PROGRESS: at 60.38% examples, 315339 words/s, in_qsize 15, out_qsize 0\n",
            "2023-05-19 16:50:40,132 : INFO : EPOCH 12: training on 1372110 raw words (528458 effective words) took 1.5s, 359051 effective words/s\n",
            "2023-05-19 16:50:41,235 : INFO : EPOCH 13 - PROGRESS: at 66.07% examples, 317449 words/s, in_qsize 15, out_qsize 0\n",
            "2023-05-19 16:50:41,522 : INFO : EPOCH 13: training on 1372110 raw words (528832 effective words) took 1.4s, 382374 effective words/s\n",
            "2023-05-19 16:50:42,531 : INFO : EPOCH 14 - PROGRESS: at 58.99% examples, 308948 words/s, in_qsize 15, out_qsize 0\n",
            "2023-05-19 16:50:42,908 : INFO : EPOCH 14: training on 1372110 raw words (528534 effective words) took 1.4s, 383169 effective words/s\n",
            "2023-05-19 16:50:43,916 : INFO : EPOCH 15 - PROGRESS: at 56.20% examples, 293697 words/s, in_qsize 13, out_qsize 5\n",
            "2023-05-19 16:50:44,315 : INFO : EPOCH 15: training on 1372110 raw words (528474 effective words) took 1.4s, 377542 effective words/s\n",
            "2023-05-19 16:50:45,360 : INFO : EPOCH 16 - PROGRESS: at 62.50% examples, 316090 words/s, in_qsize 15, out_qsize 0\n",
            "2023-05-19 16:50:45,683 : INFO : EPOCH 16: training on 1372110 raw words (528209 effective words) took 1.4s, 388072 effective words/s\n",
            "2023-05-19 16:50:46,694 : INFO : EPOCH 17 - PROGRESS: at 59.64% examples, 311919 words/s, in_qsize 14, out_qsize 2\n",
            "2023-05-19 16:50:47,166 : INFO : EPOCH 17: training on 1372110 raw words (528146 effective words) took 1.5s, 357818 effective words/s\n",
            "2023-05-19 16:50:48,194 : INFO : EPOCH 18 - PROGRESS: at 63.95% examples, 329106 words/s, in_qsize 11, out_qsize 0\n",
            "2023-05-19 16:50:48,554 : INFO : EPOCH 18: training on 1372110 raw words (528266 effective words) took 1.4s, 382305 effective words/s\n",
            "2023-05-19 16:50:49,568 : INFO : EPOCH 19 - PROGRESS: at 60.40% examples, 314766 words/s, in_qsize 15, out_qsize 0\n",
            "2023-05-19 16:50:49,933 : INFO : EPOCH 19: training on 1372110 raw words (528170 effective words) took 1.4s, 384856 effective words/s\n",
            "2023-05-19 16:50:50,970 : INFO : EPOCH 20 - PROGRESS: at 61.10% examples, 311802 words/s, in_qsize 15, out_qsize 0\n",
            "2023-05-19 16:50:51,313 : INFO : EPOCH 20: training on 1372110 raw words (529147 effective words) took 1.4s, 385306 effective words/s\n",
            "2023-05-19 16:50:52,335 : INFO : EPOCH 21 - PROGRESS: at 60.37% examples, 311978 words/s, in_qsize 15, out_qsize 0\n",
            "2023-05-19 16:50:52,695 : INFO : EPOCH 21: training on 1372110 raw words (528285 effective words) took 1.4s, 384256 effective words/s\n",
            "2023-05-19 16:50:53,716 : INFO : EPOCH 22 - PROGRESS: at 60.40% examples, 312488 words/s, in_qsize 15, out_qsize 0\n",
            "2023-05-19 16:50:54,179 : INFO : EPOCH 22: training on 1372110 raw words (528458 effective words) took 1.5s, 357823 effective words/s\n",
            "2023-05-19 16:50:55,213 : INFO : EPOCH 23 - PROGRESS: at 61.08% examples, 312720 words/s, in_qsize 15, out_qsize 0\n",
            "2023-05-19 16:50:55,563 : INFO : EPOCH 23: training on 1372110 raw words (528675 effective words) took 1.4s, 383830 effective words/s\n",
            "2023-05-19 16:50:56,586 : INFO : EPOCH 24 - PROGRESS: at 59.71% examples, 308704 words/s, in_qsize 11, out_qsize 6\n",
            "2023-05-19 16:50:56,951 : INFO : EPOCH 24: training on 1372110 raw words (529400 effective words) took 1.4s, 383345 effective words/s\n",
            "2023-05-19 16:50:56,952 : INFO : Word2Vec lifecycle event {'msg': 'training on 34302750 raw words (13212227 effective words) took 35.6s, 371045 effective words/s', 'datetime': '2023-05-19T16:50:56.952106', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'train'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to train the model: 0.59 mins\n"
          ]
        }
      ],
      "source": [
        "#Entrenamos modelo w2v\n",
        "t = time()\n",
        "Simpsons_w2v.train(sentences, total_examples=Simpsons_w2v.corpus_count, epochs=25, report_delay=10)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-19 16:53:22,160 : INFO : collecting all words and their counts\n",
            "2023-05-19 16:53:22,161 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2023-05-19 16:53:22,253 : INFO : PROGRESS: at sentence #10000, processed 101844 words, keeping 15730 word types\n",
            "2023-05-19 16:53:22,324 : INFO : PROGRESS: at sentence #20000, processed 205879 words, keeping 25594 word types\n",
            "2023-05-19 16:53:22,404 : INFO : PROGRESS: at sentence #30000, processed 319516 words, keeping 34491 word types\n",
            "2023-05-19 16:53:22,476 : INFO : PROGRESS: at sentence #40000, processed 424692 words, keeping 41294 word types\n",
            "2023-05-19 16:53:22,547 : INFO : PROGRESS: at sentence #50000, processed 522288 words, keeping 47863 word types\n",
            "2023-05-19 16:53:22,617 : INFO : PROGRESS: at sentence #60000, processed 614063 words, keeping 53932 word types\n",
            "2023-05-19 16:53:22,690 : INFO : PROGRESS: at sentence #70000, processed 715810 words, keeping 60223 word types\n",
            "2023-05-19 16:53:22,765 : INFO : PROGRESS: at sentence #80000, processed 823239 words, keeping 66326 word types\n",
            "2023-05-19 16:53:22,839 : INFO : PROGRESS: at sentence #90000, processed 928559 words, keeping 72293 word types\n",
            "2023-05-19 16:53:22,914 : INFO : PROGRESS: at sentence #100000, processed 1034630 words, keeping 77908 word types\n",
            "2023-05-19 16:53:22,996 : INFO : PROGRESS: at sentence #110000, processed 1142844 words, keeping 83922 word types\n",
            "2023-05-19 16:53:23,078 : INFO : PROGRESS: at sentence #120000, processed 1248271 words, keeping 89307 word types\n",
            "2023-05-19 16:53:23,156 : INFO : PROGRESS: at sentence #130000, processed 1352861 words, keeping 93841 word types\n",
            "2023-05-19 16:53:23,171 : INFO : collected 94548 word types from a corpus of 1372110 raw words and 131853 sentences\n",
            "2023-05-19 16:53:23,171 : INFO : Creating a fresh vocabulary\n",
            "2023-05-19 16:53:23,213 : INFO : FastText lifecycle event {'msg': 'effective_min_count=10 retains 8907 unique words (9.42% of original 94548, drops 85641)', 'datetime': '2023-05-19T16:53:23.213874', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
            "2023-05-19 16:53:23,214 : INFO : FastText lifecycle event {'msg': 'effective_min_count=10 leaves 1214250 word corpus (88.50% of original 1372110, drops 157860)', 'datetime': '2023-05-19T16:53:23.214398', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
            "2023-05-19 16:53:23,245 : INFO : deleting the raw counts dictionary of 94548 items\n",
            "2023-05-19 16:53:23,247 : INFO : sample=6e-05 downsamples 805 most-common words\n",
            "2023-05-19 16:53:23,248 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 528492.7114590418 word corpus (43.5%% of prior 1214250)', 'datetime': '2023-05-19T16:53:23.248032', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
            "2023-05-19 16:53:23,328 : INFO : estimated required memory for 8907 words, 2000000 buckets and 200 dimensions: 1620303436 bytes\n",
            "2023-05-19 16:53:23,328 : INFO : resetting layer weights\n",
            "2023-05-19 16:53:25,406 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-05-19T16:53:25.406885', 'gensim': '4.3.1', 'python': '3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]', 'platform': 'Linux-5.19.0-41-generic-x86_64-with-glibc2.35', 'event': 'build_vocab'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to train the model: 0.05 mins\n"
          ]
        }
      ],
      "source": [
        "#Entrenamos modelo ft:\n",
        "t = time()\n",
        "Simpsons_ft.build_vocab(sentences, progress_per=10000)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### NOTA: Falta entrenar FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_116941/2085262728.py:2: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
            "  Simpsons_w2v.init_sims(replace=True)\n",
            "2023-05-19 16:51:20,535 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
          ]
        }
      ],
      "source": [
        "#Ahora que terminamos de entrenar el modelo, le indicamos que no lo entrenaremos mas. Esto nos permitir√° ejecutar eficientemente las tareas que realizaremos.\n",
        "Simpsons_w2v.init_sims(replace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lr8U5wOTNcr"
      },
      "source": [
        "**Pregunta 2**: Encuentre las palabras mas similares a las siguientes: Lisa, Bart, Homer, Marge. C√∫al es la diferencia entre ambos resultados? Por qu√© ocurre esto? Intente comparar ahora Liisa en ambos modelos (doble i). Cuando escoger√≠a uno vs el otro? **(0.5 puntos)**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yMLyGffVTNcs"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "w6RvJGpbTNcs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Bart', 0.8010185360908508),\n",
              " ('Marge', 0.7423198223114014),\n",
              " ('Homer', 0.7359989881515503),\n",
              " ('Grampa', 0.6516071557998657),\n",
              " ('Dad', 0.5698140263557434),\n",
              " ('Mom', 0.5520078539848328),\n",
              " ('Mona', 0.5205153226852417),\n",
              " ('Maggie', 0.4405081272125244),\n",
              " ('saxophone', 0.43468791246414185),\n",
              " ('pony', 0.41573822498321533)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Simpsons_w2v.wv.most_similar(positive=[\"Lisa\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Lisa', 0.8010185360908508),\n",
              " ('Homer', 0.7109986543655396),\n",
              " ('Marge', 0.6230092644691467),\n",
              " ('Grampa', 0.6223070025444031),\n",
              " ('Dad', 0.6031257510185242),\n",
              " ('Mom', 0.5371394157409668),\n",
              " ('Whatd', 0.4600909352302551),\n",
              " ('Lis', 0.4447820782661438),\n",
              " ('squeal', 0.4309525787830353),\n",
              " ('meatloaf', 0.4243447184562683)]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Simpsons_w2v.wv.most_similar(positive=[\"Bart\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Marge', 0.7779496908187866),\n",
              " ('Grampa', 0.7603975534439087),\n",
              " ('Lisa', 0.7359989881515503),\n",
              " ('Bart', 0.7109986543655396),\n",
              " ('Mona', 0.4952374994754791),\n",
              " ('Homie', 0.4469567537307739),\n",
              " ('thisll', 0.44663798809051514),\n",
              " ('sweetie', 0.4435059428215027),\n",
              " ('honey', 0.4401094317436218),\n",
              " ('Dad', 0.43942978978157043)]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Simpsons_w2v.wv.most_similar(positive=[\"Homer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Homer', 0.7779496908187866),\n",
              " ('Lisa', 0.7423198223114014),\n",
              " ('Grampa', 0.6660140752792358),\n",
              " ('Bart', 0.6230092644691467),\n",
              " ('Homie', 0.6041595935821533),\n",
              " ('Mona', 0.556485652923584),\n",
              " ('honey', 0.5446598529815674),\n",
              " ('sweetie', 0.5164464712142944),\n",
              " ('Maggie', 0.4595811367034912),\n",
              " ('Honey', 0.4521809220314026)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Simpsons_w2v.wv.most_similar(positive=[\"Marge\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Podemos hacer un analisis pca de algunas plabras clave para ver como se distribuyen:\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_pca_scatterplot(model, words=None, sample=0):\n",
        "    if words == None:\n",
        "        if sample > 0:\n",
        "            words = np.random.choice(list(model.wv.index_to_key), sample)\n",
        "        else:\n",
        "            words = [word for word in model.wv.index_to_key]\n",
        "    \n",
        "    word_vectors = np.array([model.wv[word] for word in words])\n",
        "    \n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "    \n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)\n",
        "    plt.title(\"PCA Simpsons\")\n",
        "    plt.show()\n",
        "\n",
        "display_pca_scatterplot(Simpsons_w2v,\n",
        "                        [\"Homer\", \"Marge\", \"Bart\", \"Lisa\", \"Maggie\", \"Ned\", \"Milhouse\", \"Krusty\", \"Burns\", \"Moe\"])\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTA: En el analisis pca puede observarse que Lisa esta cerca de Bart y Marge esta cerca de Homer. Ademas los miembros de la familia Simpsons esta cerca entre ellos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IRCB-jqgTNcs"
      },
      "source": [
        "### **Parte 4 (1 Punto): Aplicar embeddings para clasificar**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zlqzlJRSTNcs"
      },
      "source": [
        "Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n",
        "\n",
        "Para esto ocuparemos el lexic√≥n AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotaci√≥n es positiva y un -1 si es negativa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CMskFDmHTNcs"
      },
      "outputs": [],
      "source": [
        "AFINN = 'data/AFINN_full.csv'\n",
        "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uaKl8hsCTNcs"
      },
      "source": [
        "Hint: Para w2v son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendr√°n una representaci√≥n en AFINN. Pueden utilizar esta funci√≥n auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tWSSuctiTNcs"
      },
      "outputs": [],
      "source": [
        "def try_apply(model,word):\n",
        "    try:\n",
        "        aux = model[word]\n",
        "        return True\n",
        "    except KeyError:\n",
        "        #logger.error('Word {} not in dictionary'.format(word))\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>tops</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wasting</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>complaining</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>super</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>loving</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>awaits</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fool</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>attractive</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>admire</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bold</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>899 rows √ó 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             score\n",
              "word              \n",
              "tops             1\n",
              "wasting         -1\n",
              "complaining     -1\n",
              "super            1\n",
              "loving           1\n",
              "...            ...\n",
              "awaits          -1\n",
              "fool            -1\n",
              "attractive       1\n",
              "admire           1\n",
              "bold             1\n",
              "\n",
              "[899 rows x 1 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Aplicamos la funcion try_apply para filtrar las palabras que no estan en el modelo de word2vec Simpsons:\n",
        "df_afinn['in_model'] = df_afinn[0].apply(lambda x: try_apply(Simpsons_w2v.wv,x))\n",
        "df_afinn = df_afinn[df_afinn['in_model'] == True]\n",
        "df_afinn = df_afinn.drop('in_model', axis=1)\n",
        "df_afinn.columns = ['word', 'score']\n",
        "df_afinn = df_afinn.set_index('word')\n",
        "df_afinn"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LrVPeEzgTNcs"
      },
      "source": [
        "**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representaci√≥n en embedding que acabamos de calcular (con ambos modelos). \n",
        "\n",
        "Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n",
        "\n",
        "Para ambos modelos, separar train y test de acuerdo a la siguiente funci√≥n. **(0.75 puntos)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0Bkt26BwTNcs"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_afinn.index.values, df_afinn['score'].values, random_state=0, test_size=0.1, stratify=df_afinn['score'].values)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iDcq5czXTNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kDKe4gA3TNct"
      },
      "source": [
        "**Pregunta 2**: Entrenar una regresi√≥n log√≠stica (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qu√© se obtienen estos resultados? C√≥mo los mejorar√≠as? **(0.75 puntos)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.79      0.90      0.84        51\n",
            "           1       0.84      0.69      0.76        39\n",
            "\n",
            "    accuracy                           0.81        90\n",
            "   macro avg       0.82      0.80      0.80        90\n",
            "weighted avg       0.82      0.81      0.81        90\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Entrenamos un modelo de clasificacion con regresion logistica, suando como features los vectores de word2vec:\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
        "\n",
        "def train_model(model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "model = LogisticRegression()\n",
        "train_model(model, Simpsons_w2v.wv[X_train], y_train, Simpsons_w2v.wv[X_test], y_test)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hJMzq_dETNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "izppruGQTNct"
      },
      "source": [
        "# Bonus: +0.25 puntos en cualquier pregunta"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YW0aeK2KTNct"
      },
      "source": [
        "**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset m√°s grande y obtener mejores resultados. Les puede servir [√©sta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qvHcVS3sTNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSc8p-T8TNcu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
