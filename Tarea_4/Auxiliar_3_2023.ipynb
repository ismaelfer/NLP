{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rc4NC3hnhjZ"
      },
      "source": [
        "<h1><center>Auxiliar 3: Introducci칩n a Pytorch 游댠</center></h1>\n",
        "\n",
        "<center><strong>CC6205: Procesamiento del Lenguaje Natural</strong></center>\n",
        "\n",
        "\n",
        "## 游닄 **Objetivos de la clase** 游닄\n",
        "\n",
        "La clase auxiliar de esta semana tendr치 varios objetivos:\n",
        "\n",
        "- Introducir PyTorch, un Framework para trabajar con Deep Learning\n",
        "- Definir y explicar el concepto de Tensor.\n",
        "- Motivar el uso de Mini-Batch.\n",
        "- Explicar el uso de GPU en Deep Learning.\n",
        "- Explicar el c치lculo de derivadas con PyTorch.\n",
        "- Construir un modelo de Embedding m치s una red Feed Forward 游쑆n",
        "- Construir un modelo de Embedding m치s una CNN 游쑆n",
        "\n",
        "Una vez resuelto, pueden utilizar cualquier parte del c칩digo que les parezca prudente para futuras tareas 游땕."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLLTxJRSwscP"
      },
      "source": [
        "# **Parte 1: Introducci칩n**\n",
        "\n",
        "<!-- Tomar en consideracion que los chicos no han visto nada practico en deep learning. No se les explicaron cosas como optimizers, schedulers, etc asique hay que ser pedagogicos.\n",
        "\n",
        "* Motivacion, trabajo con tensores, dimensionalidad alta, optimizacion en gpu, utilidad de computo por batch (eficiencia, robustez del modelo, etc)\n",
        "\n",
        "\n",
        "* introducir la api, las operaciones basicas, operaciones de creacion, operaciones inplace, cambiar la forma de los tensores, etc\n",
        "\n",
        "* mostrar el uso de la gpu con ejemplos (mostrar nvidia-smi), revisar codigo agnostico al dispositivo -->\n",
        "\n",
        "\n",
        "------------------------------------------------------\n",
        "En esta auxiliar vamos a introducir PyTorch, un framework para hacer deep learning, y tambi칠n mostrar dos aplicaciones. Esta herramienta va a ser usada de aqu칤 hasta el final del curso, as칤 que es importante que tengan un conocimiento base sobre este framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2EvTlRNbfSV"
      },
      "source": [
        "## **쯈u칠 es PyTorch exactamente?**\n",
        "\n",
        "PyTorch es un framework para hacer Deep Learning. Las caracter칤sticas principales que ofrece son:\n",
        "\n",
        "- Operaciones basicas con **tensores**.\n",
        "- Usar facilmente y de forma transparente la **GPU**, si es que existe.\n",
        "- Utilidades ya implementadas para acelerar el desarrollo de **redes neuronales**. Capas lineales, capas recurrentes, capas convolucionales, funciones de activaci칩n, funciones de p칠rdida, etc.\n",
        "- Motor de **diferenciaci칩n y propagaci칩n de gradientes autom치tico**. Se guarda un registro de las operaciones que se realizan sobre un tensor y luego se puede calcular autom치ticamente la derivada de un tensor de salida con respecto a los par치metros que estuvieron involucrados es un c치lculo.\n",
        "\n",
        "<!-- Como les decia, PyTorch es un framework para hacer deep learning. Las caracteristicas principales de un framework de este tipo es que permite trabajar y realizar operaciones basicas con tensores (abajo explicamos que son y por qu칠 nos interesan), permite usar facilmente y de forma transparente la GPU, si es que existe (mas delante explicamos por que querriamos hacer esto)  y tambien viene con varias utilidades ya implementadas para acelerar el desarrollo de redes neuronales. Por ejemplo, viene con varios modulos de redes neuronales, como capas lineales (como las que vieron en clases), capas recurrentes, capas convulocionales (estas se ven mas adelante), funciones de activacion, funciones de perdida, etc. Finalmente, y quiza lo mas importante del framework, es que viene con un motor de diferenciacion y propagacion de gradientes automatico, es decir, se guarda un registro de las operaciones que se realizan sobre un tensor y luego se puede calcular automaticamente la derivada de un tensor de salida con respecto a los parametros que estuvieron involucrados en su calculo.\n",
        "\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woZQw3j2bkPw"
      },
      "source": [
        "## **쯈u칠 es un tensor?**\n",
        "\n",
        "Un tensor es una estructura matem치tica para organizar datos. De toda la vida hemos sabido lo que es un **n칰mero**. En 치lgebra lineal vimos que podemos organizar (de forma indexada) varios n칰meros en lo que llamamos **vector**, y luego extendimos esta organizaci칩n a una estructura bidimensional, una **matriz**, con filas y columnas.\n",
        "\n",
        "Los **tensores** son una forma de generalizar esta idea. Decimos que un numero wacho es un tensor de 0 dimensiones, un **vector** es un tensor de **1 dimensi칩n** y una **matriz** es un tensor de **2 dimensiones**. Este concepto nos permite generalizar esta organizaci칩n de los datos sobre dimensiones mayores. Podemos hablar, por ejemplo, de un **cubo** (un tensor de 3 dimensiones), que se podr칤a interpretar como varias matrices apiladas, o m치s generalmente un **tensor de N dimensiones** donde N es un n칰mero cualquiera (entre mas grande N mas dif칤cil de imaginar :D).\n",
        "\n",
        "Los tensores de dimensiones mayores se los pueden imaginar como listas, donde sus elementos son tensores de dimensiones menores. Esto lo pueden ver en la siguiente fotaza:\n",
        "\n",
        "(source: knoldus)\n",
        "\n",
        "![visualizacion tensor](https://i.stack.imgur.com/Lv1qU.jpg)\n",
        "\n",
        "Otro Ejemplo un poco mas burdo:\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1tb7popMBUSSj4YzD-Ypytoo6n7PbXzuJ\" width=300 height=300 />\n",
        "</center>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-CrbVL5bsrm"
      },
      "source": [
        "## **쯇or qu칠 me interesan los tensores?**\n",
        "\n",
        "Cuando uno trabaja en Deep Learning, es muy com칰n encontrarse con datos de **alta dimensionalidad**.\n",
        "\n",
        "En **NLP** por ejemplo, en el capitulo de embeddings vimos que es 칰til representar una **palabra** como un **vector** que captura informaci칩n de la palabra, ya sea de su contexto, de los caracteres que contiene, etc.\n",
        "\n",
        "![imagen.png](data:image/png;base64,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)\n",
        "\n",
        "Tomando esto en cuenta, vemos que una lista de palabras (una frase) la podemos representar como una **matriz**. A estas alturas ya tenemos un **tensor de 2 dimensiones**\n",
        "\n",
        "![imagen.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASsAAADmCAYAAACNpaNqAAAVwUlEQVR4nO2dsW/b1trG+Q848uzmQt4cIAXYwb6A78LAWdLlo6d0aGJmaAvYH2AaSL5mI9PUWU073UXvTaShU9JC1JbgBjBR3GTIYo03U8+f8HyDQVa0bEf0ax6d8+r9AQQqibJ+j/ToFXmkoA4EQRAswJm2gCAIwiTIsBIEwQpkWAmCYAUyrARBsAIZVoIgWIEMK0EQrECGlSAIViDDShAEK5BhJQiCFciwEgTBCmRYNYTjOLLJJtsFW+33VAPvUwG41IuhE/GjIX40ZFgZBMey6ET8aHD0MzuRxXAsi07EjwZHP7MTWQzHsuhE/Ghw9DM7kcVwLItOxI8GRz+zE1kMx7LoRPxocPQzO5HFcCyLTsSPBkc/sxNZDMey6ET8aHD0MzuRxXAsi07EjwZHP7MTWQzHsuhE/Ghw9DM7kcVwLItOxI8GRz+zE1kMx7LoRPxocPQzO5HFcCyLTsSPBkc/sxNZDMey6ET8aHD0054ojmM4joMoinQ/tFY4lkUn4keDo99UEjmOg36/P42H1gbHsuhE/Ghw9NOe6OjoyPgn8iowPaP40RA/GsYNqzzPMRgMMBwOy+v29vbgeV6TD2sEHMuiE/GjwdGvkURKKfi+jyRJkGUZXNctb/N9H3EcN/GwRsGxLDoRPxoc/RpJ5LoukiQBcDK4RodVq9Viv14F8CyLTsSPBke/K0/U7/fhOA7CMEQcxwjDsDwNnJX1KoBnWXQifjQ4+l15oiiK0G63z7ytWK9SSkEpddUPbRQcy6IT8aPB0e/KE+V5XjntA4A0Tct1rDiOkSQJ8jy/6oc2Co5l0Yn40eDo10iiIAiws7ODJEkQBAG63S6Akx+E3rp1C0EQNPGwRsGxLDoRPxoc/RpLdHx8jOPj47HruR9RFXAsi07EjwZHP7MTWQzHsuhE/Ghw9DM7kcVwLItOxI8GRz+zE1kMx7LoRPxocPQzO5HFcCyLTsSPBkc/sxNZDMey6ET8aHD0MzuRxXAsi07EjwZHP7MTWQzHsuhE/Ghw9DM7kcVwLItOxI8GRz+zE1kMx7LoRPxocPQzO5HFcCyLTsSPBkc/sxNZDMey6ET8aHD0MzuRxXAsi07EjwZHP7MTWQzHsuhE/Ghw9DM7kcVwLItOxI8GRz+zE1mM4ziyySbbBVvt91QD71MBPD/ZdCJ+NDj6mZ3IYjiWRSfiR4Ojn9mJLIZjWXQifjQ4+pmdyGI4lkUn4keDo5/ZiSyGY1l0In40OPqZnchiOJZFJ+JHg6Of2YkshmNZdCJ+NDj6mZ3IYjiWRSfiR4Ojn9mJLIZjWXQifjQ4+pmdyGI4lkUn4keDo5/ZiSyGY1l0In40OPqZnchiOJZFJ+JHg6Of2YkshmNZdCJ+NDj6mZ3IYjiWRSfiR4OjX2OJ0jSF53lN/Xnj4VgWnYgfDY5+jSXyfR/b29tN/Xnj4VgWnYgfDY5+5ER5nmMwGGA4HAIAhsMhsizD/Pw8njx5gjzPy+sHg0HlPqfp9/sYDAZQSlG1pg7HsuhE/Ghw9Lt0IqUU1tfXkSQJsiyD67ro9XrodDoIggCO4yCKInQ6HSilEAQBfN/HrVu3kCQJPM9DmqYAToZXcf9er4d2u10OP1vhWBadiB8Njn6XTuR5HqIoKi/HcVyuURXDqCBNU+R5Dt/3EYZhef88z6GUwvz8PLIsO/dv2wjHsuhE/Ghw9LtUon6/D8dxKkc/URSVA8r3fcRxPHa/VqtVGUoAsL29Ddd1K9fJsGoe8aMhfjS0DasoitButyvXua5bHjWNDqVioOV5fqag53nl/YCT00vHccaGmm1wLItOxI8GR79LJep0OpXTvH6/j3a7DaVUOZSUUuj3++W61N7e3pk/ZTh9FBZFEXzfv4yWUXAsi07EjwZHP9KaVRzH2NnZge/75RGUUgqtVgv7+/uVIybf95EkydjfGQ6HcF0XSZLgwYMHCMNQvg3UgPjRED8a2n+6cHR0hOPj47HriyOsUT737V6WZSyGVAHHsuhE/Ghw9DM7kcVwLItOxI8GRz+zE1kMx7LoRPxocPQzO5HFcCyLTsSPBkc/sxNZDMey6ET8aHD0MzuRxXAsi07EjwZHP7MTWQzHsuhE/Ghw9DM7kcVwLItOxI8GRz+zE1kMx7LoRPxocPQzO5HFcCyLTsSPBkc/sxNZDMey6ET8aHD0MzuRxXAsi07EjwZHP7MTWQzHsuhE/Ghw9DM7kcVwLItOxI8GRz+zE1kMx7LoRPxocPQzO5HFcCyLTsSPBkc/sxNZjOM4sskm2wVb7fdUA+9TATw/2XQifjQ4+pmdyGI4lkUn4keDo5/ZiSyGY1l0In40OPqZnchiOJZFJ+JHg6Of2YkshmNZdCJ+NDj6mZ3IYjiWRSfiR4Ojn9mJLIZjWXQifjQ4+pmdyGI4lkUn4keDo5/ZiSyGY1l0In40OPqZnchiOJZFJ+JHg6Of2YkshmNZdCJ+NDj6mZ3IYjiWRSfiR4Ojn9mJLIZjWXQifjQ4+pmdyGI4lkUn4keDo18jiZIkgeM4iOO4vC7Pc/R6vSYezkg4lkUn4keDo19jiVqtFrIsKy/7vo8wDJt6OOPgWBadiB8Njn6NJMqyzPgnq2lMzy9+NMSPxtSHVb/fR57n2Nvbg+d55fVZlmE4HI7tr5TCYDCAUuoqNYyAY1l0In40OPpdSSKlFDzPQ5qmiOMYi4uL5XpVkiTo9XpwHKcylDqdDnzfR5Zl8H3f+Ce3LqbnET8a4kdjasPKdV1EUVQR6ff7UEphe3sbeZ5X5I6OjtBqtcrhFcdx5UiMAxzLohPxo8HRj5yo2+3CcRwcHx8DOBlEp0W2t7cRBEF52fd9bGxsVC6PfnPIAY5l0Yn40eDoR04URRHa7XZ5uVivUkqVR07FN4PFulWr1UKn0ynv02q10Ov1zlzXshWOZdGJ+NHg6EdOdHox3fM8xHGMvb09DIdDZFlWDrPi6Krdbpc/ayi+OVRKsfppA8ey6ET8aHD0IycqFteTJEEYhgiCAOvr6+VgyvMc7XYbYRgiz3MAfy+uJ0mCKIrQarXw5MkT9Pt9qo4xcCyLTsSPBke/K0s0+gPQYigVDIfDsZ8nDIfD8rRPKcXqFBDgWRadiB8Njn5mJ7IYjmXRifjR4OhndiKL4VgWnYgfDY5+ZieyGI5l0Yn40eDoZ3Yii+FYFp2IHw2OfmYnshiOZdGJ+NHg6Gd2IovhWBadiB8Njn5mJ7IYjmXRifjR4OhndiKL4VgWnYgfDY5+ZieyGI5l0Yn40eDoZ3Yii+FYFp2IHw2OfmYnshiOZdGJ+NHg6Gd2IovhWBadiB8Njn5mJ7IYjmXRifjR4OhndiKL4VgWnYgfDY5+ZieyGMdxZJNNtgu22u+pBt6nAnh+sulE/Ghw9DM7kcVwLItOxI8GRz+zE1kMx7LoRPxocPQzO5HFcCyLTsSPBkc/sxNZDMey6ET8aHD0MzuRxXAsi07EjwZHP7MTWQzHsuhE/Ghw9DM7kcVwLItOxI8GRz+zE1kMx7LoRPxocPQzO5HFcCyLTsSPBkc/sxNZDMey6ET8aHD0MzuRxXAsi07EjwZHP7MTWQzHsuhE/Ghw9DM6kVIKT548mbbGpeBYFp2IHw2OfkYn6vV6aLfb09a4FBzLohPxo8HRz+xEFsOxLDoRPxoc/S6d6Pj4GIPBAMPhsHL9cDjEYDBAnudj1wEnp3ZZlpW3K6XG9j99HxvhWBadiB8Njn6176GUwvr6OuI4Rq/Xw+LiIpRSAIA4jhEEAbIsQxAECMMQSikEQYAgCPDgwYPydtd1sbOzU7nc6/UAAHmeIwxDBEGAvb292qFMgGNZdCJ+NDj61b6H7/uI4xgAkGUZPM+DUgpRFMHzvHK/LMvgOA7SNC2Hl+/75e2e541djqIIABCGIQDAdV2kaVo7lAlwLItOxI8GR79a9zg6OoLjOGOnfkopzM/PVwZLv9+vCLXbbXS73XMvt1qtyuU8z+E4TnnUZhscy6IT8aPB0a/WPaIoOvPbueIoanSIhWEI13UBnKw/jQ6e05d7vR5arVZ5GwBsb28jCAIZVg0hfjTEj0bjw6rT6VRO9QBgb28Pf/75Z+XB//rrL8zPz+Po6Ki8XzG4ACBN08rlYn3q+Pi4XKNqtVro9XrlqaFtcCyLTsSPBke/2vfwPA9xHJeL6cWpWxRF8H0fSZLA8zz0+/3yPsVi+3mXoyjC+vp65UjKdV3s7+/LsGoI8aMhfjS0/XTh6OioPGoaZTgcnnv9RZcBjP104bzrbIFjWXQifjQ4+pmdyGI4lkUn4keDo5/ZiSyGY1l0In40OPqZnchiOJZFJ+JHg6Of2YkshmNZdCJ+NDj6mZ3IYjiWRSfiR4Ojn9mJLIZjWXQifjQ4+pmdyGI4lkUn4keDo5/ZiSyGY1l0In40OPqZnchiOJZFJ+JHg6Of2YkshmNZdCJ+NDj6mZ3IYjiWRSfiR4Ojn9mJLIZjWXQifjQ4+pmdyGI4lkUn4keDo5/ZiSyGY1l0In40OPqZnchiOJZFJ+JHg6Of2Yks5ulvP8gmm2wXbHWRYdUQT3/7AR8+vTd2Ez/xm7ZfXWRYNYQNZZm2g/jNtl9dZFg1hA1lmbaD+M22X11kWDWEDWWZtoP4zbZfXWRYNYQNZZm2g/jNtl9dZFg1hA1lmbaD+M22X11kWDWEDWWZtoP4zbZfXWRYNYQNZZm2g/jNtl9dZFg1hA1lmbaD+M22X11kWDWEDWWZtoP4zbZfXWRYNYQNZZm2g/jNtl9dZFg1hA1lmbaD+M22X11kWDWEDWWZtoP4zbZfXWRYNcQkZUlfdrD1aAvPDnYv3G93/2c8O9jFYTc92XqHOOymePvxDV6/e1W57dnBLl6/e9VImSf1Hd1369HWRD6X8avj8zw9wNajLTx++hhvP745c5+3H9/g9te3p+L34dN7vH73Cofd9EyvrUdb+N//m/y5vGq/F7//eubrSelfXWRYNcTnyvL4px9x//t7+PDpPZ4d7GJldfncfdfurMFxnMo2d22uLPfo9cXfvIoyX9b3eXoA/65/cr+njzHXmqs9sK7y+dt8uFm+ify7PuZac2cOrHvffQvHcbT7PTvYxdqdNcxdm8PWo63Kba/fvcLC9QW8fvcKbz++wdLNJbz844VWv8NuWr6er9+9wsrqcvl6UvpXFxlWDfG5ssxdq75hFq4v4MXvv5657+kC/Jz8XH4aFkU67Ka1BkLdYVXHd2V1GQvXF8pyO46Dx08f13q8q3z+Tg+BuWtzuPfdt5V9Xvz+K1ZWl69sWNXxG33eTg+rtTtrWLuzVl7eerQF/+7/aPVbu7NWGZDFESq1f3UhD6ssy6CUglIKWZYhz/Oxffr9PobDYeU6pRQGgwGUUuU+WZZRdYzhorKkLztjb4qzilpsB52D8r9f/ftVZXgddtNz70cpM8X39btXZbmfpwdwHGeio4FJ/er6rN1ZqwzLldXlsSOJ4g13FcOqrt9F+5wetJsPNzHXmtPq59/1cePLG+UwGh1elP7VhTSswjBEkiT46quvEIYhsiyD67rl0MnzHJ7nodfrIQxDzM/Pl7cFQYA4jnHr1i0EQYButwvf9xHHMUXJGC4qy0Hn4MyybD7c/OyLvLK6XPlEPOymuPHlDfh3fdz//l4jp4GF7/v//mdi37cf3+CXw+dYWV2eeM1mUj/K8/f24xs4jlNx2ny4iRe//3plw+qyfmcNjIXrC2PDahLHq/R7/e4V5q7NwXEc/PNfK5XnjtK/upCHVa/XQ7vdLq+L4xi+70MphcXFRfT7fQAnR2CO40ApheFwiCRJkCQJ2u12eXQVRRE8z6MoGcNFZTmrcCury+W6wHlb+rIzVoZigbO4vHRzaaI3bZ1hdZ7v6OnJWUOhOEXY+OH+xI81id9ln78Pn06OEkb3e/H7r+XzdVXD6rJ+Zw2Mxz/9iBtf3qi8vtRhVdfv7cc38O/65drpjS9vlB+YlP7VhXwauL29jSAIysu+72NjY6McRAWdTgeu61bue/pIyvd97O3tUZWMoKlP3s8dpdz77ttKuS9T5tPbeacNk5Tyw6f3cBxn4n0n8bvs8/fsYHfsDenf9csjxmJYjR5B6vQ7b5/739/D2p01bD3awr3vvi3XA3X5rawul99SPk8PMHdt7tzBVqd/dSEPK9d10el0ysutVgtpmsL3ffi+X16/sbGBMAwra1etVqs8LVRKwXEcDIfDsfUtG7moLC//eHFmWT63CO04zthX26cXYDcfbpLLTPVdu7NWGapz1+Yu/Laprt9lnr/Tg2rr0RZe/vGiXL8qvhRwHAcrq8ufXWNr4vWdZKDd//4eeYG9rt/SzaXK5cNuWr6elP7VhTSsigFTDJw0Tcujp2JNqthvcXGxXLsCTtazRv93PMV9lVLlPjYzybcxo9++OI5TLmA+Tw+w/s16Zf/i6OasYTU6GFZWlydaN6j7beDpb4vO8y3WhEYdHMeZ6BStjl+d5+/lHy+w/s36ye/Teof45fD5mYvCW4+2rvTbwDqvb/HanR5WW4+2KgvqC9cXzvwtVpN+C9cXxtZJi9eT0r+6kIZVt9tFu92G7/tIkqRcqwL+Xlzf399HGIbwPA87OzvlaV6x/+jfcl0XQRCwP7L68OnkUHzp5hIOe4dY/2a9UtLNh5uYu1b9xqc4RTld1OLo4NnBLjZ+uI+lm0vn/uiRMqzq+D7+6Uf4d308Tw9w++vbEzvV8avjUywOj26Pf/qx8vfuf38PX/zji3IRmXJkVdfv2cEu/vmvFcy15vDFP76oLGI/Tw+wsrqMXw6f4/bXt7G7/7P25293/2cs3VzCs4Pd8jdhxetJ6V9dyAvsQRBAKYWjo6Ox25VSlZ8yjP40ofi5wyjD4XDsOluZZBi8fvcK6cvOxF/rn/eJ+vbjG6QvO5/9HQ9lWNX1Lfat41TXb1Kf9//9z9h2Gaem/Cb5O038jq6OX9Gx9GXnyvpXF9KwOr1eJfyNDf82a9oO4jfbfnW59LAqTu1OL5oLJ9hQlmk7iN9s+9VF/rlNQ9hQlmk7iN9s+9VFhlVD2FCWaTuI32z71UWGVUPYUJZpO4jfbPvVRYZVQ9hQlmk7iN9s+9VFhlVD2FCWaTuI32z71UWGVUPYUJZpO4jfbPvVRYZVQ9hQlmk7iN9s+9VFhlVD2FCWaTuI32z71UWGVUPYUJZpO4jfbPvVRYZVQ9hQlmk7iN9s+9VFhlVD2FCWaTuI32z71UWGVUPYUJZpO4jfbPvVRYZVQ9hQlmk7iN9s+9VFhlVDPP3tB9lkk+2CrS4yrARBsAIZVoIgWIEMK0EQrECGlSAIViDDShAEK5BhJQiCFciwEgTBCmRYCYJgBTKsBEGwAhlWgiBYgQwrQRCsQIaVIAhWIMNKEAQrkGElCIIVyLASBMEKZFgJgmAFMqwEQbACGVaCIFjB/wO1MeL+aDNxqgAAAABJRU5ErkJggg==)\n",
        "\n",
        "쯈u칠 pasa si por alguna raz칩n queremos operar sobre varias frases a la vez? 쯈u칠 pasa si tenemos una lista de frases? Bueno, ahora tenemos un \"cubo\", un tensor de 3 dimensiones, donde la primera dimensiones corresponde a cada frase dentro del conjunto, la segunda a cada palabra dentro de la frase y finalmente la tercera a cada una las posiciones dentro del vector de embeddings.\n",
        "\n",
        "![imagen.png](data:image/png;base64,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)\n",
        "\n",
        "Esto se repite en muchas m치s 치reas, por ejemplo una imagen RGB es un tensor de 3 dimensiones, un video, donde hay una lista de im치genes se puede interpretar como un tensor de 4 dimensiones, y as칤.\n",
        "\n",
        "Analizar **varios elementos a la vez** es una pr치ctica com칰n en Deep Learning. Esto hace que el manejo de **tensores** se vuelva importante debido a que normalmente aumentamos en uno el n칰mero de dimensiones de los ejemplos al hacer esto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQgtuoz9p9W_"
      },
      "source": [
        "## **Mini-Batches y entrenamiento**\n",
        "\n",
        "En clases se vio que la forma en la que se entrenan las redes neuronales es un **proceso iterativo**, donde en primer lugar se realiza una **predicci칩n que es mala**, **se calcula una funci칩n de perdida**, **para cada par치metro se calcula el gradiente de la _loss_ con respecto a este par치metro** y luego **se desciende en la direcci칩n de este gradiente para tratar de llevar los par치metros a los valores que minimizan la funci칩n de perdida**.\n",
        "\n",
        "Si este proceso iterativo se llevara a cabo de a un ejemplo a la vez, el valor de **la _loss_ ser칤a muy dependiente del ejemplo concreto que se acaba de observar** y podr칤a no ser representativo de la _loss_ general. Esto resulta en actualizaciones ruidosas de los par치metros, porque la _loss_ para el siguiente ejemplo puede ser muy distinta al valor anterior y as칤 es como los par치metros pueden oscilar y tener dificultad para converger.\n",
        "\n",
        "Ac치 es donde nos viene a rescatar el concepto de **_mini-batch_**. Un **Mini-Batch** es un **peque침o subconjunto aleatorio de muestras** de los datos de entrenamiento. Los ejemplos se pasan por la red en **grupos peque침os** para que cada conjunto produzca una  **_loss_ m치s representativa**. Esto le agrega robustez al modelo y lo **ayuda a converger**. El tama침o del _mini-batch_ (cantidad de ejemplos que se pasan a la vez) se vuelve un hiper par치metro de la red. Los valores 칩ptimos de tama침o de _mini-batch_ pueden variar mucho, pero los n칰meros que yo he visto var칤an entre 8 y 32, aunque para algunas aplicaciones he visto valores del orden de 1000.\n",
        "\n",
        "\n",
        "Pueden leer un poco m치s [ac치](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/) o [ac치](https://ruder.io/optimizing-gradient-descent/), la secci칩n introductoria lo explica un poco en m치s detalle, y cita al libro [Deep Learning](https://www.deeplearningbook.org/), que es muy weno :-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVtK2_G4bBsp"
      },
      "source": [
        "<br>\n",
        "<center>\n",
        "<img src=\"https://ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif\" width=300 height=300 />\n",
        "</center>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqCm5gsibx5s"
      },
      "source": [
        "\n",
        "\n",
        "## **Ejemplos**\n",
        "La API realmente es muy similar a la de Numpy, asique veremos solo unos pocos ejemplos. La documentaci칩n sobre los tensores la pueden ver [ac치](https://pytorch.org/docs/stable/tensors.html) y la documentacion general de las operaciones sobre tensores que ofrece pytorch esta [ac치](https://pytorch.org/docs/stable/torch.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTn7dZVwq-SO",
        "outputId": "c10c3966-1d64-40c0-faed-dd692d4307a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n"
          ]
        }
      ],
      "source": [
        "# Instalamos portalocker para luego acceder a los datasets de Pytorch\n",
        "!pip install portalocker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsaS37KAzZoj",
        "outputId": "8f2687be-ed36-4084-d4dd-b5557c65630d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.22.4)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext) (1.26.15)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (16.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchtext) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchtext) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Nos aseguramos que pytorch y torchtext esten en la ultima version\n",
        "!pip install torch -U\n",
        "!pip install torchtext -U\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhqLuYsIYTtt",
        "outputId": "693866ab-36a9-4c6c-cfbe-72e1c2a31fb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Desde una lista de listas\n",
            " tensor([[2, 3, 4],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "Dimensiones del tensor\n",
            " torch.Size([2, 3])\n",
            "\n",
            "Numero de dimensiones del tensor\n",
            " 2\n"
          ]
        }
      ],
      "source": [
        "# Creacion de un tensor a partir de otra estructura\n",
        "a = [[2,3,4], [4,5,6]]\n",
        "t = torch.tensor(a)\n",
        "print(\"Desde una lista de listas\\n\", t)\n",
        "print(\"\\nDimensiones del tensor\\n\", t.size())\n",
        "print(\"\\nNumero de dimensiones del tensor\\n\", t.dim())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbmxaTNpeeCI",
        "outputId": "d8366134-e2e6-4208-c524-75506de0dca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor vacio\n",
            " tensor([[[0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.]]])\n"
          ]
        }
      ],
      "source": [
        "# Creacion de un tensor \"vacio\"\n",
        "t = torch.empty(2,2,3)\n",
        "print(\"Tensor vacio\\n\", t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4oh5DZJehaF",
        "outputId": "fa2069a7-609a-4461-b32f-892ae9861a1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puros unos\n",
            " tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n"
          ]
        }
      ],
      "source": [
        "# Creacion de tensores con puros 1 o puros ceros\n",
        "t = torch.ones(2,3,4)\n",
        "# t = torch.zeros(2,3,4,5)\n",
        "print(\"Puros unos\\n\", t) # notar la tercera dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PLHgzdMe2cb",
        "outputId": "b7bb5ace-0d43-45e3-e206-9341e09b340e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distribucion uniforme\n",
            " tensor([[0.3622, 0.7138],\n",
            "        [0.0177, 0.9653],\n",
            "        [0.7910, 0.9969]])\n",
            "\n",
            "Distribucion normal\n",
            " tensor([[-1.8151,  0.0762, -1.1272],\n",
            "        [ 1.5216,  0.9126, -0.3856]])\n"
          ]
        }
      ],
      "source": [
        "# Random sampling\n",
        "t = torch.empty(3, 2).uniform_() # notar operacion in-place\n",
        "print(\"Distribucion uniforme\\n\", t)\n",
        "\n",
        "t = torch.randn(2, 3)\n",
        "print(\"\\nDistribucion normal\\n\", t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33AQwoOXobdD",
        "outputId": "dbccc524-1ff8-488c-b466-d8f85414abb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Operaciones con escalares\n",
            " tensor([[6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.]])\n"
          ]
        }
      ],
      "source": [
        "# Operaciones matematicas\n",
        "t = torch.ones(3,4)\n",
        "print(\"Operaciones con escalares\\n\", t + 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBINbVumph2W",
        "outputId": "ab65b020-2ce9-4788-e81f-1797419fe6e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Operaciones entre tensores\n",
        "t1 = torch.ones(2, 3)\n",
        "t1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2fJNHSJisbC",
        "outputId": "3b3b69d2-24d4-405b-bc32-6459a3cc36d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Operaciones entre tensores\n",
            " tensor([[3., 3., 3.],\n",
            "        [3., 3., 3.]])\n"
          ]
        }
      ],
      "source": [
        "t2 = torch.ones(2, 3) * 2\n",
        "print(\"Operaciones entre tensores\\n\", t1 + t2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2ouTAQYXxyH",
        "outputId": "30880b5d-a6f0-4d30-d00e-1496927c6985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Suma in-place\n",
            " tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.]])\n"
          ]
        }
      ],
      "source": [
        "# Tambien se pueden hacer operaciones in-place, se modifica el mismo tensor\n",
        "t = torch.ones(2,3)\n",
        "t.add_(1)\n",
        "print(\"Suma in-place\\n\", t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N33ZYc3LYH94",
        "outputId": "7a1cf312-219d-4be6-b697-2ee2d39463f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensiones de partida\n",
            " torch.Size([16])\n",
            "\n",
            "Usamos el metodo .view() y el -1 para que torch infiera dimensiones\n",
            " torch.Size([2, 8])\n",
            "\n",
            "Podemos volver a aplanar el tensor con .flatten()\n",
            " torch.Size([16])\n",
            "\n",
            "Podemos agregar dimensiones sin agregar datos con .unsqueeze()\n",
            " torch.Size([4, 1, 4])\n",
            "\n",
            "Con .squeeze() podemos sacar todas las dimensiones de tama침o 1\n",
            " torch.Size([4, 4])\n"
          ]
        }
      ],
      "source": [
        "# Hay veces que es util reorganizar los datos de un tensor, o agregar\n",
        "# dimensiones\n",
        "t = torch.arange(16)\n",
        "print(\"Dimensiones de partida\\n\", t.shape)\n",
        "\n",
        "t = t.view(-1, 8)\n",
        "print(\"\\nUsamos el metodo .view() y el -1 para que torch infiera dimensiones\\n\", t.shape)\n",
        "\n",
        "t = t.flatten() # Aqui tambien se podria usar .view(-1)\n",
        "print(\"\\nPodemos volver a aplanar el tensor con .flatten()\\n\", t.shape)\n",
        "\n",
        "t = t.view(-1, 4).unsqueeze(1) # tambien podria ser .view(-1, 1, 4)\n",
        "print(\"\\nPodemos agregar dimensiones sin agregar datos con .unsqueeze()\\n\", t.shape)\n",
        "\n",
        "t = t.squeeze()\n",
        "print(\"\\nCon .squeeze() podemos sacar todas las dimensiones de tama침o 1\\n\", t.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6MMvvLpL3ro",
        "outputId": "8ebc9fbc-adcf-41b7-8d64-76e93ae746cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dim=0: tensor([-2.0317,  3.2109,  3.9659, -2.3611,  2.0870, -0.5390, -1.8922,  0.9365,\n",
            "         3.0435,  0.1149])\n",
            "dim=1: tensor([-2.1338,  3.8021, -1.3782,  1.7595,  4.4852])\n"
          ]
        }
      ],
      "source": [
        "# Podemos hacer las tipicas sumas\n",
        "t = torch.randn(5, 10)\n",
        "# dim = 0 es suma de filas y 1 de columnas\n",
        "print(f\"dim=0: {t.sum(dim=0)}\")\n",
        "print(f\"dim=1: {t.sum(dim=1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aoqanb1yNtTe"
      },
      "outputs": [],
      "source": [
        "# Tal como con numpy podemos hacer funciones\n",
        "def softmax(T, dim):\n",
        "  T = torch.as_tensor(T)\n",
        "  T = T - torch.max(T)\n",
        "  deno = torch.exp(T)\n",
        "  suma = torch.sum(torch.exp(T), dim=dim, keepdim=True)\n",
        "  output = deno/suma\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGcH3FZqOBOz",
        "outputId": "c47878ec-0bb6-4330-ae62-746ef3bd00e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0054, 0.0704, 0.0274, 0.0212, 0.0399, 0.0093, 0.0299, 0.0046, 0.0257,\n",
              "         0.0248, 0.0420, 0.0315, 0.0839, 0.0160, 0.0153, 0.0034, 0.0144, 0.0369,\n",
              "         0.0971, 0.0120, 0.0083, 0.0699, 0.0094, 0.0057, 0.0087, 0.0049, 0.0234,\n",
              "         0.0255, 0.2015, 0.0134, 0.0142, 0.0042],\n",
              "        [0.0179, 0.0024, 0.0129, 0.0122, 0.0180, 0.0012, 0.0079, 0.0133, 0.0211,\n",
              "         0.0165, 0.0125, 0.0069, 0.0048, 0.0537, 0.0606, 0.0137, 0.0153, 0.0039,\n",
              "         0.0510, 0.0365, 0.1179, 0.0415, 0.0071, 0.0267, 0.0082, 0.0115, 0.0094,\n",
              "         0.0126, 0.0048, 0.0270, 0.0010, 0.3496],\n",
              "        [0.0183, 0.0413, 0.0406, 0.0109, 0.0587, 0.0117, 0.0062, 0.0090, 0.0328,\n",
              "         0.1288, 0.0484, 0.0246, 0.0158, 0.0086, 0.0339, 0.0211, 0.1779, 0.0149,\n",
              "         0.0818, 0.0236, 0.0127, 0.0215, 0.0083, 0.0098, 0.0520, 0.0197, 0.0062,\n",
              "         0.0296, 0.0056, 0.0197, 0.0024, 0.0037],\n",
              "        [0.0109, 0.0152, 0.0126, 0.0249, 0.0393, 0.0289, 0.0264, 0.0079, 0.0632,\n",
              "         0.0146, 0.0581, 0.1166, 0.0272, 0.0814, 0.0149, 0.0100, 0.0116, 0.0171,\n",
              "         0.0618, 0.0087, 0.0042, 0.0254, 0.0020, 0.0035, 0.0044, 0.0050, 0.0148,\n",
              "         0.0759, 0.0332, 0.1598, 0.0083, 0.0118],\n",
              "        [0.0529, 0.0670, 0.0086, 0.0449, 0.0268, 0.1578, 0.0064, 0.0065, 0.0117,\n",
              "         0.0065, 0.0325, 0.0321, 0.0282, 0.0090, 0.0280, 0.0055, 0.0117, 0.0204,\n",
              "         0.0113, 0.0584, 0.0490, 0.0172, 0.0912, 0.0086, 0.0139, 0.0021, 0.0389,\n",
              "         0.0234, 0.0105, 0.0334, 0.0182, 0.0673]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t = torch.randn(5, 32)\n",
        "softmax(t, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Noj-NgUarIXt"
      },
      "source": [
        "## **GPUs y Deep Learning**\n",
        "\n",
        "La **GPU** es lo mismo que la **tarjeta de video** de los computadores. Es un chip que esta dise침ado para manipular gran cantidad de matrices de p칤xeles, aplicarles transformaciones, y enviarlas a la pantalla para que las podamos ver. Lo interesante de las **GPU** es que est치n pensadas espec칤ficamente para paralelizar c치lculos debido a su aplicaci칩n en **matrices**.\n",
        "\n",
        "Como referencia un procesador multi-nucleo tiene entre 4 y 16 n칰cleos actualmente, mientras que una **GPU** puede f치cilmente superar los 1000 n칰cleos (aunque son m치s simples).\n",
        "\n",
        "La mayor칤a de las operaciones tensoriales se pueden paralelizar, por lo que la GPU se aprovecha de esta propiedad y es capaz de realizar operaciones sobre una matriz completa en **un solo ciclo de reloj** (muy muy r치pido). Esto puede mejorar el tiempo de computaci칩n hasta por un factor de 100 en cierto casos.\n",
        "\n",
        "Es por esto que las GPUs se usan tanto en deep learning, porque el **deep learning esta basado en operaciones b치sicas sobre tensores**, pero en cantidades enormes. Nos estamos aprovechando de a침os de investigaci칩n y desarrollo en c칩mo subir los FPS de tu juego favorito para darle un uso ~~productivo~~ cient칤fico.\n",
        "\n",
        "Si bien las GPU son la principal componente utilizada para realizar deep learning, el 2018 Google hizo publicas unos procesadores llamaods [TPU](https://cloud.google.com/tpu?hl=es-419), quienes permiten realizar una parelizaci칩n extrema en las operaciones. A pesar de sus beneficios, actualmente estos dispositivos solo se encuentran disponibles en los nubes de Google (de puro taca침os..)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "QKNjqMt5qFIJ",
        "outputId": "080fb94c-e167-45c5-871a-ca1c11b92916"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe \n",
              "    width=\"560\"\n",
              "    height=\"315\"\n",
              "    src=\"https://www.youtube.com/embed/-P28LKWTzrI\"\n",
              "    frameborder=\"0\"\n",
              "    allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\"\n",
              "    allowfullscreen>\n",
              "</iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<iframe\n",
        "    width=\"560\"\n",
        "    height=\"315\"\n",
        "    src=\"https://www.youtube.com/embed/-P28LKWTzrI\"\n",
        "    frameborder=\"0\"\n",
        "    allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\"\n",
        "    allowfullscreen>\n",
        "</iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3in0Nv1fZ9N"
      },
      "source": [
        "\n",
        "## **Usando la GPU en PyTorch**\n",
        "\n",
        "Otra de las gracias de **PyTorch** es que permite **interactuar con la GPU** de forma muy transparente para el usuario. Mover **tensores** desde la CPU (que es donde se crean por default) hacia la GPU se hace en una pura l칤nea, y adem치s es muy simple escribir c칩digo **agn칩stico** al dispositivo, lo que significa que si el sistema donde se corre el c칩digo dispone de GPUs estas se ocupan, pero si no, se ocupa la CPU nom치s.\n",
        "\n",
        "A continuaci칩n hay unos ejemplos, y pueden leer m치s al respecto [ac치](https://pytorch.org/docs/stable/notes/cuda.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8EjrIZnypw5",
        "outputId": "9ccb4855-89e5-4526-8481-127279a5efc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu May 18 18:19:29 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Primero usemos un comando de shell para obtener informacion de la GPU\n",
        "# Recuerden cambiar el runtime del colab\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgAok3BJzSWi",
        "outputId": "c2a89e00-092e-4f39-9f38-a75e95c2e19f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Habemus GPU? True\n",
            "Cuantas GPUs me regala Google? 1\n"
          ]
        }
      ],
      "source": [
        "# Verificar si cuda esta disponible en el entorno\n",
        "print(\"Habemus GPU?\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available(): # Usar esto para codigo agnostico\n",
        "    print(\"Cuantas GPUs me regala Google?\", torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1cyvw3mzeMP",
        "outputId": "e29828a5-7200-434f-dd52-4c1b6be1f70b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Los tensores se instancian en la cpu por default\n",
            "Pero se pueden mover al dispositivo cuda:0 usando el methodo .cuda()\n",
            "Tambien se pueden llevar a cuda:0 usando el metodo .to()\n"
          ]
        }
      ],
      "source": [
        "# Mover tensores entre gpu y cpu\n",
        "t = torch.empty(3, 4)\n",
        "print(f\"Los tensores se instancian en la {t.device} por default\")\n",
        "\n",
        "t = t.cuda() # .cuda() retorna un nuevo tensor en GPU\n",
        "print(f\"Pero se pueden mover al dispositivo {t.device} usando el methodo .cuda()\")\n",
        "\n",
        "t = torch.empty(3, 4).to(\"cuda\") # Tambien se puede usar con \"cpu\"\n",
        "print(f\"Tambien se pueden llevar a {t.device} usando el metodo .to()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeYMoyGFv9zO",
        "outputId": "5e4bf078-a931-4fbc-db8e-7874dee1d6dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu May 18 18:19:35 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    26W /  70W |    601MiB / 15360MiB |      1%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Veamos el uso de la gpu\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IisFuXt1Bqi",
        "outputId": "42e0ad31-55b4-4e1c-9642-47d9e4fe8f48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu May 18 18:19:35 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    26W /  70W |   6325MiB / 15360MiB |      2%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Ahora creemos un tensor tremendo\n",
        "t = torch.empty(6000, 1000, 1000, device=\"cuda\", dtype=torch.int8) # Cada elemento pesa 1 byte\n",
        "\n",
        "# Y veamos cuanta VRAM estamos usando\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi57aSyh12CV"
      },
      "source": [
        "## Oye pero mi GPU no es tan bac치n, no tengo tanta VRAM, me voy a echar el ramo u.u\n",
        "\n",
        "Pucha, las GPUs son caras, pero por suerte Google se ba침a en dinero y nos regala tiempo de GPU en Colab.\n",
        "\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "    Google \\downarrow\n",
        "\\end{equation}\n",
        "\n",
        "<center><img src=\"https://media.giphy.com/media/Xy2PrQq6BIw7u/giphy.gif\"></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn0RaV-s4kx_"
      },
      "source": [
        "## **Derivar con PyTorch**\n",
        "\n",
        "<center><img src=\"https://pytorch.org/assets/images/computational_graph_reverse_auto_differentiation.png\" height=300></center>\n",
        "\n",
        "Otra gracia m치s de PyTorch (un framework muy agraciado) es que puede **almacenar el grafo de computaci칩n** luego de realizar **operaciones sobre tensores**. Adem치s, todas las **funciones** que usa el framework tienen sus respectivas **derivadas implementadas**, lo permite que se pueda calcular la derivada de un **nodo ra칤z del grafo de computaci칩n (tensores de salida)** con respecto una **hoja (tensores de entrada)**. M치s informaci칩n con respecto a autograd se puede revisar [ac치](https://pytorch.org/docs/stable/notes/autograd.html).\n",
        "\n",
        "\n",
        "\n",
        "Veamos un ejemplo super simple (para mantenerlo simple, todas las operaciones son punto a punto). Queremos derivar la siguiente expresi칩n:\n",
        "\n",
        "$$out = mean(x^2 + log(x))$$\n",
        "\n",
        "Asumamos que $x$ es un **tensor** de **una sola dimensi칩n**, pero la explicaci칩n aplica tambi칠n para tensores de m치s dimensiones. Notemos tambi칠n que $out$ es un escalar.\n",
        "\n",
        "Podemos definir una variable auxiliar como:\n",
        "$$u = x^2 + log(x)$$\n",
        "\n",
        "Con lo cual:\n",
        "$$out = mean(u)$$\n",
        "\n",
        "Al derivarlo tenemos:\n",
        "$$\\frac{\\partial out}{\\partial x_i} = \\sum_j\\frac{\\partial out}{\\partial u_j}\\frac{\\partial u_j}{\\partial x_i}$$\n",
        "\n",
        "Reemplazando tenemos:\n",
        "$$\\frac{\\partial out}{\\partial x_i} = \\sum_j\\frac{\\partial mean(u)}{\\partial u_j}\\frac{\\partial (x_j^2 + log(x_j))}{\\partial x_i}$$\n",
        "\n",
        "\n",
        "###Primera parte de la derivada\n",
        "Veamos primero como queda la primera derivada.\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial mean(u)}{\\partial u_j} = \\frac{1}{len(u)}\\frac{u_1 + \\dots + u_j + \\dots + u_n}{\\partial u_j} = \\frac{1}{len(u)}, \\forall j\n",
        "\\end{equation}\n",
        "\n",
        "Si se fijan, el largo de un vector no depende de ning칰n valor espec칤fico, y como el promedio es una suma de los elementos del tensor, la derivada es $\\frac{1}{len(y)}$.\n",
        "\n",
        "###Segunda parte de la derivada\n",
        "La siguiente derivada es m치s f치cil, ya que son solo operaciones punto a punto. Todas las posiciones que no dependen de la posici칩n _i-esima_ se van a cero. La derivada queda como sigue.\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\frac{\\partial (x_j^2 + log(x_j))}{\\partial x_i} & =\n",
        "\\begin{cases}\n",
        "2x_i + \\frac{1}{x_i} & \\text{ if } j = i \\\\\n",
        "0 & \\text{ if } j \\neq i\n",
        "\\end{cases} \\\\\n",
        "\\sum_j \\frac{\\partial (x_j^2 + log(x_j))}{\\partial x_i} & = 2x_i + \\frac{1}{x_i}\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Con estas dos derivadas calculadas podemos volver a la ecuaci칩n incial y calcular el valor total de la derivada original. Queda de la siguiente forma (m치s un poco de algebra para que la ecuaci칩n quede bonita).\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial out}{\\partial x_i} = \\frac{2x_i^2 + 1}{x_ilen(x)}\n",
        "\\end{equation}\n",
        "\n",
        "Si probamos con el vector $x=[1,2,3,4]$, reemplazando en la formula deber칤amos obtener que el gradiente con respecto a $x$ es $[\\frac{3}{4},\\frac{9}{8},\\frac{19}{12},\\frac{33}{16}] = [0.75, 1.125, 1.5833, 2.0625]$\n",
        "\n",
        "Ahora veamos como pytorch lo hace todo autom치ticamente. ~~Para no hacer esto nunca m치s~~."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxDqe1fn4juH",
        "outputId": "f8df2bde-99b0-4993-c724-909dfd75a85e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradiente de out con respecto a x\n",
            " tensor([0.7500, 1.1250, 1.5833, 2.0625])\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(1., 5., requires_grad=True) # Se registra en el grafo de computacion\n",
        "out = torch.mean(x**2 + torch.log(x))\n",
        "out.backward() # Se usa backpropagation para calcular gradientes\n",
        "\n",
        "print(\"Gradiente de out con respecto a x\\n\", x.grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENHIYO-LOjuf",
        "outputId": "dae91013-f8f5-4b4f-e8ec-1649b4474573"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[2., 2., 2., 2., 2.],\n",
              "        [2., 2., 2., 2., 2.],\n",
              "        [2., 2., 2., 2., 2.]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Otro ejemplo no esta de mas (mas simple)\n",
        "y = torch.ones(3,5, requires_grad=True)\n",
        "z = torch.sum(y**2+10)\n",
        "z.backward()\n",
        "y.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCldodKYqVVV"
      },
      "source": [
        "## **Recomendaciones**\n",
        "\n",
        "La api de PyTorch es **gigante** as칤 que yo recomiendo que cuando quieran realizar alguna operaci칩n sobre un tensor y no pillen f치cilmente en la documentaci칩n una forma directa de hacerlo, nos pregunten nom치s. Preguntas del tipo: \"como hago x en pytorch\" o \"como hago y en pytorch\" son perfectamente razonables, no tengan verguenza de preguntar en el foro/discord :D\n",
        "\n",
        "Hasta ac치 llega la introducci칩n a [PyTorch](https://pytorch.org/), cualquier duda extra nos pueden preguntar extensivamente en el foro o por Discord .\n",
        "\n",
        "\n",
        "_Eso es todo amigos_\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "u-AOSmuAsmA8",
        "outputId": "dd38bbb1-b1c8-4152-c113-8fb480abafbc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe \n",
              "    width=\"560\"\n",
              "    height=\"315\"\n",
              "    src=\"https://www.youtube.com/embed/Ga_RwPmx-N0\"\n",
              "    frameborder=\"0\"\n",
              "    allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\"\n",
              "    allowfullscreen>\n",
              "</iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<iframe\n",
        "    width=\"560\"\n",
        "    height=\"315\"\n",
        "    src=\"https://www.youtube.com/embed/Ga_RwPmx-N0\"\n",
        "    frameborder=\"0\"\n",
        "    allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\"\n",
        "    allowfullscreen>\n",
        "</iframe>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z3WpurA4hIx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46jrmdTCxmC1"
      },
      "source": [
        "# Parte 2: Clasificaci칩n de Texto usando la librer칤a torchtext (Embeddings + FeedForward)\n",
        "\n",
        "Ahora usaremos capas de Embedding y en redes feed forward para la clasificaci칩n de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2T2cua6q11t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCYwbzIolQZg"
      },
      "source": [
        "## Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_a7nTyllOUS"
      },
      "source": [
        "Descargu칠mos el dataset que usaremos en los ejmplos de esta parte de la auxiliar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiTRFyZBlV_4",
        "outputId": "39a6b030-7907-4a74-86c4-8ef1bcb4721b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-18 18:19:35--  http://raw.githubusercontent.com/uchile-nlp/ArgumentMining2017/master/data/complete_data.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://raw.githubusercontent.com/uchile-nlp/ArgumentMining2017/master/data/complete_data.csv.gz [following]\n",
            "--2023-05-18 18:19:35--  https://raw.githubusercontent.com/uchile-nlp/ArgumentMining2017/master/data/complete_data.csv.gz\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9304385 (8.9M) [application/octet-stream]\n",
            "Saving to: 딲omplete_data.csv.gz뗖n",
            "\n",
            "complete_data.csv.g 100%[===================>]   8.87M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-05-18 18:19:36 (89.1 MB/s) - 딲omplete_data.csv.gz saved [9304385/9304385]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget raw.githubusercontent.com/uchile-nlp/ArgumentMining2017/master/data/complete_data.csv.gz\n",
        "# !gunzip complete_data.csv.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvUG4iRIlYHP",
        "outputId": "935ec2bd-56bd-49d5-a95a-6a50e63ab250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ejemplo aleatorio:\n",
            " ('Solidaridad', 'ser generoso con la comunidad y sobre todo con quienes se ven m치s desprotegidos')\n",
            "\n",
            "Ejemplo aleatorio:\n",
            " ('Democracia', 'el cambio constitucional es un proceso ciudadano que trasladar치 a las instituciones, los valores, principios, derechos, deberes e instituciones.')\n",
            "\n",
            "Ejemplo aleatorio:\n",
            " ('Democracia', 'nuestra opini칩n es importante porque nosotros formamos el estado. todos podemos elegir y no solo los que han recibido una educaci칩n mejor.')\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import csv\n",
        "with gzip.open('complete_data.csv.gz', 'rt') as f:\n",
        "  data = csv.DictReader(f, strict=True, escapechar=\"\\\\\")\n",
        "\n",
        "  # Para este ejemplo solo voy a trabajar con documentos de la categoria 1, \"Valores\"\n",
        "  dataset = tuple(\n",
        "      # Usemos lowercase para que el vocabulario no quede tan grande\n",
        "      (row[\"constitutional_concept\"], row[\"argument\"].lower())\n",
        "      for row in data if row[\"topic\"] == \"1\" and row[\"argument\"]\n",
        "  )\n",
        "\n",
        "dataset = dataset[:10000]\n",
        "\n",
        "# Mostremos algunos ejemplos\n",
        "from random import sample\n",
        "for example in sample(dataset, 3):\n",
        "    print(\"\\nEjemplo aleatorio:\\n\", example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvvzxraUlaCY"
      },
      "source": [
        "## Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOIc2PHhldJ_",
        "outputId": "9ddc6851-8b19-4e5e-ad17-0fa8a5119bc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Algunos ejemplos del dataset:\n",
            "('Respeto', 'es un valor fundamental el respeto, como signo vivo de la aceptaci칩n incondicional de unos con otros, a pesar de nuestras diferencias pol칤ticas, religiosas, sociales, etc')\n",
            "('Respeto', 'debido a que el concepto engloba valores y principios desglosados anteriormente.')\n",
            "('Descentralizaci칩n', 'la pr치ctica de los poderes centralizados no ha logrado superar las barreras geogr치ficas que permitan tomar buenas y atingentes decisiones a la realidad de cada regi칩n.')\n"
          ]
        }
      ],
      "source": [
        "# Ahora con este vocabulario podemos armar un set de train y uno de validacion\n",
        "import torch\n",
        "from torch.utils.data.dataset import random_split\n",
        "train_len = int(len(dataset) * .8)\n",
        "\n",
        "train_split, validation_split = random_split(dataset, [train_len, len(dataset) - train_len])\n",
        "\n",
        "print(\"Algunos ejemplos del dataset:\")\n",
        "for example in sample(list(train_split), 3):\n",
        "    print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEgOXqAAlg7I"
      },
      "source": [
        "## Vocabulario (a partir del train split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zekFPQqWlnmY"
      },
      "source": [
        "Ahora construiremos el vocabulario, para esto necesitamos un tokenizador, pero ``torchtext`` no tiene un tokenizador para espa침ol as칤 que bajaremos uno de ``spacy``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_M21_JWlkkW",
        "outputId": "5ce833cb-716e-491e-f1ec-03bdaf936102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-18 18:19:40.748938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-18 18:19:41.781464: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-18 18:19:43.059097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-18 18:19:43.059561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-18 18:19:43.059736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "\u001b[38;5;3m丘 As of spaCy v3.0, shortcuts like 'es' are deprecated. Please use the\n",
            "full pipeline package name 'es_core_news_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.5.0/es_core_news_sm-3.5.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.1.2)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.5.0\n",
            "\u001b[38;5;2m九 Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xgDAsk4lpiv",
        "outputId": "50f05b60-01e7-48b2-d391-71c869fe87b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"es\" could not be loaded, trying \"es_core_news_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\Tama침o del vocabulario: 9493\n",
            "Algunas palabras del vocabulario: ['hablamos', 'dependa', 'particularidades', 'reprimido', 'cultivar']\n",
            "\n",
            "Cantidad de labels: 54\n",
            "Algunos labels: ['Ciudadan칤a', 'Pluralismo', 'Respeto']\n"
          ]
        }
      ],
      "source": [
        "# Ahora si construiremos el vocabulario y la lista de labels\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer(\"spacy\", \"es\")\n",
        "min_freq = 1\n",
        "vocab = build_vocab_from_iterator((tokenizer(text[1]) for text in train_split), min_freq=min_freq)\n",
        "labels = list({doc[0] for doc in train_split})\n",
        "label_map = {label: index for index, label in enumerate(labels)}\n",
        "\n",
        "UNK_IDX = 0\n",
        "vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "vocab.insert_token('<pad>', 1)\n",
        "\n",
        "stoi = vocab.get_stoi()\n",
        "\n",
        "print(\"\\Tama침o del vocabulario:\", len(vocab))\n",
        "print(\"Algunas palabras del vocabulario:\", sample(vocab.get_itos(), 5))\n",
        "print(\"\\nCantidad de labels:\", len(labels))\n",
        "print(\"Algunos labels:\", sample(labels, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFWm6ulxlt4w",
        "outputId": "e1e95b0a-218a-47ec-e6f4-1156a62d5004"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Identidad cultural': 0,\n",
              " 'Paz / Convivencia pac칤fica': 1,\n",
              " 'Equidad de g칠nero': 2,\n",
              " 'Familia': 3,\n",
              " 'Familia basada en matrimonio heterosexual': 4,\n",
              " 'Solidaridad': 5,\n",
              " 'Respeto': 6,\n",
              " 'Participaci칩n': 7,\n",
              " 'Respeto / Conservaci칩n de la naturaleza o medio ambiente': 8,\n",
              " 'Democracia participativa': 9,\n",
              " 'Tolerancia': 10,\n",
              " 'Desarrollo': 11,\n",
              " 'Seguridad': 12,\n",
              " 'Ciudadan칤a': 13,\n",
              " 'Rep칰blica': 14,\n",
              " 'Estado laico': 15,\n",
              " 'Propiedad Privada': 16,\n",
              " 'Integraci칩n': 17,\n",
              " 'Estado de Derecho': 18,\n",
              " 'Amistad c칤vica': 19,\n",
              " 'Unidad': 20,\n",
              " 'Soberan칤a': 21,\n",
              " 'Libertad de culto': 22,\n",
              " 'Libertad de expresi칩n': 23,\n",
              " 'Bien Com칰n / Comunidad': 24,\n",
              " 'Patriotismo': 25,\n",
              " 'Pluralismo': 26,\n",
              " 'Emprendimiento libre': 27,\n",
              " 'Descentralizaci칩n': 28,\n",
              " 'Inclasificable/No corresponde': 29,\n",
              " 'Justicia': 30,\n",
              " 'Desarrollo integral': 31,\n",
              " 'Libertad': 32,\n",
              " 'Estado garante': 33,\n",
              " 'Equidad': 34,\n",
              " 'Otro': 35,\n",
              " 'Seguridad Social': 36,\n",
              " 'Dignidad': 37,\n",
              " 'Libertad de conciencia': 38,\n",
              " 'Inclusi칩n': 39,\n",
              " 'Justicia social': 40,\n",
              " 'Autonom칤a / Libertad': 41,\n",
              " 'Democracia': 42,\n",
              " 'Probidad': 43,\n",
              " 'Plurinacionalismo': 44,\n",
              " 'Derechos humanos': 45,\n",
              " 'Desarrollo sustentable': 46,\n",
              " 'Innovaci칩n / Creatividad': 47,\n",
              " 'Subsidiaridad': 48,\n",
              " 'Diversidad': 49,\n",
              " 'Igualdad': 50,\n",
              " 'Multiculturalidad': 51,\n",
              " 'Responsabilidad': 52,\n",
              " 'Transparencia y publicidad': 53}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5WUbpwDlvtX"
      },
      "source": [
        "## Modelo con capa de Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRGbqpOwlxi9"
      },
      "outputs": [],
      "source": [
        "# Pum ahora hagamos la arquitectura\n",
        "# simplecita, un capa de embedding, y luego una red feed forward de\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Red neuronal con una sola capa oculta\n",
        "class ArgumentClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class, hidden_size, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        # capa de embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, pad_idx)\n",
        "        # self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\")\n",
        "\n",
        "        # capas de la MLP\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        # self.fc1 = nn.Linear(embed_dim, hidden_size)\n",
        "        # self.fc2 = nn.Linear(hidden_size, num_class)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # La representacion de un documento sera el promedio de los\n",
        "        # embeddings de sus palabras.\n",
        "        # (B, N, 1) -> (B, N, E)\n",
        "        h = self.embedding(batch)\n",
        "        # (B, N, E) -> (B, E)\n",
        "        h = h.mean(dim=1)\n",
        "        # h = self.embedding(batch)\n",
        "\n",
        "        # computar las capas de la red MLP\n",
        "        h = self.fc(h)\n",
        "        # h = F.relu(self.fc1(h))\n",
        "        # h = self.fc2(h)\n",
        "\n",
        "        return h\n",
        "        # return torch.softmax(h, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVq1K4tGl1cH"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PybMd2z_l5BI"
      },
      "source": [
        "Primero, necesitamos definir una funci칩n que convierta un conjunto de items de nuestro dataset en un batch,recordando que los tensores en pytorch tienen que ser homogeneos. Esta funci칩n recibe una lista de muestras del dataset y debe retornar tensores que agrupan estas muestras.\n",
        "\n",
        "Por ejemplo, si cada ejemplo de nuestro dataset contiene 2 elementos y nuestro tama침o de batch es de 16, entonces esta funci칩n debe retorna una tupla de 2 tensores, cada uno de dimension 16 x ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47v6ZhdkvJ26"
      },
      "outputs": [],
      "source": [
        "#from itertools import zip_longest\n",
        "\n",
        "# creamos lista de tensores\n",
        "#train_dataset, validation_dataset = [\n",
        "#    [\n",
        "#        (\n",
        "#            label_map[item[0]],\n",
        "#            torch.tensor([vocab[token] if token in vocab else 0 for token in tokenizer(item[1])]),\n",
        "#        ) for item in split\n",
        "#    ] for split in [train_split, validation_split]\n",
        "#]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L28eCml2l3DF"
      },
      "outputs": [],
      "source": [
        "from itertools import zip_longest\n",
        "\n",
        "# creamos lista de tensores\n",
        "train_dataset, validation_dataset = [\n",
        "    [\n",
        "        (\n",
        "            label_map[item[0]],\n",
        "            torch.tensor([vocab[token] for token in tokenizer(item[1])]),\n",
        "        ) for item in split\n",
        "    ] for split in [train_split, validation_split]\n",
        "]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhMt7CZAkukz"
      },
      "source": [
        "쯇ero que estamos haciendo en la lista de comprehesion anterior?... Bueno, basicamente estamos entregando los vectores de una forma legible por el computador, de tal forma que el modelo entienda en el entrenamiento una forma numerica de las palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od2Xy8FzkTIH",
        "outputId": "7eb9544e-4842-430c-d1a8-1112a403c0f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(42, tensor([   7,  156,  213,    0,    4,   58,  147,  884,    4,  194,    5,    7,\n",
            "          45,   21,  110,    9,  244,    5, 1019,    9,   38,  553,    3]))\n",
            "(7, tensor([   6,  100,   72,  411,    2,   14,  487,    6,    4,   39,  245,    9,\n",
            "         237,    0, 6377, 1915,   11,  717,   63,   21,   36,    3]))\n"
          ]
        }
      ],
      "source": [
        "# Lo que ve el comput\n",
        "for data in sample(train_dataset, 2):\n",
        "  print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKZhS0VIkuIn",
        "outputId": "891db0b2-7d8e-4bdd-b39d-aeefe19a8e9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label: Participaci칩n\n",
            "text: que esta sea vinculante , es decir que la constituci칩n garantice a trav칠s de ejercicios participativos las grandes decisiones del pa칤s . \n"
          ]
        }
      ],
      "source": [
        "# Lo que ver칤a un humano\n",
        "for key in label_map:\n",
        "  if label_map[key] == data[0]:\n",
        "    print(f\"label: {key}\")\n",
        "\n",
        "human_text = ''\n",
        "for i in data[1]:\n",
        "  human_text += vocab.get_itos()[i] + ' '\n",
        "print(f\"text: {human_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHT0-U4vNP2U"
      },
      "outputs": [],
      "source": [
        "def generate_batch(batch):\n",
        "    return (\n",
        "        # En este caso como los labels son n칰meros,\n",
        "        # el tensor es de una sola dimension de tama침o batch_size\n",
        "        torch.tensor([item[0] for item in batch]),\n",
        "\n",
        "        # En este caso se retorna un tensor de 2 dimensiones, batch_size x N,\n",
        "        # donde N es mayor largo de los ejemplo en el batch. Aca se realiza\n",
        "        # padding de los ejemplos mas cortos.\n",
        "        torch.tensor(\n",
        "            list(\n",
        "                zip(\n",
        "                    *zip_longest(\n",
        "                        *[item[1] for item in batch], fillvalue=vocab[\"<pad>\"]\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        ),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHOKyrL7l7t5"
      },
      "outputs": [],
      "source": [
        "# Ahora creamos funciones para entrenar y validar el modelo\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def train_func(train_dataset):\n",
        "\n",
        "    # Entranamos el modelo\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    data = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=generate_batch,\n",
        "    )\n",
        "    for i, (cls, text) in enumerate(data):\n",
        "        optimizer.zero_grad()\n",
        "        cls, text = cls.to(device), text.to(device)\n",
        "        output = model(text)\n",
        "\n",
        "        loss = criterion(output, cls)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    # Ajustar el learning rate\n",
        "    # scheduler.step()\n",
        "\n",
        "    return train_loss / len(train_dataset), train_acc / len(train_dataset)\n",
        "\n",
        "\n",
        "def test(test_dataset):\n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "    data = DataLoader(\n",
        "        test_dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch\n",
        "    )\n",
        "    for cls, text in data:\n",
        "        cls, text = cls.to(device), text.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(text)\n",
        "            loss = criterion(output, cls)\n",
        "            test_loss += loss.item()\n",
        "            acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    return test_loss / len(test_dataset), acc / len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkUm9pQal8zo",
        "outputId": "2430fed6-b69a-4e13-c28f-aa196b18c44f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 4 seconds\n",
            "\tLoss: 0.1966(train)\t|\tAcc: 21.9%(train)\n",
            "\tLoss: 0.1771(valid)\t|\tAcc: 31.6%(valid)\n",
            "Epoch: 2  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.1669(train)\t|\tAcc: 34.6%(train)\n",
            "\tLoss: 0.1611(valid)\t|\tAcc: 35.9%(valid)\n",
            "Epoch: 3  | time in 0 minutes, 2 seconds\n",
            "\tLoss: 0.1501(train)\t|\tAcc: 40.7%(train)\n",
            "\tLoss: 0.1499(valid)\t|\tAcc: 41.0%(valid)\n",
            "Epoch: 4  | time in 0 minutes, 2 seconds\n",
            "\tLoss: 0.1379(train)\t|\tAcc: 45.1%(train)\n",
            "\tLoss: 0.1431(valid)\t|\tAcc: 44.5%(valid)\n",
            "Epoch: 5  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.1281(train)\t|\tAcc: 47.9%(train)\n",
            "\tLoss: 0.1380(valid)\t|\tAcc: 45.5%(valid)\n",
            "Epoch: 6  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.1212(train)\t|\tAcc: 51.6%(train)\n",
            "\tLoss: 0.1345(valid)\t|\tAcc: 47.1%(valid)\n",
            "Epoch: 7  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.1141(train)\t|\tAcc: 53.5%(train)\n",
            "\tLoss: 0.1331(valid)\t|\tAcc: 47.1%(valid)\n",
            "Epoch: 8  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.1085(train)\t|\tAcc: 55.6%(train)\n",
            "\tLoss: 0.1314(valid)\t|\tAcc: 47.8%(valid)\n",
            "Epoch: 9  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.1033(train)\t|\tAcc: 57.6%(train)\n",
            "\tLoss: 0.1286(valid)\t|\tAcc: 49.4%(valid)\n",
            "Epoch: 10  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.0984(train)\t|\tAcc: 59.0%(train)\n",
            "\tLoss: 0.1280(valid)\t|\tAcc: 50.3%(valid)\n",
            "Epoch: 11  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.0944(train)\t|\tAcc: 60.2%(train)\n",
            "\tLoss: 0.1273(valid)\t|\tAcc: 49.6%(valid)\n",
            "Epoch: 12  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.0904(train)\t|\tAcc: 62.4%(train)\n",
            "\tLoss: 0.1260(valid)\t|\tAcc: 51.0%(valid)\n",
            "Epoch: 13  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.0869(train)\t|\tAcc: 63.8%(train)\n",
            "\tLoss: 0.1271(valid)\t|\tAcc: 50.4%(valid)\n",
            "Epoch: 14  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.0826(train)\t|\tAcc: 65.7%(train)\n",
            "\tLoss: 0.1263(valid)\t|\tAcc: 50.9%(valid)\n",
            "Epoch: 15  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.0792(train)\t|\tAcc: 67.0%(train)\n",
            "\tLoss: 0.1260(valid)\t|\tAcc: 50.9%(valid)\n",
            "Epoch: 16  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.0759(train)\t|\tAcc: 68.2%(train)\n",
            "\tLoss: 0.1259(valid)\t|\tAcc: 51.1%(valid)\n",
            "Epoch: 17  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.0730(train)\t|\tAcc: 69.1%(train)\n",
            "\tLoss: 0.1264(valid)\t|\tAcc: 51.2%(valid)\n",
            "Epoch: 18  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.0698(train)\t|\tAcc: 70.5%(train)\n",
            "\tLoss: 0.1283(valid)\t|\tAcc: 51.3%(valid)\n",
            "Epoch: 19  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.0673(train)\t|\tAcc: 72.3%(train)\n",
            "\tLoss: 0.1269(valid)\t|\tAcc: 51.8%(valid)\n",
            "Epoch: 20  | time in 0 minutes, 1 seconds\n",
            "\tLoss: 0.0644(train)\t|\tAcc: 73.5%(train)\n",
            "\tLoss: 0.1286(valid)\t|\tAcc: 51.5%(valid)\n"
          ]
        }
      ],
      "source": [
        "# Ahora por fin tenemos todo lo necesario para entrenar el modelo.\n",
        "import time\n",
        "\n",
        "N_EPOCHS = 20\n",
        "LEARN_RATE = 2.0\n",
        "STEP_SIZE = 1\n",
        "BATCH_SIZE = 16\n",
        "EMBED_DIM = 100\n",
        "HIDDEN_SIZE = 1024\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = ArgumentClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    embed_dim=EMBED_DIM,\n",
        "    num_class=len(labels),\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    pad_idx=vocab[\"<pad>\"],\n",
        ").to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARN_RATE)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, STEP_SIZE)\n",
        "\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_func(train_dataset)\n",
        "    valid_loss, valid_acc = test(validation_dataset)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs // 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print(\n",
        "        f\"Epoch: {epoch + 1}\", f\" | time in {mins} minutes, {secs} seconds\",\n",
        "    )\n",
        "    print(\n",
        "        f\"\\tLoss: {train_loss:.4f}(train)\\t|\"\n",
        "        f\"\\tAcc: {train_acc * 100:.1f}%(train)\"\n",
        "    )\n",
        "    print(\n",
        "        f\"\\tLoss: {valid_loss:.4f}(valid)\\t|\"\n",
        "        f\"\\tAcc: {valid_acc * 100:.1f}%(valid)\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2DyCzBUbfwk"
      },
      "source": [
        "# Parte 3: Clasificaci칩n de texto usando CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HylpfYQcADP"
      },
      "source": [
        "Para este caso vamos a trabajar con un dataset de noticias, el cual es f치cilmente descargable con la librer칤a y da muchos mejores resultados (ya que los anteriores estaban ah칤 nomas)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZgbo70IcGrq"
      },
      "outputs": [],
      "source": [
        "# por si las moscas importamos todo denuevo (para los que reci칠n se unen a la sintonia)\n",
        "# https://pytorch.org/text/stable/datasets.html#ag-news\n",
        "import os\n",
        "import torch\n",
        "from random import choice\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "train_dataset, test_dataset = AG_NEWS(root=\"data\", split=('train', 'test'))\n",
        "train_list = list(train_dataset)\n",
        "test_list = list(test_dataset)\n",
        "\n",
        "# Informacion relevante del dataset\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "vocab = build_vocab_from_iterator(tokenizer(x[1]) for x in train_list)\n",
        "\n",
        "vocab.set_default_index(0)\n",
        "vocab.insert_token('<pad>', 1)\n",
        "\n",
        "stoi = vocab.get_stoi()\n",
        "\n",
        "num_classes = 4\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTNLFNbTc5rx"
      },
      "source": [
        "Luego, creamos una red no tan profunda pero bien competente para nuestra tarea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOwf8CZZctxG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from itertools import zip_longest\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=32, num_classes=10,\n",
        "                 cnn_pool_channels=24, cnn_kernel_size=3):\n",
        "\n",
        "        # Inicializamos la clase padre\n",
        "        super().__init__()\n",
        "\n",
        "        # Creamos la capa de embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Creamos la capa de convoluci칩n\n",
        "        # `in_channels`: Es el n칰mero de canales de entrada de la convoluci칩n. En este caso, como estamos trabajando con texto, s칩lo tenemos un canal, por lo que `in_channels=1`.\n",
        "        # `out_channels`: Es el n칰mero de canales de salida de la convoluci칩n. Especifica la cantidad de filtros que se aplicar치n a la entrada. En este caso, queremos generar `cnn_pool_channels` canales de salida, por lo que `out_channels=cnn_pool_channels`.\n",
        "        # `kernel_size`: Es el tama침o del kernel de la convoluci칩n. En este caso, estamos usando un kernel de tama침o `cnn_kernel_size * embed_dim`, donde `embed_dim` es la dimensi칩n de los vectores de embedding. Esto significa que cada filtro de la convoluci칩n cubrir치 `cnn_kernel_size` palabras (o tokens) en una dimensi칩n y `embed_dim` en la otra.\n",
        "        # `stride`: Es el desplazamiento que se aplica a la entrada de la convoluci칩n. En este caso, estamos desplazando la entrada `embed_dim` unidades en cada paso. Esto significa que se aplicar치n filtros a cada palabra (o token) de la entrada.\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels=1,\n",
        "            out_channels=cnn_pool_channels,\n",
        "            kernel_size=cnn_kernel_size * embed_dim,\n",
        "            stride=embed_dim,\n",
        "        )\n",
        "\n",
        "        # Calculamos el tama침o de entrada de la capa lineal\n",
        "        fc_in_size = cnn_pool_channels\n",
        "\n",
        "        # Creamos la capa lineal\n",
        "        self.fc = nn.Linear(fc_in_size, num_classes)\n",
        "\n",
        "        # Inicializamos los pesos de las capas\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Definimos el rango de los valores iniciales de los pesos\n",
        "        initrange = 0.5\n",
        "\n",
        "        # Inicializamos los pesos de la capa de embedding\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        # Inicializamos los pesos de la capa lineal\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        # Inicializamos los sesgos de la capa lineal en cero\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "\n",
        "        # Preparamos el input de la capa de embeddings a partir de text y offsets\n",
        "        text = torch.tensor(\n",
        "            list(\n",
        "                zip(\n",
        "                    *zip_longest(\n",
        "                        *([text[o:offsets[i+1]] for i, o in enumerate(offsets[:-1])] + [text[offsets[-1]:len(texts)]]),\n",
        "                        fillvalue=vocab[\"<pad>\"]\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        ).to(text.device)\n",
        "\n",
        "        # Obtenemos la representaci칩n de la frase a partir de la capa de embedding\n",
        "        h = self.embedding(text)\n",
        "\n",
        "        # Aplicamos la capa de convoluci칩n\n",
        "        h = h.view(h.size(0), 1, -1)\n",
        "        h = torch.relu(self.conv(h))\n",
        "        h = h.mean(dim=2)\n",
        "\n",
        "        # Obtenemos el resultado final a partir de la capa lineal\n",
        "        output = self.fc(h)\n",
        "\n",
        "        # Aplicamos la funci칩n de activaci칩n log-softmax\n",
        "        return F.log_softmax(output, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLOXQIhec4PY"
      },
      "source": [
        "Finalmente, generamos la funci칩n para cargar por batch y luego entrenamos directamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3HqZeLxA1q5",
        "outputId": "0a14186e-eb70-4139-a924-f5de11dffc75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Epoch: 001\t Phase: train Iter: 469/469\t iter-Acc: 23.958%\t iter-Loss: 1.377\n",
            " train\tAvg. Acc: 26.982%\t Avg. Loss: 0.005\n",
            "Epoch: 001\t Phase: test Iter: 006/006\t iter-Acc: 26.083%\t iter-Loss: 1.383\n",
            " test\tAvg. Acc: 25.118%\t Avg. Loss: 0.001\n",
            "Epoch: 002\t Phase: train Iter: 469/469\t iter-Acc: 26.562%\t iter-Loss: 1.369\n",
            " train\tAvg. Acc: 29.483%\t Avg. Loss: 0.005\n",
            "Epoch: 002\t Phase: test Iter: 006/006\t iter-Acc: 29.667%\t iter-Loss: 1.380\n",
            " test\tAvg. Acc: 29.329%\t Avg. Loss: 0.001\n",
            "Epoch: 003\t Phase: train Iter: 469/469\t iter-Acc: 31.771%\t iter-Loss: 1.357\n",
            " train\tAvg. Acc: 31.831%\t Avg. Loss: 0.005\n",
            "Epoch: 003\t Phase: test Iter: 006/006\t iter-Acc: 33.583%\t iter-Loss: 1.375\n",
            " test\tAvg. Acc: 33.000%\t Avg. Loss: 0.001\n",
            "Epoch: 004\t Phase: train Iter: 469/469\t iter-Acc: 39.583%\t iter-Loss: 1.339\n",
            " train\tAvg. Acc: 34.222%\t Avg. Loss: 0.005\n",
            "Epoch: 004\t Phase: test Iter: 006/006\t iter-Acc: 35.500%\t iter-Loss: 1.367\n",
            " test\tAvg. Acc: 35.013%\t Avg. Loss: 0.001\n",
            "Epoch: 005\t Phase: train Iter: 469/469\t iter-Acc: 40.625%\t iter-Loss: 1.312\n",
            " train\tAvg. Acc: 36.841%\t Avg. Loss: 0.005\n",
            "Epoch: 005\t Phase: test Iter: 006/006\t iter-Acc: 37.667%\t iter-Loss: 1.351\n",
            " test\tAvg. Acc: 37.303%\t Avg. Loss: 0.001\n",
            "Epoch: 006\t Phase: train Iter: 469/469\t iter-Acc: 44.271%\t iter-Loss: 1.269\n",
            " train\tAvg. Acc: 39.628%\t Avg. Loss: 0.005\n",
            "Epoch: 006\t Phase: test Iter: 006/006\t iter-Acc: 40.917%\t iter-Loss: 1.324\n",
            " test\tAvg. Acc: 40.829%\t Avg. Loss: 0.001\n",
            "Epoch: 007\t Phase: train Iter: 469/469\t iter-Acc: 48.438%\t iter-Loss: 1.205\n",
            " train\tAvg. Acc: 43.268%\t Avg. Loss: 0.005\n",
            "Epoch: 007\t Phase: test Iter: 006/006\t iter-Acc: 45.083%\t iter-Loss: 1.279\n",
            " test\tAvg. Acc: 45.303%\t Avg. Loss: 0.001\n",
            "Epoch: 008\t Phase: train Iter: 469/469\t iter-Acc: 53.125%\t iter-Loss: 1.112\n",
            " train\tAvg. Acc: 47.923%\t Avg. Loss: 0.005\n",
            "Epoch: 008\t Phase: test Iter: 006/006\t iter-Acc: 50.250%\t iter-Loss: 1.209\n",
            " test\tAvg. Acc: 50.526%\t Avg. Loss: 0.001\n",
            "Epoch: 009\t Phase: train Iter: 469/469\t iter-Acc: 57.292%\t iter-Loss: 1.004\n",
            " train\tAvg. Acc: 53.368%\t Avg. Loss: 0.004\n",
            "Epoch: 009\t Phase: test Iter: 006/006\t iter-Acc: 55.833%\t iter-Loss: 1.119\n",
            " test\tAvg. Acc: 55.816%\t Avg. Loss: 0.001\n",
            "Epoch: 010\t Phase: train Iter: 469/469\t iter-Acc: 61.979%\t iter-Loss: 0.901\n",
            " train\tAvg. Acc: 58.326%\t Avg. Loss: 0.004\n",
            "Epoch: 010\t Phase: test Iter: 006/006\t iter-Acc: 60.833%\t iter-Loss: 1.028\n",
            " test\tAvg. Acc: 59.855%\t Avg. Loss: 0.001\n",
            "Epoch: 011\t Phase: train Iter: 469/469\t iter-Acc: 65.625%\t iter-Loss: 0.818\n",
            " train\tAvg. Acc: 62.263%\t Avg. Loss: 0.004\n",
            "Epoch: 011\t Phase: test Iter: 006/006\t iter-Acc: 64.500%\t iter-Loss: 0.953\n",
            " test\tAvg. Acc: 63.408%\t Avg. Loss: 0.001\n",
            "Epoch: 012\t Phase: train Iter: 469/469\t iter-Acc: 70.833%\t iter-Loss: 0.740\n",
            " train\tAvg. Acc: 66.037%\t Avg. Loss: 0.003\n",
            "Epoch: 012\t Phase: test Iter: 006/006\t iter-Acc: 66.583%\t iter-Loss: 0.884\n",
            " test\tAvg. Acc: 66.250%\t Avg. Loss: 0.001\n",
            "Epoch: 013\t Phase: train Iter: 469/469\t iter-Acc: 73.438%\t iter-Loss: 0.674\n",
            " train\tAvg. Acc: 69.068%\t Avg. Loss: 0.003\n",
            "Epoch: 013\t Phase: test Iter: 006/006\t iter-Acc: 69.333%\t iter-Loss: 0.824\n",
            " test\tAvg. Acc: 69.250%\t Avg. Loss: 0.001\n",
            "Epoch: 014\t Phase: train Iter: 469/469\t iter-Acc: 75.000%\t iter-Loss: 0.622\n",
            " train\tAvg. Acc: 71.575%\t Avg. Loss: 0.003\n",
            "Epoch: 014\t Phase: test Iter: 006/006\t iter-Acc: 71.333%\t iter-Loss: 0.770\n",
            " test\tAvg. Acc: 72.066%\t Avg. Loss: 0.001\n",
            "Epoch: 015\t Phase: train Iter: 469/469\t iter-Acc: 77.083%\t iter-Loss: 0.585\n",
            " train\tAvg. Acc: 73.690%\t Avg. Loss: 0.003\n",
            "Epoch: 015\t Phase: test Iter: 006/006\t iter-Acc: 73.083%\t iter-Loss: 0.724\n",
            " test\tAvg. Acc: 73.579%\t Avg. Loss: 0.001\n",
            "Epoch: 016\t Phase: train Iter: 469/469\t iter-Acc: 78.125%\t iter-Loss: 0.559\n",
            " train\tAvg. Acc: 75.435%\t Avg. Loss: 0.003\n",
            "Epoch: 016\t Phase: test Iter: 006/006\t iter-Acc: 74.167%\t iter-Loss: 0.684\n",
            " test\tAvg. Acc: 75.013%\t Avg. Loss: 0.001\n",
            "Epoch: 017\t Phase: train Iter: 469/469\t iter-Acc: 79.688%\t iter-Loss: 0.543\n",
            " train\tAvg. Acc: 76.525%\t Avg. Loss: 0.003\n",
            "Epoch: 017\t Phase: test Iter: 006/006\t iter-Acc: 76.250%\t iter-Loss: 0.651\n",
            " test\tAvg. Acc: 76.421%\t Avg. Loss: 0.001\n",
            "Epoch: 018\t Phase: train Iter: 469/469\t iter-Acc: 80.729%\t iter-Loss: 0.527\n",
            " train\tAvg. Acc: 78.055%\t Avg. Loss: 0.002\n",
            "Epoch: 018\t Phase: test Iter: 006/006\t iter-Acc: 76.750%\t iter-Loss: 0.619\n",
            " test\tAvg. Acc: 77.526%\t Avg. Loss: 0.001\n",
            "Epoch: 019\t Phase: train Iter: 469/469\t iter-Acc: 81.771%\t iter-Loss: 0.515\n",
            " train\tAvg. Acc: 79.168%\t Avg. Loss: 0.002\n",
            "Epoch: 019\t Phase: test Iter: 006/006\t iter-Acc: 77.667%\t iter-Loss: 0.593\n",
            " test\tAvg. Acc: 78.539%\t Avg. Loss: 0.000\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from torch.optim import SGD, lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def generate_batch(batch):\n",
        "  label = torch.tensor([entry[0]-1 for entry in batch])\n",
        "  texts = [tokenizer(entry[1]) for entry in batch]\n",
        "  offsets = [0] + [len(text) for text in texts]\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  big_text = torch.cat([torch.tensor([vocab[t] if t in stoi else 0 for t in text]) for text in texts])\n",
        "  #big_text = torch.cat([torch.tensor([vocab.stoi[t] for t in text]) for text in texts])\n",
        "\n",
        "  return big_text, offsets, label\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 20\n",
        "TEST_BATCH_SIZE = BATCH_SIZE * 5\n",
        "LR = 1e-1\n",
        "\n",
        "model = CNNClassifier(len(vocab), num_classes=num_classes).to(device)\n",
        "optimizer = SGD(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda epoch: .9 ** (epoch // 10)])\n",
        "\n",
        "split_size = {'train': len(train_list), 'test': len(test_list)}\n",
        "\n",
        "# train_dataset, test_dataset = AG_NEWS(root=\"data\")\n",
        "for epoch in range(1, NUM_EPOCHS):\n",
        "  train_loader = DataLoader(train_list, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "  test_loader = DataLoader(test_list, batch_size=TEST_BATCH_SIZE, collate_fn=generate_batch)\n",
        "  loaders = {'train': train_loader, 'test': test_loader}\n",
        "  for phase in ['train', 'test']:\n",
        "    if phase == 'train':\n",
        "      model.train()\n",
        "    else:\n",
        "      model.eval()\n",
        "\n",
        "    total_acc, total_loss = 0, 0\n",
        "    for i, (texts, offsets, cls) in enumerate(loaders[phase]):\n",
        "      texts = texts.to(device)\n",
        "      offsets = offsets.to(device)\n",
        "      cls = cls.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      with torch.set_grad_enabled(phase == 'train'):\n",
        "        output = model(texts, offsets)\n",
        "        loss = criterion(output, cls)\n",
        "        total_loss += loss.item()\n",
        "        if phase == 'train':\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      acc = (output.argmax(1) == cls).sum().item()\n",
        "      total_acc += acc\n",
        "\n",
        "      sys.stdout.write('\\rEpoch: {0:03d}\\t Phase: {1} Iter: {2:03d}/{3:03d}\\t iter-Acc: {4:.3f}%\\t iter-Loss: {5:.3f}'.format(epoch, phase, i+1, len(loaders[phase]), acc/len(offsets)*100, loss.item()))\n",
        "\n",
        "    if phase == 'train':\n",
        "      scheduler.step()\n",
        "    print('\\n {0}\\tAvg. Acc: {1:.3f}%\\t Avg. Loss: {2:.3f}'.format(phase, total_acc/split_size[phase]*100, total_loss/split_size[phase]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NuHEAZsdL1u"
      },
      "source": [
        "Como podemos ver, con esta segunda perspectiva tenemos un clasificador mas competente en relaci칩n con el anterior. La idea es que tengan diferente perspectivas en la construcci칩n."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}